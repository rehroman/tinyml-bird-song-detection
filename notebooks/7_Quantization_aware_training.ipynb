{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da80dca1-c75b-4ab3-85e5-d9ce9bda8b34",
   "metadata": {},
   "source": [
    "### Quantization aware training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac7161d-8076-4146-afba-1145311c852f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Apply quantization aware training to whole pre-trained baseline model and check summary. All layers are now prefixed by \"quant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44b3b7ca-e642-4f5f-b896-513773d130d7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-09 14:42:39.665954: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-09 14:42:39.709780: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-09 14:42:39.710356: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-09 14:42:40.461894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Error when deserializing class 'LinearSpecLayer' using config={'name': 'ADVANCED_SPEC1', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last', 'sample_rate': 48000, 'spec_shape': [64, 512], 'frame_step': 280, 'fmin': 150, 'fmax': 15000, 'frame_length': 512}.\n\nException encountered: ('Keyword argument not understood:', 'data_format')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/engine/base_layer.py:868\u001b[0m, in \u001b[0;36mLayer.from_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/engine/base_layer.py:340\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Validate optional keyword arguments.\u001b[39;00m\n\u001b[0;32m--> 340\u001b[0m \u001b[43mgeneric_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# Mutable properties\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Indicates whether the layer's weights are updated during training\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# and whether the layer's updates are run during training.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/utils/generic_utils.py:514\u001b[0m, in \u001b[0;36mvalidate_kwargs\u001b[0;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwarg \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_kwargs:\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(error_message, kwarg)\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'data_format')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tfmot\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mquantize_annotate_layer(layer)\n\u001b[1;32m     16\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m layer\n\u001b[0;32m---> 18\u001b[0m annotated_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeras_baselineModel_activation_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Register the custom layer with TensorFlow\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#tf.keras.utils.get_custom_objects()['LinearSpecLayer'] = LinearSpecLayer\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#tf.keras.utils.get_custom_objects()['GlobalLogExpPooling2D'] = GlobalLogExpPooling2D\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Apply quantization using `quantize_apply`\u001b[39;00m\n\u001b[1;32m     25\u001b[0m quant_aware_model \u001b[38;5;241m=\u001b[39m tfmot\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mquantize_apply(annotated_model)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/models/cloning.py:539\u001b[0m, in \u001b[0;36mclone_model\u001b[0;34m(model, input_tensors, clone_function)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, functional\u001b[38;5;241m.\u001b[39mFunctional):\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;66;03m# If the get_config() method is the same as a regular Functional\u001b[39;00m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;66;03m# model, we're safe to use _clone_functional_model (which relies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;66;03m# or input_tensors are passed, we attempt it anyway\u001b[39;00m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;66;03m# in order to preserve backwards compatibility.\u001b[39;00m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generic_utils\u001b[38;5;241m.\u001b[39mis_default(model\u001b[38;5;241m.\u001b[39mget_config) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    537\u001b[0m         clone_function \u001b[38;5;129;01mor\u001b[39;00m input_tensors\n\u001b[1;32m    538\u001b[0m     ):\n\u001b[0;32m--> 539\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_functional_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclone_function\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# Case of a custom model class\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clone_function \u001b[38;5;129;01mor\u001b[39;00m input_tensors:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/models/cloning.py:222\u001b[0m, in \u001b[0;36m_clone_functional_model\u001b[0;34m(model, input_tensors, layer_fn)\u001b[0m\n\u001b[1;32m    218\u001b[0m         model_configs, created_layers \u001b[38;5;241m=\u001b[39m _clone_layers_and_model_config(\n\u001b[1;32m    219\u001b[0m             model, new_input_layers, layer_fn\n\u001b[1;32m    220\u001b[0m         )\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     model_configs, created_layers \u001b[38;5;241m=\u001b[39m \u001b[43m_clone_layers_and_model_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_input_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_fn\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Reconstruct model from the config, using the cloned layers.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m (\n\u001b[1;32m    227\u001b[0m     input_tensors,\n\u001b[1;32m    228\u001b[0m     output_tensors,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m     model_configs, created_layers\u001b[38;5;241m=\u001b[39mcreated_layers\n\u001b[1;32m    232\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/models/cloning.py:298\u001b[0m, in \u001b[0;36m_clone_layers_and_model_config\u001b[0;34m(model, input_layers, layer_fn)\u001b[0m\n\u001b[1;32m    295\u001b[0m         created_layers[layer\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m layer_fn(layer)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 298\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_network_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserialize_layer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_copy_layer\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config, created_layers\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/engine/functional.py:1590\u001b[0m, in \u001b[0;36mget_network_config\u001b[0;34m(network, serialize_layer_fn, config)\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Functional) \u001b[38;5;129;01mand\u001b[39;00m set_layers_legacy:\n\u001b[1;32m   1589\u001b[0m     layer\u001b[38;5;241m.\u001b[39muse_legacy_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1590\u001b[0m layer_config \u001b[38;5;241m=\u001b[39m \u001b[43mserialize_layer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1591\u001b[0m layer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m   1592\u001b[0m layer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minbound_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m filtered_inbound_nodes\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/models/cloning.py:295\u001b[0m, in \u001b[0;36m_clone_layers_and_model_config.<locals>._copy_layer\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    293\u001b[0m     created_layers[layer\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m InputLayer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlayer\u001b[38;5;241m.\u001b[39mget_config())\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     created_layers[layer\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/models/cloning.py:52\u001b[0m, in \u001b[0;36m_clone_layer\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_clone_layer\u001b[39m(layer):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/engine/base_layer.py:870\u001b[0m, in \u001b[0;36mLayer.from_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 870\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when deserializing class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    873\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Error when deserializing class 'LinearSpecLayer' using config={'name': 'ADVANCED_SPEC1', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last', 'sample_rate': 48000, 'spec_shape': [64, 512], 'frame_step': 280, 'fmin': 150, 'fmax': 15000, 'frame_length': 512}.\n\nException encountered: ('Keyword argument not understood:', 'data_format')"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from model import LinearSpecLayer\n",
    "\n",
    "# load baseline keras model which still has the activation layer in the end\n",
    "keras_baselineModel_activation_path = \"/home/jovyan/models/checkpoints_/baseline_two_class_model_activation/\"\n",
    "keras_baselineModel_activation_model = keras.models.load_model(keras_baselineModel_activation_path)\n",
    "\n",
    "annotated_model = tf.keras.models.clone_model(keras_baselineModel_activation_model)\n",
    "\n",
    "# Register the custom layer with TensorFlow\n",
    "#tf.keras.utils.get_custom_objects()['LinearSpecLayer'] = LinearSpecLayer\n",
    "#tf.keras.utils.get_custom_objects()['GlobalLogExpPooling2D'] = GlobalLogExpPooling2D\n",
    "\n",
    "# Apply quantization using `quantize_apply`\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d2c979-e542-4ab0-afc8-4f8860968a44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Does not work because of custom layer. Try to just quantize certain layers such as Conv2D for istance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0fe5e10-4727-4f02-8ac9-8d10921fcee0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to clone model. This generally happens if you used custom Keras layers or objects in your model. Please specify them via `quantize_scope` for your calls to `quantize_model` and `quantize_apply`. [Error when deserializing class 'LinearSpecLayer' using config={'name': 'ADVANCED_SPEC1', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last', 'sample_rate': 48000, 'spec_shape': [64, 512], 'frame_step': 280, 'fmin': 150, 'fmax': 15000, 'frame_length': 512}.\n\nException encountered: ('Keyword argument not understood:', 'data_format')].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/engine/base_layer.py:868\u001b[0m, in \u001b[0;36mLayer.from_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/engine/base_layer.py:340\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Validate optional keyword arguments.\u001b[39;00m\n\u001b[0;32m--> 340\u001b[0m \u001b[43mgeneric_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# Mutable properties\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Indicates whether the layer's weights are updated during training\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# and whether the layer's updates are run during training.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/utils/generic_utils.py:514\u001b[0m, in \u001b[0;36mvalidate_kwargs\u001b[0;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwarg \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_kwargs:\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(error_message, kwarg)\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'data_format')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py:456\u001b[0m, in \u001b[0;36mquantize_apply\u001b[0;34m(model, scheme, quantized_layer_name_prefix)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m   model_copy \u001b[38;5;241m=\u001b[39m \u001b[43m_clone_model_with_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m er:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py:365\u001b[0m, in \u001b[0;36mquantize_apply.<locals>._clone_model_with_weights\u001b[0;34m(model_to_clone)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_clone_model_with_weights\u001b[39m(model_to_clone):\n\u001b[0;32m--> 365\u001b[0m   cloned_model \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_clone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m   cloned_model\u001b[38;5;241m.\u001b[39mset_weights(model_to_clone\u001b[38;5;241m.\u001b[39mget_weights())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/models/cloning.py:539\u001b[0m, in \u001b[0;36mclone_model\u001b[0;34m(model, input_tensors, clone_function)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generic_utils\u001b[38;5;241m.\u001b[39mis_default(model\u001b[38;5;241m.\u001b[39mget_config) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    537\u001b[0m         clone_function \u001b[38;5;129;01mor\u001b[39;00m input_tensors\n\u001b[1;32m    538\u001b[0m     ):\n\u001b[0;32m--> 539\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_functional_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclone_function\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# Case of a custom model class\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/models/cloning.py:222\u001b[0m, in \u001b[0;36m_clone_functional_model\u001b[0;34m(model, input_tensors, layer_fn)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     model_configs, created_layers \u001b[38;5;241m=\u001b[39m \u001b[43m_clone_layers_and_model_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_input_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_fn\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Reconstruct model from the config, using the cloned layers.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/models/cloning.py:298\u001b[0m, in \u001b[0;36m_clone_layers_and_model_config\u001b[0;34m(model, input_layers, layer_fn)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 298\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_network_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserialize_layer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_copy_layer\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config, created_layers\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/engine/functional.py:1590\u001b[0m, in \u001b[0;36mget_network_config\u001b[0;34m(network, serialize_layer_fn, config)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     layer\u001b[38;5;241m.\u001b[39muse_legacy_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1590\u001b[0m layer_config \u001b[38;5;241m=\u001b[39m \u001b[43mserialize_layer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1591\u001b[0m layer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/models/cloning.py:295\u001b[0m, in \u001b[0;36m_clone_layers_and_model_config.<locals>._copy_layer\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     created_layers[layer\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/models/cloning.py:52\u001b[0m, in \u001b[0;36m_clone_layer\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_clone_layer\u001b[39m(layer):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/engine/base_layer.py:870\u001b[0m, in \u001b[0;36mLayer.from_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 870\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when deserializing class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    873\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Error when deserializing class 'LinearSpecLayer' using config={'name': 'ADVANCED_SPEC1', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last', 'sample_rate': 48000, 'spec_shape': [64, 512], 'frame_step': 280, 'fmin': 150, 'fmax': 15000, 'frame_length': 512}.\n\nException encountered: ('Keyword argument not understood:', 'data_format')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m annotated_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mclone_model(\n\u001b[1;32m     16\u001b[0m     keras_baselineModel_activation_model,\n\u001b[1;32m     17\u001b[0m     clone_function\u001b[38;5;241m=\u001b[39mapply_quantization_to_conv,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Apply quantization using `quantize_apply`\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m quant_aware_model \u001b[38;5;241m=\u001b[39m \u001b[43mtfmot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotated_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/keras/metrics.py:74\u001b[0m, in \u001b[0;36mMonitorBoolGauge.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     73\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbool_gauge\u001b[38;5;241m.\u001b[39mget_cell(MonitorBoolGauge\u001b[38;5;241m.\u001b[39m_FAILURE_LABEL)\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 74\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/keras/metrics.py:69\u001b[0m, in \u001b[0;36mMonitorBoolGauge.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     68\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbool_gauge\u001b[38;5;241m.\u001b[39mget_cell(MonitorBoolGauge\u001b[38;5;241m.\u001b[39m_SUCCESS_LABEL)\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py:458\u001b[0m, in \u001b[0;36mquantize_apply\u001b[0;34m(model, scheme, quantized_layer_name_prefix)\u001b[0m\n\u001b[1;32m    456\u001b[0m   model_copy \u001b[38;5;241m=\u001b[39m _clone_model_with_weights(model)\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m er:\n\u001b[0;32m--> 458\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    459\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnable to clone model. This generally happens if you used custom \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    460\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKeras layers or objects in your model. Please specify them via \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    461\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`quantize_scope` for your calls to `quantize_model` and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    462\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`quantize_apply`. [\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m].\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m er) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mer\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_config\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    465\u001b[0m   model_copy\u001b[38;5;241m.\u001b[39muse_legacy_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39muse_legacy_config\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to clone model. This generally happens if you used custom Keras layers or objects in your model. Please specify them via `quantize_scope` for your calls to `quantize_model` and `quantize_apply`. [Error when deserializing class 'LinearSpecLayer' using config={'name': 'ADVANCED_SPEC1', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last', 'sample_rate': 48000, 'spec_shape': [64, 512], 'frame_step': 280, 'fmin': 150, 'fmax': 15000, 'frame_length': 512}.\n\nException encountered: ('Keyword argument not understood:', 'data_format')]."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from model import LinearSpecLayer\n",
    "\n",
    "# load baseline keras model which still has the activation layer in the end\n",
    "keras_baselineModel_activation_path = \"/home/jovyan/models/checkpoints_/baseline_two_class_model_activation/\"\n",
    "keras_baselineModel_activation_model = keras.models.load_model(keras_baselineModel_activation_path)\n",
    "\n",
    "def apply_quantization_to_conv(layer):\n",
    "  if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "  return layer\n",
    "\n",
    "annotated_model = tf.keras.models.clone_model(\n",
    "    keras_baselineModel_activation_model,\n",
    "    clone_function=apply_quantization_to_conv,\n",
    ")\n",
    "\n",
    "# Apply quantization using `quantize_apply`\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "\n",
    "#quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac1d76-bdcd-46a1-98ee-dc690b9a63ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### As it is during Clone, also this filtered Quantization is not possible. We add custom code for the layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "667474a0-1ce5-47ca-87cb-d63b7b67e270",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final spec shape:Tensor(\"quantize_annotate_1312/ExpandDims:0\", shape=(None, 128, 513, 1), dtype=float32)\n",
      "final spec shape:Tensor(\"ADVANCED_SPEC1/ExpandDims:0\", shape=(None, 128, 513, 1), dtype=float32)\n",
      "final spec shape:Tensor(\"ADVANCED_SPEC1/ExpandDims:0\", shape=(None, 128, 513, 1), dtype=float32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Layer ADVANCED_SPEC1:<class 'model.LinearSpecLayer'> is not supported. You can quantize this layer by passing a `tfmot.quantization.keras.QuantizeConfig` instance to the `quantize_annotate_layer` API.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 55\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m###\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m### all Layers\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m###\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m quantize_scope(\n\u001b[1;32m     52\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinearSpecLayer\u001b[39m\u001b[38;5;124m'\u001b[39m: LinearSpecLayer,\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGlobalLogExpPooling2D\u001b[39m\u001b[38;5;124m'\u001b[39m: GlobalLogExpPooling2D\n\u001b[1;32m     54\u001b[0m     }):\n\u001b[0;32m---> 55\u001b[0m     quant_aware_model \u001b[38;5;241m=\u001b[39m \u001b[43mtfmot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotated_model_debug\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/keras/metrics.py:74\u001b[0m, in \u001b[0;36mMonitorBoolGauge.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     73\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbool_gauge\u001b[38;5;241m.\u001b[39mget_cell(MonitorBoolGauge\u001b[38;5;241m.\u001b[39m_FAILURE_LABEL)\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 74\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/keras/metrics.py:69\u001b[0m, in \u001b[0;36mMonitorBoolGauge.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     68\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbool_gauge\u001b[38;5;241m.\u001b[39mget_cell(MonitorBoolGauge\u001b[38;5;241m.\u001b[39m_SUCCESS_LABEL)\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py:496\u001b[0m, in \u001b[0;36mquantize_apply\u001b[0;34m(model, scheme, quantized_layer_name_prefix)\u001b[0m\n\u001b[1;32m    490\u001b[0m quantize_registry \u001b[38;5;241m=\u001b[39m scheme\u001b[38;5;241m.\u001b[39mget_quantize_registry()\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# 4. Actually quantize all the relevant layers in the model. This is done by\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# wrapping the layers with QuantizeWrapper, and passing the associated\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# `QuantizeConfig`.\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclone_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_quantize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/models/cloning.py:539\u001b[0m, in \u001b[0;36mclone_model\u001b[0;34m(model, input_tensors, clone_function)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, functional\u001b[38;5;241m.\u001b[39mFunctional):\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;66;03m# If the get_config() method is the same as a regular Functional\u001b[39;00m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;66;03m# model, we're safe to use _clone_functional_model (which relies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;66;03m# or input_tensors are passed, we attempt it anyway\u001b[39;00m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;66;03m# in order to preserve backwards compatibility.\u001b[39;00m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generic_utils\u001b[38;5;241m.\u001b[39mis_default(model\u001b[38;5;241m.\u001b[39mget_config) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    537\u001b[0m         clone_function \u001b[38;5;129;01mor\u001b[39;00m input_tensors\n\u001b[1;32m    538\u001b[0m     ):\n\u001b[0;32m--> 539\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_functional_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclone_function\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# Case of a custom model class\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clone_function \u001b[38;5;129;01mor\u001b[39;00m input_tensors:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/models/cloning.py:218\u001b[0m, in \u001b[0;36m_clone_functional_model\u001b[0;34m(model, input_tensors, layer_fn)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_legacy_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m keras_option_scope(\n\u001b[1;32m    216\u001b[0m         save_traces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, in_tf_saved_model_scope\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     ):\n\u001b[0;32m--> 218\u001b[0m         model_configs, created_layers \u001b[38;5;241m=\u001b[39m \u001b[43m_clone_layers_and_model_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_input_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_fn\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     model_configs, created_layers \u001b[38;5;241m=\u001b[39m _clone_layers_and_model_config(\n\u001b[1;32m    223\u001b[0m         model, new_input_layers, layer_fn\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/models/cloning.py:298\u001b[0m, in \u001b[0;36m_clone_layers_and_model_config\u001b[0;34m(model, input_layers, layer_fn)\u001b[0m\n\u001b[1;32m    295\u001b[0m         created_layers[layer\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m layer_fn(layer)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 298\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_network_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserialize_layer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_copy_layer\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config, created_layers\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/engine/functional.py:1590\u001b[0m, in \u001b[0;36mget_network_config\u001b[0;34m(network, serialize_layer_fn, config)\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Functional) \u001b[38;5;129;01mand\u001b[39;00m set_layers_legacy:\n\u001b[1;32m   1589\u001b[0m     layer\u001b[38;5;241m.\u001b[39muse_legacy_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1590\u001b[0m layer_config \u001b[38;5;241m=\u001b[39m \u001b[43mserialize_layer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1591\u001b[0m layer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m   1592\u001b[0m layer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minbound_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m filtered_inbound_nodes\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/src/models/cloning.py:295\u001b[0m, in \u001b[0;36m_clone_layers_and_model_config.<locals>._copy_layer\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    293\u001b[0m     created_layers[layer\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m InputLayer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlayer\u001b[38;5;241m.\u001b[39mget_config())\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     created_layers[layer\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py:442\u001b[0m, in \u001b[0;36mquantize_apply.<locals>._quantize\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quantize_config:\n\u001b[1;32m    437\u001b[0m   error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    438\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not supported. You can quantize this \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    439\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer by passing a `tfmot.quantization.keras.QuantizeConfig` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    440\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstance to the `quantize_annotate_layer` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    441\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAPI.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 442\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    443\u001b[0m       error_msg\u001b[38;5;241m.\u001b[39mformat(layer\u001b[38;5;241m.\u001b[39mname, layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m,\n\u001b[1;32m    444\u001b[0m                        quantize_registry\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m))\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# `QuantizeWrapper` does not copy any additional layer params from\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# `QuantizeAnnotate`. This should generally be fine, but occasionally\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# `QuantizeAnnotate` wrapper may contain `batch_input_shape` like params.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# TODO(pulkitb): Ensure this does not affect model cloning.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m quantize_wrapper\u001b[38;5;241m.\u001b[39mQuantizeWrapperV2(\n\u001b[1;32m    451\u001b[0m     layer, quantize_config, name_prefix\u001b[38;5;241m=\u001b[39mquantized_layer_name_prefix)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Layer ADVANCED_SPEC1:<class 'model.LinearSpecLayer'> is not supported. You can quantize this layer by passing a `tfmot.quantization.keras.QuantizeConfig` instance to the `quantize_annotate_layer` API."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from model import LinearSpecLayer, GlobalLogExpPooling2D\n",
    "\n",
    "keras_baselineModel_activation_path = \"/home/jovyan/models/checkpoints_/baseline_two_class_model_activation/\"\n",
    "keras_baselineModel_activation_model = keras.models.load_model(keras_baselineModel_activation_path)\n",
    "\n",
    "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
    "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
    "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "\n",
    "### just custom layers\n",
    "# def apply_quantization_to_custom_layers(layer):\n",
    "#     if type(layer).__name__ == 'LinearSpecLayer':\n",
    "#         return quantize_annotate_layer(layer)\n",
    "#     elif type(layer).__name__ == 'GlobalLogExpPooling2D':\n",
    "#         return quantize_annotate_layer(layer)\n",
    "#     return layer\n",
    "\n",
    "# annotated_model = tf.keras.models.clone_model(\n",
    "#     keras_baselineModel_activation_model,\n",
    "#     clone_function=apply_quantization_to_custom_layers,\n",
    "# )\n",
    "###\n",
    "\n",
    "\n",
    "### just conv2D layers\n",
    "def apply_quantization_to_conv(layer):\n",
    "  if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "  return layer\n",
    "\n",
    "annotated_model = tf.keras.models.clone_model(\n",
    "    keras_baselineModel_activation_model,\n",
    "    clone_function=apply_quantization_to_conv,\n",
    ")\n",
    "###\n",
    "\n",
    "### all Layers\n",
    "# def annotate_all_layers(layer):\n",
    "#     return quantize_annotate_layer(layer)\n",
    "\n",
    "# annotated_model_debug = tf.keras.models.clone_model(\n",
    "#     keras_baselineModel_activation_model,\n",
    "#     clone_function=annotate_all_layers,\n",
    "# )\n",
    "###\n",
    "\n",
    "\n",
    "with quantize_scope(\n",
    "    {'LinearSpecLayer': LinearSpecLayer,\n",
    "    'GlobalLogExpPooling2D': GlobalLogExpPooling2D\n",
    "    }):\n",
    "    quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model_debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6373ad3c-9cad-4d33-aef0-ed921fb956fc",
   "metadata": {},
   "source": [
    "##### No matter if we just use the custom layers, all layers, or only Conv layers and additionally given own layer code, own tfmot.quantization.keras.QuantizeConfig seems to be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc5742f0-7f1e-47a9-8f4c-ccf5cef6a5e9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: InputLayer, Class: <class 'keras.src.engine.input_layer.InputLayer'>\n",
      "Layer: QuantizeAnnotate, Class: <class 'tensorflow_model_optimization.python.core.quantization.keras.quantize_annotate.QuantizeAnnotate'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: MaxPooling2D, Class: <class 'keras.src.layers.pooling.max_pooling2d.MaxPooling2D'>\n",
      "Layer: AveragePooling2D, Class: <class 'keras.src.layers.pooling.average_pooling2d.AveragePooling2D'>\n",
      "Layer: Concatenate, Class: <class 'keras.src.layers.merging.concatenate.Concatenate'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Add, Class: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Add, Class: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Add, Class: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Add, Class: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Add, Class: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: DepthwiseConv2D, Class: <class 'keras.src.layers.convolutional.depthwise_conv2d.DepthwiseConv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: GlobalAveragePooling2D, Class: <class 'keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D'>\n",
      "Layer: Reshape, Class: <class 'keras.src.layers.reshaping.reshape.Reshape'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Multiply, Class: <class 'keras.src.layers.merging.multiply.Multiply'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: DepthwiseConv2D, Class: <class 'keras.src.layers.convolutional.depthwise_conv2d.DepthwiseConv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: GlobalAveragePooling2D, Class: <class 'keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D'>\n",
      "Layer: Reshape, Class: <class 'keras.src.layers.reshaping.reshape.Reshape'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Multiply, Class: <class 'keras.src.layers.merging.multiply.Multiply'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Add, Class: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: DepthwiseConv2D, Class: <class 'keras.src.layers.convolutional.depthwise_conv2d.DepthwiseConv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: GlobalAveragePooling2D, Class: <class 'keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D'>\n",
      "Layer: Reshape, Class: <class 'keras.src.layers.reshaping.reshape.Reshape'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Multiply, Class: <class 'keras.src.layers.merging.multiply.Multiply'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Add, Class: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: DepthwiseConv2D, Class: <class 'keras.src.layers.convolutional.depthwise_conv2d.DepthwiseConv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: GlobalAveragePooling2D, Class: <class 'keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D'>\n",
      "Layer: Reshape, Class: <class 'keras.src.layers.reshaping.reshape.Reshape'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Multiply, Class: <class 'keras.src.layers.merging.multiply.Multiply'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Add, Class: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: DepthwiseConv2D, Class: <class 'keras.src.layers.convolutional.depthwise_conv2d.DepthwiseConv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: GlobalAveragePooling2D, Class: <class 'keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D'>\n",
      "Layer: Reshape, Class: <class 'keras.src.layers.reshaping.reshape.Reshape'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Multiply, Class: <class 'keras.src.layers.merging.multiply.Multiply'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Add, Class: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: DepthwiseConv2D, Class: <class 'keras.src.layers.convolutional.depthwise_conv2d.DepthwiseConv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: GlobalAveragePooling2D, Class: <class 'keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D'>\n",
      "Layer: Reshape, Class: <class 'keras.src.layers.reshaping.reshape.Reshape'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Multiply, Class: <class 'keras.src.layers.merging.multiply.Multiply'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: DepthwiseConv2D, Class: <class 'keras.src.layers.convolutional.depthwise_conv2d.DepthwiseConv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: GlobalAveragePooling2D, Class: <class 'keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D'>\n",
      "Layer: Reshape, Class: <class 'keras.src.layers.reshaping.reshape.Reshape'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Multiply, Class: <class 'keras.src.layers.merging.multiply.Multiply'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Add, Class: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: DepthwiseConv2D, Class: <class 'keras.src.layers.convolutional.depthwise_conv2d.DepthwiseConv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: GlobalAveragePooling2D, Class: <class 'keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D'>\n",
      "Layer: Reshape, Class: <class 'keras.src.layers.reshaping.reshape.Reshape'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Multiply, Class: <class 'keras.src.layers.merging.multiply.Multiply'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Add, Class: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: DepthwiseConv2D, Class: <class 'keras.src.layers.convolutional.depthwise_conv2d.DepthwiseConv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: GlobalAveragePooling2D, Class: <class 'keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D'>\n",
      "Layer: Reshape, Class: <class 'keras.src.layers.reshaping.reshape.Reshape'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: Multiply, Class: <class 'keras.src.layers.merging.multiply.Multiply'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Add, Class: <class 'keras.src.layers.merging.add.Add'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: Conv2D, Class: <class 'keras.src.layers.convolutional.conv2d.Conv2D'>\n",
      "Layer: BatchNormalization, Class: <class 'keras.src.layers.normalization.batch_normalization.BatchNormalization'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n",
      "Layer: QuantizeAnnotate, Class: <class 'tensorflow_model_optimization.python.core.quantization.keras.quantize_annotate.QuantizeAnnotate'>\n",
      "Layer: Dense, Class: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer: Dense, Class: <class 'keras.src.layers.core.dense.Dense'>\n",
      "Layer: Activation, Class: <class 'keras.src.layers.core.activation.Activation'>\n"
     ]
    }
   ],
   "source": [
    "for layer in annotated_model.layers:\n",
    "    print(f\"Layer: {type(layer).__name__}, Class: {layer.__class__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd753e2-8053-4c6c-bd0e-cd8b2805fbe6",
   "metadata": {},
   "source": [
    "## Add custom layer as custom objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62808df0-d738-4a11-9805-d932e6adda56",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final spec shape:Tensor(\"ADVANCED_SPEC1/ExpandDims:0\", shape=(None, 128, 513, 1), dtype=float32)\n",
      "final spec shape:Tensor(\"ADVANCED_SPEC1/ExpandDims:0\", shape=(None, 128, 513, 1), dtype=float32)\n",
      "final spec shape:Tensor(\"ADVANCED_SPEC1/ExpandDims:0\", shape=(None, 128, 513, 1), dtype=float32)\n",
      "final spec shape:Tensor(\"ADVANCED_SPEC1/ExpandDims:0\", shape=(None, 128, 513, 1), dtype=float32)\n",
      "final spec shape:Tensor(\"ADVANCED_SPEC1/ExpandDims:0\", shape=(None, 128, 513, 1), dtype=float32)\n",
      "final spec shape:Tensor(\"ADVANCED_SPEC1/ExpandDims:0\", shape=(None, 128, 513, 1), dtype=float32)\n",
      "Done\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " INPUT (InputLayer)          [(None, 144000)]             0         []                            \n",
      "                                                                                                  \n",
      " ADVANCED_SPEC1 (LinearSpec  (None, 128, 513, 1)          1         ['INPUT[0][0]']               \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " BNORM_SPEC_NOQUANT (BatchN  (None, 128, 513, 1)          4         ['ADVANCED_SPEC1[1][0]']      \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " quant_CONV_0 (QuantizeWrap  (None, 64, 257, 30)          1023      ['BNORM_SPEC_NOQUANT[1][0]']  \n",
      " perV2)                                                                                           \n",
      "                                                                                                  \n",
      " BNORM_0 (BatchNormalizatio  (None, 64, 257, 30)          120       ['quant_CONV_0[0][0]']        \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " ACT_0 (Activation)          (None, 64, 257, 30)          0         ['BNORM_0[1][0]']             \n",
      "                                                                                                  \n",
      " pool_0_MAX (MaxPooling2D)   (None, 64, 128, 30)          0         ['ACT_0[1][0]']               \n",
      "                                                                                                  \n",
      " pool_0_AVG (AveragePooling  (None, 64, 128, 30)          0         ['ACT_0[1][0]']               \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " pool_0_CONCAT (Concatenate  (None, 64, 128, 60)          0         ['pool_0_MAX[1][0]',          \n",
      " )                                                                   'pool_0_AVG[1][0]']          \n",
      "                                                                                                  \n",
      " quant_pool_0_ACT_QUANT (Qu  (None, 64, 128, 60)          1         ['pool_0_CONCAT[1][0]']       \n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " quant_pool_0_CONV (Quantiz  (None, 64, 128, 30)          1893      ['quant_pool_0_ACT_QUANT[0][0]\n",
      " eWrapperV2)                                                        ']                            \n",
      "                                                                                                  \n",
      " quant_BLOCK_1-1_CONV_1 (Qu  (None, 32, 64, 60)           16323     ['quant_pool_0_CONV[0][0]']   \n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " BLOCK_1-1_BN_1 (BatchNorma  (None, 32, 64, 60)           240       ['quant_BLOCK_1-1_CONV_1[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " BLOCK_1-1_ACT_1 (Activatio  (None, 32, 64, 60)           0         ['BLOCK_1-1_BN_1[1][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " quant_BLOCK_1-1_ACT_QUANT   (None, 32, 64, 60)           1         ['BLOCK_1-1_ACT_1[1][0]']     \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_BLOCK_1-1_CONV_3 (Qu  (None, 32, 64, 60)           3723      ['quant_BLOCK_1-1_ACT_QUANT[0]\n",
      " antizeWrapperV2)                                                   [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_1-1_BN_3 (BatchNorma  (None, 32, 64, 60)           240       ['quant_BLOCK_1-1_CONV_3[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " quant_BLOCK_1-2_CONV_1 (Qu  (None, 32, 64, 60)           32523     ['BLOCK_1-1_BN_3[1][0]']      \n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " BLOCK_1-2_BN_1 (BatchNorma  (None, 32, 64, 60)           240       ['quant_BLOCK_1-2_CONV_1[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " BLOCK_1-2_ACT_1 (Activatio  (None, 32, 64, 60)           0         ['BLOCK_1-2_BN_1[1][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " quant_BLOCK_1-2_ACT_QUANT   (None, 32, 64, 60)           1         ['BLOCK_1-2_ACT_1[1][0]']     \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_BLOCK_1-2_CONV_3 (Qu  (None, 32, 64, 60)           3723      ['quant_BLOCK_1-2_ACT_QUANT[0]\n",
      " antizeWrapperV2)                                                   [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_1-2_BN_3 (BatchNorma  (None, 32, 64, 60)           240       ['quant_BLOCK_1-2_CONV_3[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " quant_BLOCK_1-2_ADD (Quant  (None, 32, 64, 60)           3         ['BLOCK_1-2_BN_3[1][0]',      \n",
      " izeWrapperV2)                                                       'BLOCK_1-1_BN_3[1][0]']      \n",
      "                                                                                                  \n",
      " quant_BLOCK_1-3_CONV_1 (Qu  (None, 32, 64, 60)           32523     ['quant_BLOCK_1-2_ADD[0][0]'] \n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " BLOCK_1-3_BN_1 (BatchNorma  (None, 32, 64, 60)           240       ['quant_BLOCK_1-3_CONV_1[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " BLOCK_1-3_ACT_1 (Activatio  (None, 32, 64, 60)           0         ['BLOCK_1-3_BN_1[1][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " quant_BLOCK_1-3_ACT_QUANT   (None, 32, 64, 60)           1         ['BLOCK_1-3_ACT_1[1][0]']     \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_BLOCK_1-3_CONV_3 (Qu  (None, 32, 64, 60)           3723      ['quant_BLOCK_1-3_ACT_QUANT[0]\n",
      " antizeWrapperV2)                                                   [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_1-3_BN_3 (BatchNorma  (None, 32, 64, 60)           240       ['quant_BLOCK_1-3_CONV_3[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " quant_BLOCK_1-3_ADD (Quant  (None, 32, 64, 60)           3         ['BLOCK_1-3_BN_3[1][0]',      \n",
      " izeWrapperV2)                                                       'quant_BLOCK_1-2_ADD[0][0]'] \n",
      "                                                                                                  \n",
      " quant_BLOCK_2-1_CONV_1 (Qu  (None, 16, 32, 240)          130083    ['quant_BLOCK_1-3_ADD[0][0]'] \n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " BLOCK_2-1_BN_1 (BatchNorma  (None, 16, 32, 240)          960       ['quant_BLOCK_2-1_CONV_1[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " BLOCK_2-1_ACT_1 (Activatio  (None, 16, 32, 240)          0         ['BLOCK_2-1_BN_1[1][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " quant_BLOCK_2-1_ACT_QUANT   (None, 16, 32, 240)          1         ['BLOCK_2-1_ACT_1[1][0]']     \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_BLOCK_2-1_CONV_3 (Qu  (None, 16, 32, 120)          29043     ['quant_BLOCK_2-1_ACT_QUANT[0]\n",
      " antizeWrapperV2)                                                   [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_2-1_BN_3 (BatchNorma  (None, 16, 32, 120)          480       ['quant_BLOCK_2-1_CONV_3[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " quant_BLOCK_2-2_CONV_1 (Qu  (None, 16, 32, 240)          259683    ['BLOCK_2-1_BN_3[1][0]']      \n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " BLOCK_2-2_BN_1 (BatchNorma  (None, 16, 32, 240)          960       ['quant_BLOCK_2-2_CONV_1[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " BLOCK_2-2_ACT_1 (Activatio  (None, 16, 32, 240)          0         ['BLOCK_2-2_BN_1[1][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " quant_BLOCK_2-2_ACT_QUANT   (None, 16, 32, 240)          1         ['BLOCK_2-2_ACT_1[1][0]']     \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_BLOCK_2-2_CONV_3 (Qu  (None, 16, 32, 120)          29043     ['quant_BLOCK_2-2_ACT_QUANT[0]\n",
      " antizeWrapperV2)                                                   [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_2-2_BN_3 (BatchNorma  (None, 16, 32, 120)          480       ['quant_BLOCK_2-2_CONV_3[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " quant_BLOCK_2-2_ADD (Quant  (None, 16, 32, 120)          3         ['BLOCK_2-2_BN_3[1][0]',      \n",
      " izeWrapperV2)                                                       'BLOCK_2-1_BN_3[1][0]']      \n",
      "                                                                                                  \n",
      " quant_BLOCK_2-3_CONV_1 (Qu  (None, 16, 32, 240)          259683    ['quant_BLOCK_2-2_ADD[0][0]'] \n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " BLOCK_2-3_BN_1 (BatchNorma  (None, 16, 32, 240)          960       ['quant_BLOCK_2-3_CONV_1[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " BLOCK_2-3_ACT_1 (Activatio  (None, 16, 32, 240)          0         ['BLOCK_2-3_BN_1[1][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " quant_BLOCK_2-3_ACT_QUANT   (None, 16, 32, 240)          1         ['BLOCK_2-3_ACT_1[1][0]']     \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_BLOCK_2-3_CONV_3 (Qu  (None, 16, 32, 120)          29043     ['quant_BLOCK_2-3_ACT_QUANT[0]\n",
      " antizeWrapperV2)                                                   [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_2-3_BN_3 (BatchNorma  (None, 16, 32, 120)          480       ['quant_BLOCK_2-3_CONV_3[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " quant_BLOCK_2-3_ADD (Quant  (None, 16, 32, 120)          3         ['BLOCK_2-3_BN_3[1][0]',      \n",
      " izeWrapperV2)                                                       'quant_BLOCK_2-2_ADD[0][0]'] \n",
      "                                                                                                  \n",
      " quant_BLOCK_2-4_CONV_1 (Qu  (None, 16, 32, 240)          259683    ['quant_BLOCK_2-3_ADD[0][0]'] \n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " BLOCK_2-4_BN_1 (BatchNorma  (None, 16, 32, 240)          960       ['quant_BLOCK_2-4_CONV_1[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " BLOCK_2-4_ACT_1 (Activatio  (None, 16, 32, 240)          0         ['BLOCK_2-4_BN_1[1][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " quant_BLOCK_2-4_ACT_QUANT   (None, 16, 32, 240)          1         ['BLOCK_2-4_ACT_1[1][0]']     \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_BLOCK_2-4_CONV_3 (Qu  (None, 16, 32, 120)          29043     ['quant_BLOCK_2-4_ACT_QUANT[0]\n",
      " antizeWrapperV2)                                                   [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_2-4_BN_3 (BatchNorma  (None, 16, 32, 120)          480       ['quant_BLOCK_2-4_CONV_3[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " quant_BLOCK_2-4_ADD (Quant  (None, 16, 32, 120)          3         ['BLOCK_2-4_BN_3[1][0]',      \n",
      " izeWrapperV2)                                                       'quant_BLOCK_2-3_ADD[0][0]'] \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-1_CONV_1 (Qu  (None, 16, 32, 640)          78083     ['quant_BLOCK_2-4_ADD[0][0]'] \n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " BLOCK_3-1_BN_1 (BatchNorma  (None, 16, 32, 640)          2560      ['quant_BLOCK_3-1_CONV_1[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " BLOCK_3-1_ACT_1 (Activatio  (None, 16, 32, 640)          0         ['BLOCK_3-1_BN_1[1][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " BLOCK_3-1_CONV_2 (Depthwis  (None, 8, 16, 640)           5760      ['BLOCK_3-1_ACT_1[1][0]']     \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " BLOCK_3-1_BN_2_NOQUANT (Ba  (None, 8, 16, 640)           2560      ['BLOCK_3-1_CONV_2[1][0]']    \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " BLOCK_3-1_ACT_2 (Activatio  (None, 8, 16, 640)           0         ['BLOCK_3-1_BN_2_NOQUANT[1][0]\n",
      " n)                                                                 ']                            \n",
      "                                                                                                  \n",
      " BLOCK_3-1_SE_AVG_POOL_1 (G  (None, 640)                  0         ['BLOCK_3-1_ACT_2[1][0]']     \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-1_SE_RESHAPE  (None, 1, 1, 640)            1         ['BLOCK_3-1_SE_AVG_POOL_1[1][0\n",
      "  (QuantizeWrapperV2)                                               ]']                           \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-1_SE_CONV_1   (None, 1, 1, 40)             25683     ['quant_BLOCK_3-1_SE_RESHAPE[0\n",
      " (QuantizeWrapperV2)                                                ][0]']                        \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-1_SE_CONV_2   (None, 1, 1, 640)            26883     ['quant_BLOCK_3-1_SE_CONV_1[0]\n",
      " (QuantizeWrapperV2)                                                [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_3-1_MULTIPLY (Multip  (None, 8, 16, 640)           0         ['BLOCK_3-1_ACT_2[1][0]',     \n",
      " ly)                                                                 'quant_BLOCK_3-1_SE_CONV_2[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-1_ACT_QUANT   (None, 8, 16, 640)           1         ['BLOCK_3-1_MULTIPLY[1][0]']  \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-1_CONV_3 (Qu  (None, 8, 16, 160)           102723    ['quant_BLOCK_3-1_ACT_QUANT[0]\n",
      " antizeWrapperV2)                                                   [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_3-1_BN_3 (BatchNorma  (None, 8, 16, 160)           640       ['quant_BLOCK_3-1_CONV_3[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-2_CONV_1 (Qu  (None, 8, 16, 640)           103683    ['BLOCK_3-1_BN_3[1][0]']      \n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " BLOCK_3-2_BN_1 (BatchNorma  (None, 8, 16, 640)           2560      ['quant_BLOCK_3-2_CONV_1[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " BLOCK_3-2_ACT_1 (Activatio  (None, 8, 16, 640)           0         ['BLOCK_3-2_BN_1[1][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " BLOCK_3-2_CONV_2 (Depthwis  (None, 8, 16, 640)           5760      ['BLOCK_3-2_ACT_1[1][0]']     \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " BLOCK_3-2_BN_2_NOQUANT (Ba  (None, 8, 16, 640)           2560      ['BLOCK_3-2_CONV_2[1][0]']    \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " BLOCK_3-2_ACT_2 (Activatio  (None, 8, 16, 640)           0         ['BLOCK_3-2_BN_2_NOQUANT[1][0]\n",
      " n)                                                                 ']                            \n",
      "                                                                                                  \n",
      " BLOCK_3-2_SE_AVG_POOL_1 (G  (None, 640)                  0         ['BLOCK_3-2_ACT_2[1][0]']     \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-2_SE_RESHAPE  (None, 1, 1, 640)            1         ['BLOCK_3-2_SE_AVG_POOL_1[1][0\n",
      "  (QuantizeWrapperV2)                                               ]']                           \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-2_SE_CONV_1   (None, 1, 1, 40)             25683     ['quant_BLOCK_3-2_SE_RESHAPE[0\n",
      " (QuantizeWrapperV2)                                                ][0]']                        \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-2_SE_CONV_2   (None, 1, 1, 640)            26883     ['quant_BLOCK_3-2_SE_CONV_1[0]\n",
      " (QuantizeWrapperV2)                                                [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_3-2_MULTIPLY (Multip  (None, 8, 16, 640)           0         ['BLOCK_3-2_ACT_2[1][0]',     \n",
      " ly)                                                                 'quant_BLOCK_3-2_SE_CONV_2[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-2_ACT_QUANT   (None, 8, 16, 640)           1         ['BLOCK_3-2_MULTIPLY[1][0]']  \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-2_CONV_3 (Qu  (None, 8, 16, 160)           102723    ['quant_BLOCK_3-2_ACT_QUANT[0]\n",
      " antizeWrapperV2)                                                   [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_3-2_BN_3 (BatchNorma  (None, 8, 16, 160)           640       ['quant_BLOCK_3-2_CONV_3[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-2_ADD (Quant  (None, 8, 16, 160)           3         ['BLOCK_3-2_BN_3[1][0]',      \n",
      " izeWrapperV2)                                                       'BLOCK_3-1_BN_3[1][0]']      \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-3_CONV_1 (Qu  (None, 8, 16, 640)           103683    ['quant_BLOCK_3-2_ADD[0][0]'] \n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " BLOCK_3-3_BN_1 (BatchNorma  (None, 8, 16, 640)           2560      ['quant_BLOCK_3-3_CONV_1[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " BLOCK_3-3_ACT_1 (Activatio  (None, 8, 16, 640)           0         ['BLOCK_3-3_BN_1[1][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " BLOCK_3-3_CONV_2 (Depthwis  (None, 8, 16, 640)           5760      ['BLOCK_3-3_ACT_1[1][0]']     \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " BLOCK_3-3_BN_2_NOQUANT (Ba  (None, 8, 16, 640)           2560      ['BLOCK_3-3_CONV_2[1][0]']    \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " BLOCK_3-3_ACT_2 (Activatio  (None, 8, 16, 640)           0         ['BLOCK_3-3_BN_2_NOQUANT[1][0]\n",
      " n)                                                                 ']                            \n",
      "                                                                                                  \n",
      " BLOCK_3-3_SE_AVG_POOL_1 (G  (None, 640)                  0         ['BLOCK_3-3_ACT_2[1][0]']     \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-3_SE_RESHAPE  (None, 1, 1, 640)            1         ['BLOCK_3-3_SE_AVG_POOL_1[1][0\n",
      "  (QuantizeWrapperV2)                                               ]']                           \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-3_SE_CONV_1   (None, 1, 1, 40)             25683     ['quant_BLOCK_3-3_SE_RESHAPE[0\n",
      " (QuantizeWrapperV2)                                                ][0]']                        \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-3_SE_CONV_2   (None, 1, 1, 640)            26883     ['quant_BLOCK_3-3_SE_CONV_1[0]\n",
      " (QuantizeWrapperV2)                                                [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_3-3_MULTIPLY (Multip  (None, 8, 16, 640)           0         ['BLOCK_3-3_ACT_2[1][0]',     \n",
      " ly)                                                                 'quant_BLOCK_3-3_SE_CONV_2[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-3_ACT_QUANT   (None, 8, 16, 640)           1         ['BLOCK_3-3_MULTIPLY[1][0]']  \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-3_CONV_3 (Qu  (None, 8, 16, 160)           102723    ['quant_BLOCK_3-3_ACT_QUANT[0]\n",
      " antizeWrapperV2)                                                   [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_3-3_BN_3 (BatchNorma  (None, 8, 16, 160)           640       ['quant_BLOCK_3-3_CONV_3[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-3_ADD (Quant  (None, 8, 16, 160)           3         ['BLOCK_3-3_BN_3[1][0]',      \n",
      " izeWrapperV2)                                                       'quant_BLOCK_3-2_ADD[0][0]'] \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-4_CONV_1 (Qu  (None, 8, 16, 640)           103683    ['quant_BLOCK_3-3_ADD[0][0]'] \n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " BLOCK_3-4_BN_1 (BatchNorma  (None, 8, 16, 640)           2560      ['quant_BLOCK_3-4_CONV_1[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " BLOCK_3-4_ACT_1 (Activatio  (None, 8, 16, 640)           0         ['BLOCK_3-4_BN_1[1][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " BLOCK_3-4_CONV_2 (Depthwis  (None, 8, 16, 640)           5760      ['BLOCK_3-4_ACT_1[1][0]']     \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " BLOCK_3-4_BN_2_NOQUANT (Ba  (None, 8, 16, 640)           2560      ['BLOCK_3-4_CONV_2[1][0]']    \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " BLOCK_3-4_ACT_2 (Activatio  (None, 8, 16, 640)           0         ['BLOCK_3-4_BN_2_NOQUANT[1][0]\n",
      " n)                                                                 ']                            \n",
      "                                                                                                  \n",
      " BLOCK_3-4_SE_AVG_POOL_1 (G  (None, 640)                  0         ['BLOCK_3-4_ACT_2[1][0]']     \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-4_SE_RESHAPE  (None, 1, 1, 640)            1         ['BLOCK_3-4_SE_AVG_POOL_1[1][0\n",
      "  (QuantizeWrapperV2)                                               ]']                           \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-4_SE_CONV_1   (None, 1, 1, 40)             25683     ['quant_BLOCK_3-4_SE_RESHAPE[0\n",
      " (QuantizeWrapperV2)                                                ][0]']                        \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-4_SE_CONV_2   (None, 1, 1, 640)            26883     ['quant_BLOCK_3-4_SE_CONV_1[0]\n",
      " (QuantizeWrapperV2)                                                [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_3-4_MULTIPLY (Multip  (None, 8, 16, 640)           0         ['BLOCK_3-4_ACT_2[1][0]',     \n",
      " ly)                                                                 'quant_BLOCK_3-4_SE_CONV_2[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-4_ACT_QUANT   (None, 8, 16, 640)           1         ['BLOCK_3-4_MULTIPLY[1][0]']  \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-4_CONV_3 (Qu  (None, 8, 16, 160)           102723    ['quant_BLOCK_3-4_ACT_QUANT[0]\n",
      " antizeWrapperV2)                                                   [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_3-4_BN_3 (BatchNorma  (None, 8, 16, 160)           640       ['quant_BLOCK_3-4_CONV_3[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-4_ADD (Quant  (None, 8, 16, 160)           3         ['BLOCK_3-4_BN_3[1][0]',      \n",
      " izeWrapperV2)                                                       'quant_BLOCK_3-3_ADD[0][0]'] \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-5_CONV_1 (Qu  (None, 8, 16, 640)           103683    ['quant_BLOCK_3-4_ADD[0][0]'] \n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " BLOCK_3-5_BN_1 (BatchNorma  (None, 8, 16, 640)           2560      ['quant_BLOCK_3-5_CONV_1[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " BLOCK_3-5_ACT_1 (Activatio  (None, 8, 16, 640)           0         ['BLOCK_3-5_BN_1[1][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " BLOCK_3-5_CONV_2 (Depthwis  (None, 8, 16, 640)           5760      ['BLOCK_3-5_ACT_1[1][0]']     \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " BLOCK_3-5_BN_2_NOQUANT (Ba  (None, 8, 16, 640)           2560      ['BLOCK_3-5_CONV_2[1][0]']    \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " BLOCK_3-5_ACT_2 (Activatio  (None, 8, 16, 640)           0         ['BLOCK_3-5_BN_2_NOQUANT[1][0]\n",
      " n)                                                                 ']                            \n",
      "                                                                                                  \n",
      " BLOCK_3-5_SE_AVG_POOL_1 (G  (None, 640)                  0         ['BLOCK_3-5_ACT_2[1][0]']     \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-5_SE_RESHAPE  (None, 1, 1, 640)            1         ['BLOCK_3-5_SE_AVG_POOL_1[1][0\n",
      "  (QuantizeWrapperV2)                                               ]']                           \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-5_SE_CONV_1   (None, 1, 1, 40)             25683     ['quant_BLOCK_3-5_SE_RESHAPE[0\n",
      " (QuantizeWrapperV2)                                                ][0]']                        \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-5_SE_CONV_2   (None, 1, 1, 640)            26883     ['quant_BLOCK_3-5_SE_CONV_1[0]\n",
      " (QuantizeWrapperV2)                                                [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_3-5_MULTIPLY (Multip  (None, 8, 16, 640)           0         ['BLOCK_3-5_ACT_2[1][0]',     \n",
      " ly)                                                                 'quant_BLOCK_3-5_SE_CONV_2[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-5_ACT_QUANT   (None, 8, 16, 640)           1         ['BLOCK_3-5_MULTIPLY[1][0]']  \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-5_CONV_3 (Qu  (None, 8, 16, 160)           102723    ['quant_BLOCK_3-5_ACT_QUANT[0]\n",
      " antizeWrapperV2)                                                   [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_3-5_BN_3 (BatchNorma  (None, 8, 16, 160)           640       ['quant_BLOCK_3-5_CONV_3[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " quant_BLOCK_3-5_ADD (Quant  (None, 8, 16, 160)           3         ['BLOCK_3-5_BN_3[1][0]',      \n",
      " izeWrapperV2)                                                       'quant_BLOCK_3-4_ADD[0][0]'] \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-1_CONV_1 (Qu  (None, 8, 16, 1120)          181443    ['quant_BLOCK_3-5_ADD[0][0]'] \n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " BLOCK_4-1_BN_1 (BatchNorma  (None, 8, 16, 1120)          4480      ['quant_BLOCK_4-1_CONV_1[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " BLOCK_4-1_ACT_1 (Activatio  (None, 8, 16, 1120)          0         ['BLOCK_4-1_BN_1[1][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " BLOCK_4-1_CONV_2 (Depthwis  (None, 4, 8, 1120)           10080     ['BLOCK_4-1_ACT_1[1][0]']     \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " BLOCK_4-1_BN_2_NOQUANT (Ba  (None, 4, 8, 1120)           4480      ['BLOCK_4-1_CONV_2[1][0]']    \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " BLOCK_4-1_ACT_2 (Activatio  (None, 4, 8, 1120)           0         ['BLOCK_4-1_BN_2_NOQUANT[1][0]\n",
      " n)                                                                 ']                            \n",
      "                                                                                                  \n",
      " BLOCK_4-1_SE_AVG_POOL_1 (G  (None, 1120)                 0         ['BLOCK_4-1_ACT_2[1][0]']     \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-1_SE_RESHAPE  (None, 1, 1, 1120)           1         ['BLOCK_4-1_SE_AVG_POOL_1[1][0\n",
      "  (QuantizeWrapperV2)                                               ]']                           \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-1_SE_CONV_1   (None, 1, 1, 70)             78543     ['quant_BLOCK_4-1_SE_RESHAPE[0\n",
      " (QuantizeWrapperV2)                                                ][0]']                        \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-1_SE_CONV_2   (None, 1, 1, 1120)           80643     ['quant_BLOCK_4-1_SE_CONV_1[0]\n",
      " (QuantizeWrapperV2)                                                [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_4-1_MULTIPLY (Multip  (None, 4, 8, 1120)           0         ['BLOCK_4-1_ACT_2[1][0]',     \n",
      " ly)                                                                 'quant_BLOCK_4-1_SE_CONV_2[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-1_ACT_QUANT   (None, 4, 8, 1120)           1         ['BLOCK_4-1_MULTIPLY[1][0]']  \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-1_CONV_3 (Qu  (None, 4, 8, 280)            314163    ['quant_BLOCK_4-1_ACT_QUANT[0]\n",
      " antizeWrapperV2)                                                   [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_4-1_BN_3 (BatchNorma  (None, 4, 8, 280)            1120      ['quant_BLOCK_4-1_CONV_3[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-2_CONV_1 (Qu  (None, 4, 8, 1120)           315843    ['BLOCK_4-1_BN_3[1][0]']      \n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " BLOCK_4-2_BN_1 (BatchNorma  (None, 4, 8, 1120)           4480      ['quant_BLOCK_4-2_CONV_1[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " BLOCK_4-2_ACT_1 (Activatio  (None, 4, 8, 1120)           0         ['BLOCK_4-2_BN_1[1][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " BLOCK_4-2_CONV_2 (Depthwis  (None, 4, 8, 1120)           10080     ['BLOCK_4-2_ACT_1[1][0]']     \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " BLOCK_4-2_BN_2_NOQUANT (Ba  (None, 4, 8, 1120)           4480      ['BLOCK_4-2_CONV_2[1][0]']    \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " BLOCK_4-2_ACT_2 (Activatio  (None, 4, 8, 1120)           0         ['BLOCK_4-2_BN_2_NOQUANT[1][0]\n",
      " n)                                                                 ']                            \n",
      "                                                                                                  \n",
      " BLOCK_4-2_SE_AVG_POOL_1 (G  (None, 1120)                 0         ['BLOCK_4-2_ACT_2[1][0]']     \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-2_SE_RESHAPE  (None, 1, 1, 1120)           1         ['BLOCK_4-2_SE_AVG_POOL_1[1][0\n",
      "  (QuantizeWrapperV2)                                               ]']                           \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-2_SE_CONV_1   (None, 1, 1, 70)             78543     ['quant_BLOCK_4-2_SE_RESHAPE[0\n",
      " (QuantizeWrapperV2)                                                ][0]']                        \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-2_SE_CONV_2   (None, 1, 1, 1120)           80643     ['quant_BLOCK_4-2_SE_CONV_1[0]\n",
      " (QuantizeWrapperV2)                                                [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_4-2_MULTIPLY (Multip  (None, 4, 8, 1120)           0         ['BLOCK_4-2_ACT_2[1][0]',     \n",
      " ly)                                                                 'quant_BLOCK_4-2_SE_CONV_2[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-2_ACT_QUANT   (None, 4, 8, 1120)           1         ['BLOCK_4-2_MULTIPLY[1][0]']  \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-2_CONV_3 (Qu  (None, 4, 8, 280)            314163    ['quant_BLOCK_4-2_ACT_QUANT[0]\n",
      " antizeWrapperV2)                                                   [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_4-2_BN_3 (BatchNorma  (None, 4, 8, 280)            1120      ['quant_BLOCK_4-2_CONV_3[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-2_ADD (Quant  (None, 4, 8, 280)            3         ['BLOCK_4-2_BN_3[1][0]',      \n",
      " izeWrapperV2)                                                       'BLOCK_4-1_BN_3[1][0]']      \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-3_CONV_1 (Qu  (None, 4, 8, 1120)           315843    ['quant_BLOCK_4-2_ADD[0][0]'] \n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " BLOCK_4-3_BN_1 (BatchNorma  (None, 4, 8, 1120)           4480      ['quant_BLOCK_4-3_CONV_1[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " BLOCK_4-3_ACT_1 (Activatio  (None, 4, 8, 1120)           0         ['BLOCK_4-3_BN_1[1][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " BLOCK_4-3_CONV_2 (Depthwis  (None, 4, 8, 1120)           10080     ['BLOCK_4-3_ACT_1[1][0]']     \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " BLOCK_4-3_BN_2_NOQUANT (Ba  (None, 4, 8, 1120)           4480      ['BLOCK_4-3_CONV_2[1][0]']    \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " BLOCK_4-3_ACT_2 (Activatio  (None, 4, 8, 1120)           0         ['BLOCK_4-3_BN_2_NOQUANT[1][0]\n",
      " n)                                                                 ']                            \n",
      "                                                                                                  \n",
      " BLOCK_4-3_SE_AVG_POOL_1 (G  (None, 1120)                 0         ['BLOCK_4-3_ACT_2[1][0]']     \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-3_SE_RESHAPE  (None, 1, 1, 1120)           1         ['BLOCK_4-3_SE_AVG_POOL_1[1][0\n",
      "  (QuantizeWrapperV2)                                               ]']                           \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-3_SE_CONV_1   (None, 1, 1, 70)             78543     ['quant_BLOCK_4-3_SE_RESHAPE[0\n",
      " (QuantizeWrapperV2)                                                ][0]']                        \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-3_SE_CONV_2   (None, 1, 1, 1120)           80643     ['quant_BLOCK_4-3_SE_CONV_1[0]\n",
      " (QuantizeWrapperV2)                                                [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_4-3_MULTIPLY (Multip  (None, 4, 8, 1120)           0         ['BLOCK_4-3_ACT_2[1][0]',     \n",
      " ly)                                                                 'quant_BLOCK_4-3_SE_CONV_2[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-3_ACT_QUANT   (None, 4, 8, 1120)           1         ['BLOCK_4-3_MULTIPLY[1][0]']  \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-3_CONV_3 (Qu  (None, 4, 8, 280)            314163    ['quant_BLOCK_4-3_ACT_QUANT[0]\n",
      " antizeWrapperV2)                                                   [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_4-3_BN_3 (BatchNorma  (None, 4, 8, 280)            1120      ['quant_BLOCK_4-3_CONV_3[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-3_ADD (Quant  (None, 4, 8, 280)            3         ['BLOCK_4-3_BN_3[1][0]',      \n",
      " izeWrapperV2)                                                       'quant_BLOCK_4-2_ADD[0][0]'] \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-4_CONV_1 (Qu  (None, 4, 8, 1120)           315843    ['quant_BLOCK_4-3_ADD[0][0]'] \n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " BLOCK_4-4_BN_1 (BatchNorma  (None, 4, 8, 1120)           4480      ['quant_BLOCK_4-4_CONV_1[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " BLOCK_4-4_ACT_1 (Activatio  (None, 4, 8, 1120)           0         ['BLOCK_4-4_BN_1[1][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " BLOCK_4-4_CONV_2 (Depthwis  (None, 4, 8, 1120)           10080     ['BLOCK_4-4_ACT_1[1][0]']     \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " BLOCK_4-4_BN_2_NOQUANT (Ba  (None, 4, 8, 1120)           4480      ['BLOCK_4-4_CONV_2[1][0]']    \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " BLOCK_4-4_ACT_2 (Activatio  (None, 4, 8, 1120)           0         ['BLOCK_4-4_BN_2_NOQUANT[1][0]\n",
      " n)                                                                 ']                            \n",
      "                                                                                                  \n",
      " BLOCK_4-4_SE_AVG_POOL_1 (G  (None, 1120)                 0         ['BLOCK_4-4_ACT_2[1][0]']     \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-4_SE_RESHAPE  (None, 1, 1, 1120)           1         ['BLOCK_4-4_SE_AVG_POOL_1[1][0\n",
      "  (QuantizeWrapperV2)                                               ]']                           \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-4_SE_CONV_1   (None, 1, 1, 70)             78543     ['quant_BLOCK_4-4_SE_RESHAPE[0\n",
      " (QuantizeWrapperV2)                                                ][0]']                        \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-4_SE_CONV_2   (None, 1, 1, 1120)           80643     ['quant_BLOCK_4-4_SE_CONV_1[0]\n",
      " (QuantizeWrapperV2)                                                [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_4-4_MULTIPLY (Multip  (None, 4, 8, 1120)           0         ['BLOCK_4-4_ACT_2[1][0]',     \n",
      " ly)                                                                 'quant_BLOCK_4-4_SE_CONV_2[0]\n",
      "                                                                    [0]']                         \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-4_ACT_QUANT   (None, 4, 8, 1120)           1         ['BLOCK_4-4_MULTIPLY[1][0]']  \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_BLOCK_4-4_CONV_3 (Qu  (None, 4, 8, 280)            314163    ['quant_BLOCK_4-4_ACT_QUANT[0]\n",
      " antizeWrapperV2)                                                   [0]']                         \n",
      "                                                                                                  \n",
      " BLOCK_4-4_BN_3 (BatchNorma  (None, 4, 8, 280)            1120      ['quant_BLOCK_4-4_CONV_3[0][0]\n",
      " lization)                                                          ']                            \n",
      "                                                                                                  \n",
      " BLOCK_4-4_ADD (Add)         (None, 4, 8, 280)            0         ['BLOCK_4-4_BN_3[1][0]',      \n",
      "                                                                     'quant_BLOCK_4-3_ADD[0][0]'] \n",
      "                                                                                                  \n",
      " BNORM_POST_NOQUANT (BatchN  (None, 4, 8, 280)            1120      ['BLOCK_4-4_ADD[1][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " quant_ACT_POST (QuantizeWr  (None, 4, 8, 280)            3         ['BNORM_POST_NOQUANT[1][0]']  \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_POST_CONV_1 (Quantiz  (None, 2, 6, 420)            1059243   ['quant_ACT_POST[0][0]']      \n",
      " eWrapperV2)                                                                                      \n",
      "                                                                                                  \n",
      " POST_BN_1 (BatchNormalizat  (None, 2, 6, 420)            1680      ['quant_POST_CONV_1[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " POST_ACT_1 (Activation)     (None, 2, 6, 420)            0         ['POST_BN_1[1][0]']           \n",
      "                                                                                                  \n",
      " GLOBAL_LME_POOL (GlobalLog  (None, 420)                  1         ['POST_ACT_1[1][0]']          \n",
      " ExpPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 2)                    842       ['GLOBAL_LME_POOL[1][0]']     \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 2)                    0         ['dense[1][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6620899 (25.26 MB)\n",
      "Trainable params: 6432202 (24.54 MB)\n",
      "Non-trainable params: 188697 (737.10 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from model import LinearSpecLayer, GlobalLogExpPooling2D\n",
    "\n",
    "keras_baselineModel_activation_path = \"/home/jovyan/models/checkpoints_/baseline_two_class_model_softmax_activation/\"\n",
    "keras_baselineModel_activation_model = keras.models.load_model(keras_baselineModel_activation_path)\n",
    "\n",
    "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
    "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
    "\n",
    "\n",
    "### just conv2D layers\n",
    "def apply_quantization_to_conv(layer):\n",
    "  if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "  return layer\n",
    "\n",
    "annotated_model = tf.keras.models.clone_model(\n",
    "    keras_baselineModel_activation_model,\n",
    "    clone_function=apply_quantization_to_conv,\n",
    ")\n",
    "\n",
    "custom_objects = {\n",
    "    'LinearSpecLayer': LinearSpecLayer,\n",
    "    'GlobalLogExpPooling2D': GlobalLogExpPooling2D\n",
    "}\n",
    "keras.utils.get_custom_objects().update(custom_objects)\n",
    "\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5149b57-2ac2-401d-a4e0-c1e60a7d1ea6",
   "metadata": {},
   "source": [
    "#### all Conv Layer have now a QuantizeWrapperV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367cdf90-db7b-4519-a7b2-5e13d3322bcb",
   "metadata": {},
   "source": [
    "## Train the quant aware Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099d00de-ea1a-4e43-b8f3-8315b200b08b",
   "metadata": {},
   "source": [
    "### Load Training data and balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58699682-117b-4c41-83f1-fcfb60cb50e1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Processed 0 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.920_0001_127764.wav\n",
      "Processed 100 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.920_0003_82178411.wav\n",
      "Processed 200 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.920_0006_718263.wav\n",
      "Processed 300 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.920_0010_540556.wav\n",
      "Processed 400 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.920_0016_324707.wav\n",
      "Processed 500 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.920_0022_355153.wav\n",
      "Processed 600 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.920_0031_270434051.wav\n",
      "Processed 700 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.920_0043_647427.wav\n",
      "Processed 800 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.920_0076_36928.wav\n",
      "Processed 900 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.930_0002_472272481.wav\n",
      "Processed 1000 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.930_0005_552529.wav\n",
      "Processed 1100 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.930_0008_716296.wav\n",
      "Processed 1200 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.930_0013_121693.wav\n",
      "Processed 1300 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.930_0017_713037.wav\n",
      "Processed 1400 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.930_0026_247411901.wav\n",
      "Processed 1500 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.930_0038_723553.wav\n",
      "Processed 1600 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.930_0063_60212751.wav\n",
      "Processed 1700 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.940_0002_102741151.wav\n",
      "Processed 1800 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.940_0005_529329181.wav\n",
      "Processed 1900 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.940_0009_472003041.wav\n",
      "Processed 2000 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.940_0014_723553.wav\n",
      "Processed 2100 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.940_0020_523504.wav\n",
      "Processed 2200 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.940_0030_578424.wav\n",
      "Processed 2300 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.940_0046_722476.wav\n",
      "Processed 2400 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.950_0002_134047.wav\n",
      "Processed 2500 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.950_0006_153920411.wav\n",
      "Processed 2600 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.950_0012_171486.wav\n",
      "Processed 2700 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.950_0020_489618321.wav\n",
      "Processed 2800 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.950_0037_461903821.wav\n",
      "Processed 2900 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.950_0087_140045.wav\n",
      "Processed 3000 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.960_0005_463339521.wav\n",
      "Processed 3100 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.960_0012_256683.wav\n",
      "Processed 3200 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.960_0028_386459751.wav\n",
      "Processed 3300 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.960_0103_310083.wav\n",
      "Processed 3400 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.970_0009_275634.wav\n",
      "Processed 3500 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.970_0025_82178411.wav\n",
      "Processed 3600 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.980_0003_474070721.wav\n",
      "Processed 3700 files. Currently processing file: /home/jovyan/cut-data/training/non_target/0.980_0025_474070721.wav\n",
      "Processed 3800 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0001_--yCGUs46y4_30.wav\n",
      "Processed 3900 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0001_-CUp_Tmg2Y0_30.wav\n",
      "Processed 4000 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0001_-Ub8LKPkhos_60.wav\n",
      "Processed 4100 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0001_-sRFfU8k0Zs_90.wav\n",
      "Processed 4200 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0001_0F3WEUPxWCM_0.wav\n",
      "Processed 4300 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0001_0fm0oU8FO0U_30.wav\n",
      "Processed 4400 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0001_1DxLwZhTj0A_30.wav\n",
      "Processed 4500 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0001_1kjnqM-ptrk_110.wav\n",
      "Processed 4600 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0001_2RpOd9MJjyQ_10.wav\n",
      "Processed 4700 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0001_35t2aAFoIW4_0.wav\n",
      "Processed 4800 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0001_4AllUpjCvI4_280.wav\n",
      "Processed 4900 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0001_619xf0ifoSQ_10.wav\n",
      "Processed 5000 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0001_zPhwKiiWzoA_30.wav\n",
      "Processed 5100 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0002_-7wUQP6G5EQ_30.wav\n",
      "Processed 5200 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0002_-KaRoM5LnHc_30.wav\n",
      "Processed 5300 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0002_-gfBxHm8lhg_90.wav\n",
      "Processed 5400 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0002_01xpKyI0rXA_180.wav\n",
      "Processed 5500 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0002_0QSZ9Fr1qeQ_30.wav\n",
      "Processed 5600 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0002_0qZ3tI4nAZE_6.wav\n",
      "Processed 5700 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0002_1PeBvAdub4w_370.wav\n",
      "Processed 5800 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0002_23Yu8RnEv3k_30.wav\n",
      "Processed 5900 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0002_2hyHI8DNWD4_200.wav\n",
      "Processed 6000 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0002_3Zt4Y3Jajrk_20.wav\n",
      "Processed 6100 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0002_551pSwSFXc0_30.wav\n",
      "Processed 6200 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0002_96vB5wR_J4w_290.wav\n",
      "Processed 6300 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0003_-5-vmt2iKT0_30.wav\n",
      "Processed 6400 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0003_-G3Ph35cTy0_30.wav\n",
      "Processed 6500 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0003_-_nPg0thx-s_410.wav\n",
      "Processed 6600 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0003_-xUhOdzwLtw_0.wav\n",
      "Processed 6700 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0003_0Lt4MP0-6Tc_590.wav\n",
      "Processed 6800 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0003_0m9CbkMad_k_150.wav\n",
      "Processed 6900 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0003_1OHxLcGzhv0_30.wav\n",
      "Processed 7000 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0003_1ya5YEWkTG0_490.wav\n",
      "Processed 7100 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0003_2_7jHXNdC_w_40.wav\n",
      "Processed 7200 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0003_3JLEt6u9CwY_240.wav\n",
      "Processed 7300 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0003_4M7jykYpnNU_0.wav\n",
      "Processed 7400 files. Currently processing file: /home/jovyan/cut-data/training/non_target/1.000_0003_6o6DsSnbpxE_60.wav\n",
      "Processed 0 files. Currently processing file: /home/jovyan/cut-data/training/target/0.390_0001_200622_1615_7.wav\n",
      "Processed 100 files. Currently processing file: /home/jovyan/cut-data/training/target/0.810_0001_200622_1605_7.wav\n",
      "Processed 200 files. Currently processing file: /home/jovyan/cut-data/training/target/0.920_0004_556978.wav\n",
      "Processed 300 files. Currently processing file: /home/jovyan/cut-data/training/target/0.920_0012_633369.wav\n",
      "Processed 400 files. Currently processing file: /home/jovyan/cut-data/training/target/0.920_0022_86290.wav\n",
      "Processed 500 files. Currently processing file: /home/jovyan/cut-data/training/target/0.920_0036_555437551.wav\n",
      "Processed 600 files. Currently processing file: /home/jovyan/cut-data/training/target/0.920_0052_R21_2022_02_25_09_55_07.wav\n",
      "Processed 700 files. Currently processing file: /home/jovyan/cut-data/training/target/0.920_0086_R21_2022_02_25_07_31_02.wav\n",
      "Processed 800 files. Currently processing file: /home/jovyan/cut-data/training/target/0.920_0164_R21_2022_02_23_08_14_49.wav\n",
      "Processed 900 files. Currently processing file: /home/jovyan/cut-data/training/target/0.930_0006_418056311.wav\n",
      "Processed 1000 files. Currently processing file: /home/jovyan/cut-data/training/target/0.930_0014_459920281.wav\n",
      "Processed 1100 files. Currently processing file: /home/jovyan/cut-data/training/target/0.930_0024_690624.wav\n",
      "Processed 1200 files. Currently processing file: /home/jovyan/cut-data/training/target/0.930_0040_517125591.wav\n",
      "Processed 1300 files. Currently processing file: /home/jovyan/cut-data/training/target/0.930_0063_R21_2022_02_22_07_49_45.wav\n",
      "Processed 1400 files. Currently processing file: /home/jovyan/cut-data/training/target/0.930_0097_R21_2022_02_25_08_22_06.wav\n",
      "Processed 1500 files. Currently processing file: /home/jovyan/cut-data/training/target/0.930_0240_R21_2022_02_23_08_14_49.wav\n",
      "Processed 1600 files. Currently processing file: /home/jovyan/cut-data/training/target/0.940_0006_676693.wav\n",
      "Processed 1700 files. Currently processing file: /home/jovyan/cut-data/training/target/0.940_0016_716129.wav\n",
      "Processed 1800 files. Currently processing file: /home/jovyan/cut-data/training/target/0.940_0030_27576.wav\n",
      "Processed 1900 files. Currently processing file: /home/jovyan/cut-data/training/target/0.940_0049_517127011.wav\n",
      "Processed 2000 files. Currently processing file: /home/jovyan/cut-data/training/target/0.940_0081_696775.wav\n",
      "Processed 2100 files. Currently processing file: /home/jovyan/cut-data/training/target/0.940_0126_555571531.wav\n",
      "Processed 2200 files. Currently processing file: /home/jovyan/cut-data/training/target/0.950_0002_420362.wav\n",
      "Processed 2300 files. Currently processing file: /home/jovyan/cut-data/training/target/0.950_0015_456706281.wav\n",
      "Processed 2400 files. Currently processing file: /home/jovyan/cut-data/training/target/0.950_0029_155008.wav\n",
      "Processed 2500 files. Currently processing file: /home/jovyan/cut-data/training/target/0.950_0050_558294.wav\n",
      "Processed 2600 files. Currently processing file: /home/jovyan/cut-data/training/target/0.950_0085_366916.wav\n",
      "Processed 2700 files. Currently processing file: /home/jovyan/cut-data/training/target/0.950_0170_558294.wav\n",
      "Processed 2800 files. Currently processing file: /home/jovyan/cut-data/training/target/0.960_0007_420362.wav\n",
      "Processed 2900 files. Currently processing file: /home/jovyan/cut-data/training/target/0.960_0023_384110291.wav\n",
      "Processed 3000 files. Currently processing file: /home/jovyan/cut-data/training/target/0.960_0045_R21_2022_02_25_07_31_02.wav\n",
      "Processed 3100 files. Currently processing file: /home/jovyan/cut-data/training/target/0.960_0078_R21_2022_02_25_08_22_06.wav\n",
      "Processed 3200 files. Currently processing file: /home/jovyan/cut-data/training/target/0.960_0118_R21_2022_02_23_07_59_47.wav\n",
      "Processed 3300 files. Currently processing file: /home/jovyan/cut-data/training/target/0.970_0006_556978.wav\n",
      "Processed 3400 files. Currently processing file: /home/jovyan/cut-data/training/target/0.970_0030_517125591.wav\n",
      "Processed 3500 files. Currently processing file: /home/jovyan/cut-data/training/target/0.970_0070_R21_2022_02_26_08_25_32.wav\n",
      "Processed 3600 files. Currently processing file: /home/jovyan/cut-data/training/target/0.970_0180_R21_2022_02_23_07_59_47.wav\n",
      "Processed 3700 files. Currently processing file: /home/jovyan/cut-data/training/target/0.980_0032_R21_2022_02_23_07_59_47.wav\n",
      "Processed 3800 files. Currently processing file: /home/jovyan/cut-data/training/target/0.990_0031_436601051.wav\n",
      "...Done. Loaded 11292 training samples and 2 labels.\n",
      "Balance the training data...\n",
      "Balanced training data:\n",
      "x_train shape: (7612,)\n",
      "y_train shape: (7612, 2)\n",
      "file_paths_train shape: (7612,)\n",
      "...Done. Loaded 7612 training samples and 7612 labels.\n"
     ]
    }
   ],
   "source": [
    "import data as data\n",
    "import numpy as np\n",
    "\n",
    "train_data_path = \"/home/jovyan/cut-data/training/\"\n",
    "\n",
    "# Load training data\n",
    "print('Loading training data...', flush=True)\n",
    "x_train, y_train, labels, file_paths_train = data.loadData(train_data_path)\n",
    "print('...Done. Loaded {} training samples and {} labels.'.format(x_train.shape[0], y_train.shape[1]), flush=True)\n",
    "\n",
    "# balance the training data:\n",
    "print('Balance the training data...')\n",
    "\n",
    "y_train_indices = np.argmax(y_train, axis=1)\n",
    "\n",
    "# minimum of one class\n",
    "min_samples = min(np.bincount(y_train_indices))\n",
    "\n",
    "# reduce entries until minimum after shuffle \n",
    "balanced_x_train = []\n",
    "balanced_y_train = []\n",
    "balanced_file_paths_train = []\n",
    "for label in np.unique(y_train_indices):\n",
    "    indices = np.where(y_train_indices == label)[0]\n",
    "    np.random.shuffle(indices)  # Random order for random removal of samples\n",
    "    indices = indices[:min_samples]\n",
    "    balanced_x_train.append(x_train[indices])\n",
    "    balanced_y_train.append(y_train[indices])\n",
    "    balanced_file_paths_train.extend(file_paths_train[indices])\n",
    "\n",
    "# Combine the balanced data for all classes\n",
    "balanced_x_train = np.concatenate(balanced_x_train, axis=0)\n",
    "balanced_y_train = np.concatenate(balanced_y_train, axis=0)\n",
    "balanced_file_paths_train = np.array(balanced_file_paths_train)\n",
    "\n",
    "print('Balanced training data:')\n",
    "print('x_train shape:', balanced_x_train.shape)\n",
    "print('y_train shape:', balanced_y_train.shape)\n",
    "print('file_paths_train shape:', balanced_file_paths_train.shape)\n",
    "\n",
    "print('...Done. Loaded {} training samples and {} labels.'.format(balanced_x_train.shape[0], balanced_y_train.shape[0]), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792a1683-5cab-404f-ab4d-5ed1068d0111",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbd390d4-b2db-4f52-9e78-f4b9db337bcc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data...\n",
      "Processed 0 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/0.920_0001_270097.wav\n",
      "Processed 100 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/0.930_0002_182583971.wav\n",
      "Processed 200 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/0.940_0004_534761.wav\n",
      "Processed 300 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/0.950_0018_226391901.wav\n",
      "Processed 400 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/0.970_0017_647758.wav\n",
      "Processed 500 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/1.000_0001_0H2uMhzSitY_520.wav\n",
      "Processed 600 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/1.000_0002_--ivFZu-hlc_30.wav\n",
      "Processed 700 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/1.000_0002_2RpOd9MJjyQ_10.wav\n",
      "Processed 800 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/1.000_0003_-w4HLksto_k_30.wav\n",
      "Processed 900 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/1.000_0003_6m5hv5BX7KU_40.wav\n",
      "Processed 0 files. Currently processing file: /home/jovyan/cut-data/validation/target/0.480_0001_200614_1467_1.wav\n",
      "Processed 100 files. Currently processing file: /home/jovyan/cut-data/validation/target/0.920_0103_669234.wav\n",
      "Processed 200 files. Currently processing file: /home/jovyan/cut-data/validation/target/0.940_0015_139518451.wav\n",
      "Processed 300 files. Currently processing file: /home/jovyan/cut-data/validation/target/0.950_0064_558294.wav\n",
      "Processed 400 files. Currently processing file: /home/jovyan/cut-data/validation/target/0.970_0001_155008.wav\n",
      "...Done. Loaded 1381 validation samples and 2 labels.\n"
     ]
    }
   ],
   "source": [
    "val_data_path = \"/home/jovyan/cut-data/validation/\"\n",
    "\n",
    "print('Loading validation data...', flush=True)\n",
    "x_val, y_val, labels, file_paths_val = data.loadData(val_data_path)\n",
    "print('...Done. Loaded {} validation samples and {} labels.'.format(x_val.shape[0], y_val.shape[1]), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c08809-af28-44b2-b5ed-a8950a2e53be",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "202654c8-7d18-4854-b049-056f58c96a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training labels:  (7612,)\n",
      "Shape of validation labels:  (1381,)\n",
      "Training model...\n",
      "Length of trainable Weightslen  55\n",
      "final spec shape:Tensor(\"model_1/ADVANCED_SPEC1/ExpandDims:0\", shape=(None, 128, None, 1), dtype=float32)\n",
      "final spec shape:Tensor(\"model_1/ADVANCED_SPEC1/ExpandDims:0\", shape=(None, 128, None, 1), dtype=float32)\n",
      "7612/7612 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9890 - prec: 0.9890 - recall: 0.9890final spec shape:Tensor(\"model_1/ADVANCED_SPEC1/ExpandDims:0\", shape=(None, 128, None, 1), dtype=float32)\n",
      "7612/7612 [==============================] - 913s 119ms/step - loss: 0.0233 - accuracy: 0.9890 - prec: 0.9890 - recall: 0.9890 - val_loss: 6.0866e-05 - val_accuracy: 1.0000 - val_prec: 1.0000 - val_recall: 1.0000\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import model\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "train_epochs = 1\n",
    "train_batch_size = 1\n",
    "train_learning_rate = 0.0001\n",
    "    \n",
    "# Number of last layers to be trained, rest will be frozen, 0 = all will be trained\n",
    "train_layers_num = 0\n",
    "\n",
    "on_epoch_end = None\n",
    "\n",
    "print(\"Shape of training labels: \", balanced_x_train.shape)\n",
    "print(\"Shape of validation labels: \", x_val.shape)\n",
    "\n",
    "# Train model\n",
    "print('Training model...', flush=True)\n",
    "\n",
    "# Disable Model Debug\n",
    "tf.debugging.disable_check_numerics()\n",
    "\n",
    "trained_quant_aware, history = model.trainNewModel(\n",
    "                                      quant_aware_model,\n",
    "                                      train_layers_num,\n",
    "                                      balanced_x_train, \n",
    "                                      balanced_y_train,\n",
    "                                      x_val,\n",
    "                                      y_val,\n",
    "                                      balanced_file_paths_train,\n",
    "                                      file_paths_val,\n",
    "                                      epochs=train_epochs,\n",
    "                                      batch_size=train_batch_size,\n",
    "                                      learning_rate=train_learning_rate,\n",
    "                                      on_epoch_end=on_epoch_end)\n",
    "\n",
    "print('Done', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd481b4-a9aa-474a-ac8f-92bd3299c2e1",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d71d2351-fcaf-46cf-a78c-0d699d54dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversion using Quantization...\n",
      "final spec shape:Tensor(\"model_1/ADVANCED_SPEC1/ExpandDims:0\", shape=(None, 128, 513, 1), dtype=float32)\n",
      "final spec shape:Tensor(\"ExpandDims:0\", shape=(None, 128, 513, 1), dtype=float32)\n",
      "final spec shape:Tensor(\"ExpandDims:0\", shape=(None, 128, 513, 1), dtype=float32)\n",
      "final spec shape:Tensor(\"ExpandDims:0\", shape=(None, 128, 513, 1), dtype=float32)\n",
      "final spec shape:Tensor(\"ExpandDims:0\", shape=(None, 128, 513, 1), dtype=float32)\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp3qevlpe8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp3qevlpe8/assets\n",
      "/opt/conda/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:887: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2023-09-19 20:38:36.319911: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-09-19 20:38:36.319982: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-09-19 20:38:36.320517: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp3qevlpe8\n",
      "2023-09-19 20:38:36.369277: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2023-09-19 20:38:36.369334: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmp3qevlpe8\n",
      "2023-09-19 20:38:36.486242: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2023-09-19 20:38:36.547218: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-09-19 20:38:37.633345: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmp3qevlpe8\n",
      "2023-09-19 20:38:38.041077: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 1720558 microseconds.\n",
      "2023-09-19 20:38:38.505699: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(trained_quant_aware)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "print('Starting conversion using Quantization...', flush=True)\n",
    "quant_aware_INT8_tflite = converter.convert()\n",
    "print('...Done.', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5982b349-86d8-4674-b972-a42f8828a113",
   "metadata": {},
   "source": [
    "### Evaluate Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6ac8b1f-74f1-481e-9766-768570bd6c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the TFLite model: 15.70 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=quant_aware_INT8_tflite)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "tflite_size = len(quant_aware_INT8_tflite) / (1024 * 1024)\n",
    "print(f\"Size of the TFLite model: {tflite_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9493811e-eb6c-4e94-bf7f-1b1f4429341a",
   "metadata": {},
   "source": [
    "### Save Quant. Aware Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c429b0a-1096-4369-8999-fb532690fb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the quantized model:\n",
    "quant_aware_INT8_tflite_path = \"/home/jovyan/models/checkpoints_/quantization_aware_INT8.tflite\"\n",
    "with open(quant_aware_INT8_tflite_path, \"wb\") as f:\n",
    "    f.write(quant_aware_INT8_tflite)\n",
    "print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e623ecb-889e-4775-85a7-28bb7fa7f6d0",
   "metadata": {},
   "source": [
    "### Evaluate Quant. Aware Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b29a5c8b-5896-4e10-8a4e-b17e24a2f0ea",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== /home/jovyan/models/checkpoints_/quantization_aware_INT8.tflite ===\n",
      "\n",
      "Your TFLite model has '1' subgraph(s). In the subgraph description below,\n",
      "T# represents the Tensor numbers. For example, in Subgraph#0, the REDUCE_MIN op takes\n",
      "tensor #0 and tensor #140 as input and produces tensor #186 as output.\n",
      "\n",
      "Subgraph#0 main(T#0) -> [T#633]\n",
      "  Op#0 REDUCE_MIN(T#0, T#140[1]) -> [T#186]\n",
      "  Op#1 SUB(T#0, T#186) -> [T#187]\n",
      "  Op#2 REDUCE_MAX(T#187, T#140[1]) -> [T#188]\n",
      "  Op#3 ADD(T#188, T#14) -> [T#189]\n",
      "  Op#4 DIV(T#187, T#189) -> [T#190]\n",
      "  Op#5 SHAPE(T#190) -> [T#191]\n",
      "  Op#6 SPLIT_V(T#191, T#138[1, 1, 0], T#120[0]) -> [T#192, T#193, T#194]\n",
      "  Op#7 RESHAPE(T#193, T#121[]) -> [T#195]\n",
      "  Op#8 FLOOR_DIV(T#195, T#122[8]) -> [T#196]\n",
      "  Op#9 PACK(T#196, T#122[8]) -> [T#197]\n",
      "  Op#10 MUL(T#196, T#122[8]) -> [T#198]\n",
      "  Op#11 RESHAPE(T#198, T#124[1]) -> [T#199]\n",
      "  Op#12 CONCATENATION(T#192, T#199) -> [T#200]\n",
      "  Op#13 CONCATENATION(T#192, T#197) -> [T#201]\n",
      "  Op#14 STRIDED_SLICE(T#190, T#125[0, 0], T#200, T#137[1, 1]) -> [T#202]\n",
      "  Op#15 RESHAPE(T#202, T#201) -> [T#203]\n",
      "  Op#16 SUB(T#195, T#126[512]) -> [T#204]\n",
      "  Op#17 FLOOR_DIV(T#204, T#127[280]) -> [T#205]\n",
      "  Op#18 ADD(T#205, T#140[1]) -> [T#206]\n",
      "  Op#19 MAXIMUM(T#206, T#120[0]) -> [T#207]\n",
      "  Op#20 PACK(T#207, T#140[1]) -> [T#208]\n",
      "  Op#21 PACK(T#207, T#126[512]) -> [T#209]\n",
      "  Op#22 CONCATENATION(T#192, T#209) -> [T#210]\n",
      "  Op#23 RANGE(T#120[0], T#207, T#140[1]) -> [T#211]\n",
      "  Op#24 MUL(T#211, T#136[35]) -> [T#212]\n",
      "  Op#25 RESHAPE(T#212, T#208) -> [T#213]\n",
      "  Op#26 ADD(T#213, T#139[0, 1, 2, 3, 4, ...]) -> [T#214]\n",
      "  Op#27 GATHER(T#203, T#214) -> [T#215]\n",
      "  Op#28 RESHAPE(T#215, T#210) -> [T#216]\n",
      "  Op#29 MUL(T#216, T#13) -> [T#217]\n",
      "  Op#30 EXPAND_DIMS(T#217, T#151[-2]) -> [T#218]\n",
      "  Op#31 RFFT2D(T#218, T#150[1, 512]) -> [T#219]\n",
      "  Op#32 SQUEEZE(T#219) -> [T#220]\n",
      "  Op#33 COMPLEX_ABS(T#220) -> [T#221]\n",
      "  Op#34 STRIDED_SLICE(T#221, T#128[0, 0, 0], T#129[0, 0, 128], T#130[1, 1, 1]) -> [T#222]\n",
      "  Op#35 MUL(T#222, T#222) -> [T#223]\n",
      "  Op#36 POW(T#223, T#11) -> [T#224]\n",
      "  Op#37 TRANSPOSE(T#224, T#131[0, 2, 1]) -> [T#225]\n",
      "  Op#38 EXPAND_DIMS(T#225, T#119[-1]) -> [T#226]\n",
      "  Op#39 MUL(T#226, T#15) -> [T#227]\n",
      "  Op#40 ADD(T#227, T#16) -> [T#228]\n",
      "  Op#41 CONV_2D(T#228, T#88, T#149) -> [T#229]\n",
      "  Op#42 QUANTIZE(T#229) -> [T#230]\n",
      "  Op#43 DEQUANTIZE(T#230) -> [T#231]\n",
      "  Op#44 MUL(T#231, T#17) -> [T#232]\n",
      "  Op#45 ADD(T#232, T#18) -> [T#233]\n",
      "  Op#46 AVERAGE_POOL_2D(T#233) -> [T#234]\n",
      "  Op#47 MAX_POOL_2D(T#233) -> [T#235]\n",
      "  Op#48 CONCATENATION(T#235, T#234) -> [T#236]\n",
      "  Op#49 CONV_2D(T#236, T#89, T#163) -> [T#237]\n",
      "  Op#50 QUANTIZE(T#237) -> [T#238]\n",
      "  Op#51 CONV_2D(T#238, T#239, T#180[0, 0, 0, 0, 0, ...]) -> [T#240]\n",
      "  Op#52 DEQUANTIZE(T#240) -> [T#241]\n",
      "  Op#53 MUL(T#241, T#19) -> [T#242]\n",
      "  Op#54 ADD(T#242, T#20) -> [T#243]\n",
      "  Op#55 LOGISTIC(T#243) -> [T#244]\n",
      "  Op#56 MUL(T#243, T#244) -> [T#245]\n",
      "  Op#57 CONV_2D(T#245, T#90, T#148) -> [T#246]\n",
      "  Op#58 QUANTIZE(T#246) -> [T#247]\n",
      "  Op#59 DEQUANTIZE(T#247) -> [T#248]\n",
      "  Op#60 MUL(T#248, T#21) -> [T#249]\n",
      "  Op#61 ADD(T#249, T#22) -> [T#250]\n",
      "  Op#62 CONV_2D(T#250, T#91, T#148) -> [T#251]\n",
      "  Op#63 QUANTIZE(T#251) -> [T#252]\n",
      "  Op#64 DEQUANTIZE(T#252) -> [T#253]\n",
      "  Op#65 MUL(T#253, T#23) -> [T#254]\n",
      "  Op#66 ADD(T#254, T#24) -> [T#255]\n",
      "  Op#67 LOGISTIC(T#255) -> [T#256]\n",
      "  Op#68 MUL(T#255, T#256) -> [T#257]\n",
      "  Op#69 CONV_2D(T#257, T#92, T#148) -> [T#258]\n",
      "  Op#70 QUANTIZE(T#258) -> [T#259]\n",
      "  Op#71 DEQUANTIZE(T#259) -> [T#260]\n",
      "  Op#72 MUL(T#260, T#25) -> [T#261]\n",
      "  Op#73 ADD(T#261, T#26) -> [T#262]\n",
      "  Op#74 ADD(T#262, T#250) -> [T#263]\n",
      "  Op#75 QUANTIZE(T#263) -> [T#264]\n",
      "  Op#76 DEQUANTIZE(T#264) -> [T#265]\n",
      "  Op#77 CONV_2D(T#264, T#266, T#181[0, 0, 0, 0, 0, ...]) -> [T#267]\n",
      "  Op#78 DEQUANTIZE(T#267) -> [T#268]\n",
      "  Op#79 MUL(T#268, T#27) -> [T#269]\n",
      "  Op#80 ADD(T#269, T#28) -> [T#270]\n",
      "  Op#81 LOGISTIC(T#270) -> [T#271]\n",
      "  Op#82 MUL(T#270, T#271) -> [T#272]\n",
      "  Op#83 CONV_2D(T#272, T#93, T#148) -> [T#273]\n",
      "  Op#84 QUANTIZE(T#273) -> [T#274]\n",
      "  Op#85 DEQUANTIZE(T#274) -> [T#275]\n",
      "  Op#86 MUL(T#275, T#29) -> [T#276]\n",
      "  Op#87 ADD(T#276, T#30) -> [T#277]\n",
      "  Op#88 ADD(T#277, T#265) -> [T#278]\n",
      "  Op#89 QUANTIZE(T#278) -> [T#279]\n",
      "  Op#90 CONV_2D(T#279, T#280, T#182[0, 0, 0, 0, 0, ...]) -> [T#281]\n",
      "  Op#91 DEQUANTIZE(T#281) -> [T#282]\n",
      "  Op#92 MUL(T#282, T#31) -> [T#283]\n",
      "  Op#93 ADD(T#283, T#32) -> [T#284]\n",
      "  Op#94 LOGISTIC(T#284) -> [T#285]\n",
      "  Op#95 MUL(T#284, T#285) -> [T#286]\n",
      "  Op#96 CONV_2D(T#286, T#94, T#146) -> [T#287]\n",
      "  Op#97 QUANTIZE(T#287) -> [T#288]\n",
      "  Op#98 DEQUANTIZE(T#288) -> [T#289]\n",
      "  Op#99 MUL(T#289, T#33) -> [T#290]\n",
      "  Op#100 ADD(T#290, T#34) -> [T#291]\n",
      "  Op#101 CONV_2D(T#291, T#95, T#147) -> [T#292]\n",
      "  Op#102 QUANTIZE(T#292) -> [T#293]\n",
      "  Op#103 DEQUANTIZE(T#293) -> [T#294]\n",
      "  Op#104 MUL(T#294, T#35) -> [T#295]\n",
      "  Op#105 ADD(T#295, T#36) -> [T#296]\n",
      "  Op#106 LOGISTIC(T#296) -> [T#297]\n",
      "  Op#107 MUL(T#296, T#297) -> [T#298]\n",
      "  Op#108 CONV_2D(T#298, T#96, T#146) -> [T#299]\n",
      "  Op#109 QUANTIZE(T#299) -> [T#300]\n",
      "  Op#110 DEQUANTIZE(T#300) -> [T#301]\n",
      "  Op#111 MUL(T#301, T#37) -> [T#302]\n",
      "  Op#112 ADD(T#302, T#38) -> [T#303]\n",
      "  Op#113 ADD(T#303, T#291) -> [T#304]\n",
      "  Op#114 QUANTIZE(T#304) -> [T#305]\n",
      "  Op#115 DEQUANTIZE(T#305) -> [T#306]\n",
      "  Op#116 CONV_2D(T#305, T#307, T#183[0, 0, 0, 0, 0, ...]) -> [T#308]\n",
      "  Op#117 DEQUANTIZE(T#308) -> [T#309]\n",
      "  Op#118 MUL(T#309, T#39) -> [T#310]\n",
      "  Op#119 ADD(T#310, T#40) -> [T#311]\n",
      "  Op#120 LOGISTIC(T#311) -> [T#312]\n",
      "  Op#121 MUL(T#311, T#312) -> [T#313]\n",
      "  Op#122 CONV_2D(T#313, T#97, T#146) -> [T#314]\n",
      "  Op#123 QUANTIZE(T#314) -> [T#315]\n",
      "  Op#124 DEQUANTIZE(T#315) -> [T#316]\n",
      "  Op#125 MUL(T#316, T#41) -> [T#317]\n",
      "  Op#126 ADD(T#317, T#42) -> [T#318]\n",
      "  Op#127 ADD(T#318, T#306) -> [T#319]\n",
      "  Op#128 QUANTIZE(T#319) -> [T#320]\n",
      "  Op#129 DEQUANTIZE(T#320) -> [T#321]\n",
      "  Op#130 CONV_2D(T#320, T#322, T#184[0, 0, 0, 0, 0, ...]) -> [T#323]\n",
      "  Op#131 DEQUANTIZE(T#323) -> [T#324]\n",
      "  Op#132 MUL(T#324, T#43) -> [T#325]\n",
      "  Op#133 ADD(T#325, T#44) -> [T#326]\n",
      "  Op#134 LOGISTIC(T#326) -> [T#327]\n",
      "  Op#135 MUL(T#326, T#327) -> [T#328]\n",
      "  Op#136 CONV_2D(T#328, T#98, T#146) -> [T#329]\n",
      "  Op#137 QUANTIZE(T#329) -> [T#330]\n",
      "  Op#138 DEQUANTIZE(T#330) -> [T#331]\n",
      "  Op#139 MUL(T#331, T#45) -> [T#332]\n",
      "  Op#140 ADD(T#332, T#46) -> [T#333]\n",
      "  Op#141 ADD(T#333, T#321) -> [T#334]\n",
      "  Op#142 QUANTIZE(T#334) -> [T#335]\n",
      "  Op#143 CONV_2D(T#335, T#336, T#164[0, 0, 0, 0, 0, ...]) -> [T#337]\n",
      "  Op#144 DEQUANTIZE(T#337) -> [T#338]\n",
      "  Op#145 MUL(T#338, T#47) -> [T#339]\n",
      "  Op#146 ADD(T#339, T#48) -> [T#340]\n",
      "  Op#147 LOGISTIC(T#340) -> [T#341]\n",
      "  Op#148 MUL(T#340, T#341) -> [T#342]\n",
      "  Op#149 DEPTHWISE_CONV_2D(T#342, T#1, T#154) -> [T#343]\n",
      "  Op#150 LOGISTIC(T#343) -> [T#344]\n",
      "  Op#151 MUL(T#343, T#344) -> [T#345]\n",
      "  Op#152 MEAN(T#345, T#132[1, 2]) -> [T#346]\n",
      "  Op#153 SHAPE(T#346) -> [T#347]\n",
      "  Op#154 STRIDED_SLICE(T#347, T#123[0], T#124[1], T#124[1]) -> [T#348]\n",
      "  Op#155 PACK(T#348, T#140[1], T#140[1], T#134[640]) -> [T#349]\n",
      "  Op#156 RESHAPE(T#346, T#349) -> [T#350]\n",
      "  Op#157 CONV_2D(T#350, T#99, T#145) -> [T#351]\n",
      "  Op#158 LOGISTIC(T#351) -> [T#352]\n",
      "  Op#159 MUL(T#351, T#352) -> [T#353]\n",
      "  Op#160 QUANTIZE(T#353) -> [T#354]\n",
      "  Op#161 CONV_2D(T#354, T#355, T#165[0, 0, 0, 0, 0, ...]) -> [T#356]\n",
      "  Op#162 LOGISTIC(T#356) -> [T#357]\n",
      "  Op#163 DEQUANTIZE(T#357) -> [T#358]\n",
      "  Op#164 MUL(T#345, T#358) -> [T#359]\n",
      "  Op#165 CONV_2D(T#359, T#100, T#144) -> [T#360]\n",
      "  Op#166 QUANTIZE(T#360) -> [T#361]\n",
      "  Op#167 DEQUANTIZE(T#361) -> [T#362]\n",
      "  Op#168 MUL(T#362, T#49) -> [T#363]\n",
      "  Op#169 ADD(T#363, T#50) -> [T#364]\n",
      "  Op#170 CONV_2D(T#364, T#101, T#153) -> [T#365]\n",
      "  Op#171 QUANTIZE(T#365) -> [T#366]\n",
      "  Op#172 DEQUANTIZE(T#366) -> [T#367]\n",
      "  Op#173 MUL(T#367, T#51) -> [T#368]\n",
      "  Op#174 ADD(T#368, T#52) -> [T#369]\n",
      "  Op#175 LOGISTIC(T#369) -> [T#370]\n",
      "  Op#176 MUL(T#369, T#370) -> [T#371]\n",
      "  Op#177 DEPTHWISE_CONV_2D(T#371, T#2, T#155) -> [T#372]\n",
      "  Op#178 LOGISTIC(T#372) -> [T#373]\n",
      "  Op#179 MUL(T#372, T#373) -> [T#374]\n",
      "  Op#180 MEAN(T#374, T#132[1, 2]) -> [T#375]\n",
      "  Op#181 SHAPE(T#375) -> [T#376]\n",
      "  Op#182 STRIDED_SLICE(T#376, T#123[0], T#124[1], T#124[1]) -> [T#377]\n",
      "  Op#183 PACK(T#377, T#140[1], T#140[1], T#134[640]) -> [T#378]\n",
      "  Op#184 RESHAPE(T#375, T#378) -> [T#379]\n",
      "  Op#185 CONV_2D(T#379, T#102, T#145) -> [T#380]\n",
      "  Op#186 LOGISTIC(T#380) -> [T#381]\n",
      "  Op#187 MUL(T#380, T#381) -> [T#382]\n",
      "  Op#188 QUANTIZE(T#382) -> [T#383]\n",
      "  Op#189 CONV_2D(T#383, T#384, T#166[0, 0, 0, 0, 0, ...]) -> [T#385]\n",
      "  Op#190 LOGISTIC(T#385) -> [T#386]\n",
      "  Op#191 DEQUANTIZE(T#386) -> [T#387]\n",
      "  Op#192 MUL(T#374, T#387) -> [T#388]\n",
      "  Op#193 CONV_2D(T#388, T#103, T#144) -> [T#389]\n",
      "  Op#194 QUANTIZE(T#389) -> [T#390]\n",
      "  Op#195 DEQUANTIZE(T#390) -> [T#391]\n",
      "  Op#196 MUL(T#391, T#53) -> [T#392]\n",
      "  Op#197 ADD(T#392, T#54) -> [T#393]\n",
      "  Op#198 ADD(T#393, T#364) -> [T#394]\n",
      "  Op#199 QUANTIZE(T#394) -> [T#395]\n",
      "  Op#200 DEQUANTIZE(T#395) -> [T#396]\n",
      "  Op#201 CONV_2D(T#395, T#397, T#167[0, 0, 0, 0, 0, ...]) -> [T#398]\n",
      "  Op#202 DEQUANTIZE(T#398) -> [T#399]\n",
      "  Op#203 MUL(T#399, T#55) -> [T#400]\n",
      "  Op#204 ADD(T#400, T#56) -> [T#401]\n",
      "  Op#205 LOGISTIC(T#401) -> [T#402]\n",
      "  Op#206 MUL(T#401, T#402) -> [T#403]\n",
      "  Op#207 DEPTHWISE_CONV_2D(T#403, T#3, T#156) -> [T#404]\n",
      "  Op#208 LOGISTIC(T#404) -> [T#405]\n",
      "  Op#209 MUL(T#404, T#405) -> [T#406]\n",
      "  Op#210 MEAN(T#406, T#132[1, 2]) -> [T#407]\n",
      "  Op#211 SHAPE(T#407) -> [T#408]\n",
      "  Op#212 STRIDED_SLICE(T#408, T#123[0], T#124[1], T#124[1]) -> [T#409]\n",
      "  Op#213 PACK(T#409, T#140[1], T#140[1], T#134[640]) -> [T#410]\n",
      "  Op#214 RESHAPE(T#407, T#410) -> [T#411]\n",
      "  Op#215 CONV_2D(T#411, T#104, T#145) -> [T#412]\n",
      "  Op#216 LOGISTIC(T#412) -> [T#413]\n",
      "  Op#217 MUL(T#412, T#413) -> [T#414]\n",
      "  Op#218 QUANTIZE(T#414) -> [T#415]\n",
      "  Op#219 CONV_2D(T#415, T#416, T#168[0, 0, 0, 0, 0, ...]) -> [T#417]\n",
      "  Op#220 LOGISTIC(T#417) -> [T#418]\n",
      "  Op#221 DEQUANTIZE(T#418) -> [T#419]\n",
      "  Op#222 MUL(T#406, T#419) -> [T#420]\n",
      "  Op#223 CONV_2D(T#420, T#105, T#144) -> [T#421]\n",
      "  Op#224 QUANTIZE(T#421) -> [T#422]\n",
      "  Op#225 DEQUANTIZE(T#422) -> [T#423]\n",
      "  Op#226 MUL(T#423, T#57) -> [T#424]\n",
      "  Op#227 ADD(T#424, T#58) -> [T#425]\n",
      "  Op#228 ADD(T#425, T#396) -> [T#426]\n",
      "  Op#229 QUANTIZE(T#426) -> [T#427]\n",
      "  Op#230 DEQUANTIZE(T#427) -> [T#428]\n",
      "  Op#231 CONV_2D(T#427, T#429, T#169[0, 0, 0, 0, 0, ...]) -> [T#430]\n",
      "  Op#232 DEQUANTIZE(T#430) -> [T#431]\n",
      "  Op#233 MUL(T#431, T#59) -> [T#432]\n",
      "  Op#234 ADD(T#432, T#60) -> [T#433]\n",
      "  Op#235 LOGISTIC(T#433) -> [T#434]\n",
      "  Op#236 MUL(T#433, T#434) -> [T#435]\n",
      "  Op#237 DEPTHWISE_CONV_2D(T#435, T#4, T#157) -> [T#436]\n",
      "  Op#238 LOGISTIC(T#436) -> [T#437]\n",
      "  Op#239 MUL(T#436, T#437) -> [T#438]\n",
      "  Op#240 MEAN(T#438, T#132[1, 2]) -> [T#439]\n",
      "  Op#241 SHAPE(T#439) -> [T#440]\n",
      "  Op#242 STRIDED_SLICE(T#440, T#123[0], T#124[1], T#124[1]) -> [T#441]\n",
      "  Op#243 PACK(T#441, T#140[1], T#140[1], T#134[640]) -> [T#442]\n",
      "  Op#244 RESHAPE(T#439, T#442) -> [T#443]\n",
      "  Op#245 CONV_2D(T#443, T#106, T#145) -> [T#444]\n",
      "  Op#246 LOGISTIC(T#444) -> [T#445]\n",
      "  Op#247 MUL(T#444, T#445) -> [T#446]\n",
      "  Op#248 QUANTIZE(T#446) -> [T#447]\n",
      "  Op#249 CONV_2D(T#447, T#448, T#170[0, 0, 0, 0, 0, ...]) -> [T#449]\n",
      "  Op#250 LOGISTIC(T#449) -> [T#450]\n",
      "  Op#251 DEQUANTIZE(T#450) -> [T#451]\n",
      "  Op#252 MUL(T#438, T#451) -> [T#452]\n",
      "  Op#253 CONV_2D(T#452, T#107, T#144) -> [T#453]\n",
      "  Op#254 QUANTIZE(T#453) -> [T#454]\n",
      "  Op#255 DEQUANTIZE(T#454) -> [T#455]\n",
      "  Op#256 MUL(T#455, T#61) -> [T#456]\n",
      "  Op#257 ADD(T#456, T#62) -> [T#457]\n",
      "  Op#258 ADD(T#457, T#428) -> [T#458]\n",
      "  Op#259 QUANTIZE(T#458) -> [T#459]\n",
      "  Op#260 DEQUANTIZE(T#459) -> [T#460]\n",
      "  Op#261 CONV_2D(T#459, T#461, T#171[0, 0, 0, 0, 0, ...]) -> [T#462]\n",
      "  Op#262 DEQUANTIZE(T#462) -> [T#463]\n",
      "  Op#263 MUL(T#463, T#63) -> [T#464]\n",
      "  Op#264 ADD(T#464, T#64) -> [T#465]\n",
      "  Op#265 LOGISTIC(T#465) -> [T#466]\n",
      "  Op#266 MUL(T#465, T#466) -> [T#467]\n",
      "  Op#267 DEPTHWISE_CONV_2D(T#467, T#5, T#158) -> [T#468]\n",
      "  Op#268 LOGISTIC(T#468) -> [T#469]\n",
      "  Op#269 MUL(T#468, T#469) -> [T#470]\n",
      "  Op#270 MEAN(T#470, T#132[1, 2]) -> [T#471]\n",
      "  Op#271 SHAPE(T#471) -> [T#472]\n",
      "  Op#272 STRIDED_SLICE(T#472, T#123[0], T#124[1], T#124[1]) -> [T#473]\n",
      "  Op#273 PACK(T#473, T#140[1], T#140[1], T#134[640]) -> [T#474]\n",
      "  Op#274 RESHAPE(T#471, T#474) -> [T#475]\n",
      "  Op#275 CONV_2D(T#475, T#108, T#145) -> [T#476]\n",
      "  Op#276 LOGISTIC(T#476) -> [T#477]\n",
      "  Op#277 MUL(T#476, T#477) -> [T#478]\n",
      "  Op#278 QUANTIZE(T#478) -> [T#479]\n",
      "  Op#279 CONV_2D(T#479, T#480, T#172[0, 0, 0, 0, 0, ...]) -> [T#481]\n",
      "  Op#280 LOGISTIC(T#481) -> [T#482]\n",
      "  Op#281 DEQUANTIZE(T#482) -> [T#483]\n",
      "  Op#282 MUL(T#470, T#483) -> [T#484]\n",
      "  Op#283 CONV_2D(T#484, T#109, T#144) -> [T#485]\n",
      "  Op#284 QUANTIZE(T#485) -> [T#486]\n",
      "  Op#285 DEQUANTIZE(T#486) -> [T#487]\n",
      "  Op#286 MUL(T#487, T#65) -> [T#488]\n",
      "  Op#287 ADD(T#488, T#66) -> [T#489]\n",
      "  Op#288 ADD(T#489, T#460) -> [T#490]\n",
      "  Op#289 QUANTIZE(T#490) -> [T#491]\n",
      "  Op#290 CONV_2D(T#491, T#492, T#173[0, 0, 0, 0, 0, ...]) -> [T#493]\n",
      "  Op#291 DEQUANTIZE(T#493) -> [T#494]\n",
      "  Op#292 MUL(T#494, T#67) -> [T#495]\n",
      "  Op#293 ADD(T#495, T#68) -> [T#496]\n",
      "  Op#294 LOGISTIC(T#496) -> [T#497]\n",
      "  Op#295 MUL(T#496, T#497) -> [T#498]\n",
      "  Op#296 DEPTHWISE_CONV_2D(T#498, T#6, T#159) -> [T#499]\n",
      "  Op#297 LOGISTIC(T#499) -> [T#500]\n",
      "  Op#298 MUL(T#499, T#500) -> [T#501]\n",
      "  Op#299 MEAN(T#501, T#132[1, 2]) -> [T#502]\n",
      "  Op#300 SHAPE(T#502) -> [T#503]\n",
      "  Op#301 STRIDED_SLICE(T#503, T#123[0], T#124[1], T#124[1]) -> [T#504]\n",
      "  Op#302 PACK(T#504, T#140[1], T#140[1], T#135[1120]) -> [T#505]\n",
      "  Op#303 RESHAPE(T#502, T#505) -> [T#506]\n",
      "  Op#304 CONV_2D(T#506, T#110, T#143) -> [T#507]\n",
      "  Op#305 LOGISTIC(T#507) -> [T#508]\n",
      "  Op#306 MUL(T#507, T#508) -> [T#509]\n",
      "  Op#307 QUANTIZE(T#509) -> [T#510]\n",
      "  Op#308 CONV_2D(T#510, T#511, T#174[0, 0, 0, 0, 0, ...]) -> [T#512]\n",
      "  Op#309 LOGISTIC(T#512) -> [T#513]\n",
      "  Op#310 DEQUANTIZE(T#513) -> [T#514]\n",
      "  Op#311 MUL(T#501, T#514) -> [T#515]\n",
      "  Op#312 CONV_2D(T#515, T#111, T#142) -> [T#516]\n",
      "  Op#313 QUANTIZE(T#516) -> [T#517]\n",
      "  Op#314 DEQUANTIZE(T#517) -> [T#518]\n",
      "  Op#315 MUL(T#518, T#69) -> [T#519]\n",
      "  Op#316 ADD(T#519, T#70) -> [T#520]\n",
      "  Op#317 CONV_2D(T#520, T#112, T#152) -> [T#521]\n",
      "  Op#318 QUANTIZE(T#521) -> [T#522]\n",
      "  Op#319 DEQUANTIZE(T#522) -> [T#523]\n",
      "  Op#320 MUL(T#523, T#71) -> [T#524]\n",
      "  Op#321 ADD(T#524, T#72) -> [T#525]\n",
      "  Op#322 LOGISTIC(T#525) -> [T#526]\n",
      "  Op#323 MUL(T#525, T#526) -> [T#527]\n",
      "  Op#324 DEPTHWISE_CONV_2D(T#527, T#7, T#160) -> [T#528]\n",
      "  Op#325 LOGISTIC(T#528) -> [T#529]\n",
      "  Op#326 MUL(T#528, T#529) -> [T#530]\n",
      "  Op#327 MEAN(T#530, T#132[1, 2]) -> [T#531]\n",
      "  Op#328 SHAPE(T#531) -> [T#532]\n",
      "  Op#329 STRIDED_SLICE(T#532, T#123[0], T#124[1], T#124[1]) -> [T#533]\n",
      "  Op#330 PACK(T#533, T#140[1], T#140[1], T#135[1120]) -> [T#534]\n",
      "  Op#331 RESHAPE(T#531, T#534) -> [T#535]\n",
      "  Op#332 CONV_2D(T#535, T#113, T#143) -> [T#536]\n",
      "  Op#333 LOGISTIC(T#536) -> [T#537]\n",
      "  Op#334 MUL(T#536, T#537) -> [T#538]\n",
      "  Op#335 QUANTIZE(T#538) -> [T#539]\n",
      "  Op#336 CONV_2D(T#539, T#540, T#175[0, 0, 0, 0, 0, ...]) -> [T#541]\n",
      "  Op#337 LOGISTIC(T#541) -> [T#542]\n",
      "  Op#338 DEQUANTIZE(T#542) -> [T#543]\n",
      "  Op#339 MUL(T#530, T#543) -> [T#544]\n",
      "  Op#340 CONV_2D(T#544, T#114, T#142) -> [T#545]\n",
      "  Op#341 QUANTIZE(T#545) -> [T#546]\n",
      "  Op#342 DEQUANTIZE(T#546) -> [T#547]\n",
      "  Op#343 MUL(T#547, T#73) -> [T#548]\n",
      "  Op#344 ADD(T#548, T#74) -> [T#549]\n",
      "  Op#345 ADD(T#549, T#520) -> [T#550]\n",
      "  Op#346 QUANTIZE(T#550) -> [T#551]\n",
      "  Op#347 DEQUANTIZE(T#551) -> [T#552]\n",
      "  Op#348 CONV_2D(T#551, T#553, T#176[0, 0, 0, 0, 0, ...]) -> [T#554]\n",
      "  Op#349 DEQUANTIZE(T#554) -> [T#555]\n",
      "  Op#350 MUL(T#555, T#75) -> [T#556]\n",
      "  Op#351 ADD(T#556, T#76) -> [T#557]\n",
      "  Op#352 LOGISTIC(T#557) -> [T#558]\n",
      "  Op#353 MUL(T#557, T#558) -> [T#559]\n",
      "  Op#354 DEPTHWISE_CONV_2D(T#559, T#8, T#161) -> [T#560]\n",
      "  Op#355 LOGISTIC(T#560) -> [T#561]\n",
      "  Op#356 MUL(T#560, T#561) -> [T#562]\n",
      "  Op#357 MEAN(T#562, T#132[1, 2]) -> [T#563]\n",
      "  Op#358 SHAPE(T#563) -> [T#564]\n",
      "  Op#359 STRIDED_SLICE(T#564, T#123[0], T#124[1], T#124[1]) -> [T#565]\n",
      "  Op#360 PACK(T#565, T#140[1], T#140[1], T#135[1120]) -> [T#566]\n",
      "  Op#361 RESHAPE(T#563, T#566) -> [T#567]\n",
      "  Op#362 CONV_2D(T#567, T#115, T#143) -> [T#568]\n",
      "  Op#363 LOGISTIC(T#568) -> [T#569]\n",
      "  Op#364 MUL(T#568, T#569) -> [T#570]\n",
      "  Op#365 QUANTIZE(T#570) -> [T#571]\n",
      "  Op#366 CONV_2D(T#571, T#572, T#177[0, 0, 0, 0, 0, ...]) -> [T#573]\n",
      "  Op#367 LOGISTIC(T#573) -> [T#574]\n",
      "  Op#368 DEQUANTIZE(T#574) -> [T#575]\n",
      "  Op#369 MUL(T#562, T#575) -> [T#576]\n",
      "  Op#370 CONV_2D(T#576, T#116, T#142) -> [T#577]\n",
      "  Op#371 QUANTIZE(T#577) -> [T#578]\n",
      "  Op#372 DEQUANTIZE(T#578) -> [T#579]\n",
      "  Op#373 MUL(T#579, T#77) -> [T#580]\n",
      "  Op#374 ADD(T#580, T#78) -> [T#581]\n",
      "  Op#375 ADD(T#581, T#552) -> [T#582]\n",
      "  Op#376 QUANTIZE(T#582) -> [T#583]\n",
      "  Op#377 DEQUANTIZE(T#583) -> [T#584]\n",
      "  Op#378 CONV_2D(T#583, T#585, T#178[0, 0, 0, 0, 0, ...]) -> [T#586]\n",
      "  Op#379 DEQUANTIZE(T#586) -> [T#587]\n",
      "  Op#380 MUL(T#587, T#79) -> [T#588]\n",
      "  Op#381 ADD(T#588, T#80) -> [T#589]\n",
      "  Op#382 LOGISTIC(T#589) -> [T#590]\n",
      "  Op#383 MUL(T#589, T#590) -> [T#591]\n",
      "  Op#384 DEPTHWISE_CONV_2D(T#591, T#9, T#162) -> [T#592]\n",
      "  Op#385 LOGISTIC(T#592) -> [T#593]\n",
      "  Op#386 MUL(T#592, T#593) -> [T#594]\n",
      "  Op#387 MEAN(T#594, T#132[1, 2]) -> [T#595]\n",
      "  Op#388 SHAPE(T#595) -> [T#596]\n",
      "  Op#389 STRIDED_SLICE(T#596, T#123[0], T#124[1], T#124[1]) -> [T#597]\n",
      "  Op#390 PACK(T#597, T#140[1], T#140[1], T#135[1120]) -> [T#598]\n",
      "  Op#391 RESHAPE(T#595, T#598) -> [T#599]\n",
      "  Op#392 CONV_2D(T#599, T#117, T#143) -> [T#600]\n",
      "  Op#393 LOGISTIC(T#600) -> [T#601]\n",
      "  Op#394 MUL(T#600, T#601) -> [T#602]\n",
      "  Op#395 QUANTIZE(T#602) -> [T#603]\n",
      "  Op#396 CONV_2D(T#603, T#604, T#179[0, 0, 0, 0, 0, ...]) -> [T#605]\n",
      "  Op#397 LOGISTIC(T#605) -> [T#606]\n",
      "  Op#398 DEQUANTIZE(T#606) -> [T#607]\n",
      "  Op#399 MUL(T#594, T#607) -> [T#608]\n",
      "  Op#400 CONV_2D(T#608, T#118, T#142) -> [T#609]\n",
      "  Op#401 QUANTIZE(T#609) -> [T#610]\n",
      "  Op#402 DEQUANTIZE(T#610) -> [T#611]\n",
      "  Op#403 MUL(T#611, T#81) -> [T#612]\n",
      "  Op#404 ADD(T#612, T#82) -> [T#613]\n",
      "  Op#405 ADD(T#613, T#584) -> [T#614]\n",
      "  Op#406 MUL(T#614, T#83) -> [T#615]\n",
      "  Op#407 ADD(T#615, T#84) -> [T#616]\n",
      "  Op#408 QUANTIZE(T#616) -> [T#617]\n",
      "  Op#409 CONV_2D(T#617, T#618, T#185[0, 0, 0, 0, 0, ...]) -> [T#619]\n",
      "  Op#410 DEQUANTIZE(T#619) -> [T#620]\n",
      "  Op#411 MUL(T#620, T#85) -> [T#621]\n",
      "  Op#412 ADD(T#621, T#86) -> [T#622]\n",
      "  Op#413 REDUCE_MAX(T#622, T#132[1, 2]) -> [T#623]\n",
      "  Op#414 SUB(T#622, T#623) -> [T#624]\n",
      "  Op#415 MUL(T#624, T#12) -> [T#625]\n",
      "  Op#416 EXP(T#625) -> [T#626]\n",
      "  Op#417 MEAN(T#626, T#132[1, 2]) -> [T#627]\n",
      "  Op#418 LOG(T#627) -> [T#628]\n",
      "  Op#419 MUL(T#628, T#10) -> [T#629]\n",
      "  Op#420 ADD(T#629, T#623) -> [T#630]\n",
      "  Op#421 RESHAPE(T#630, T#133[-1, 420]) -> [T#631]\n",
      "  Op#422 FULLY_CONNECTED(T#631, T#87, T#141) -> [T#632]\n",
      "  Op#423 SOFTMAX(T#632) -> [T#633]\n",
      "\n",
      "Tensors of Subgraph#0\n",
      "  T#0(serving_default_INPUT:0) shape_signature:[-1, 144000], type:FLOAT32\n",
      "  T#1(model_1/BLOCK_3-1_BN_2_NOQUANT/FusedBatchNormV3;model_1/BLOCK_3-1_CONV_2/depthwise;model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D) shape:[1, 3, 3, 640], type:FLOAT32 RO 23040 bytes, buffer: 2, data:[-0.251236, 0.372286, -0.59223, -0.1335, 0.312628, ...]\n",
      "  T#2(model_1/BLOCK_3-2_BN_2_NOQUANT/FusedBatchNormV3;model_1/BLOCK_3-2_CONV_2/depthwise;model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D) shape:[1, 3, 3, 640], type:FLOAT32 RO 23040 bytes, buffer: 3, data:[1.23551, -0.0568431, 0.0760742, -0.370292, 0.0261757, ...]\n",
      "  T#3(model_1/BLOCK_3-3_BN_2_NOQUANT/FusedBatchNormV3;model_1/BLOCK_3-3_CONV_2/depthwise;model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D) shape:[1, 3, 3, 640], type:FLOAT32 RO 23040 bytes, buffer: 4, data:[0.123467, -0.0474579, 0.25923, -0.762767, -0.0930023, ...]\n",
      "  T#4(model_1/BLOCK_3-4_BN_2_NOQUANT/FusedBatchNormV3;model_1/BLOCK_3-4_CONV_2/depthwise;model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D) shape:[1, 3, 3, 640], type:FLOAT32 RO 23040 bytes, buffer: 5, data:[-0.10532, -0.530738, 0.000815318, 0.0100449, 0.0389737, ...]\n",
      "  T#5(model_1/BLOCK_3-5_BN_2_NOQUANT/FusedBatchNormV3;model_1/BLOCK_3-5_CONV_2/depthwise;model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D) shape:[1, 3, 3, 640], type:FLOAT32 RO 23040 bytes, buffer: 6, data:[0.358909, -0.200586, 1.62531, 0.189931, 0.597165, ...]\n",
      "  T#6(model_1/BLOCK_4-1_BN_2_NOQUANT/FusedBatchNormV3;model_1/BLOCK_4-1_CONV_2/depthwise;model_1/quant_BLOCK_4-4_SE_CONV_2/Conv2D) shape:[1, 3, 3, 1120], type:FLOAT32 RO 40320 bytes, buffer: 7, data:[-0.313297, -0.692159, 1.42914, -0.135559, -0.402629, ...]\n",
      "  T#7(model_1/BLOCK_4-2_BN_2_NOQUANT/FusedBatchNormV3;model_1/BLOCK_4-2_CONV_2/depthwise;model_1/quant_BLOCK_4-4_SE_CONV_2/Conv2D) shape:[1, 3, 3, 1120], type:FLOAT32 RO 40320 bytes, buffer: 8, data:[-0.791778, -0.210047, -0.88355, -0.441906, -0.494887, ...]\n",
      "  T#8(model_1/BLOCK_4-3_BN_2_NOQUANT/FusedBatchNormV3;model_1/BLOCK_4-3_CONV_2/depthwise;model_1/quant_BLOCK_4-4_SE_CONV_2/Conv2D) shape:[1, 3, 3, 1120], type:FLOAT32 RO 40320 bytes, buffer: 9, data:[-0.734297, 0.403795, -5.34587, 0.876589, 0.309243, ...]\n",
      "  T#9(model_1/BLOCK_4-4_BN_2_NOQUANT/FusedBatchNormV3;model_1/BLOCK_4-4_CONV_2/depthwise;model_1/quant_BLOCK_4-4_SE_CONV_2/Conv2D) shape:[1, 3, 3, 1120], type:FLOAT32 RO 40320 bytes, buffer: 10, data:[-0.213518, -0.135148, -0.603809, -0.559516, -0.379189, ...]\n",
      "  T#10(model_1/GLOBAL_LME_POOL/truediv;model_1/GLOBAL_LME_POOL/ReadVariableOp) shape:[1], type:FLOAT32 RO 4 bytes, buffer: 11, data:[0.682098]\n",
      "  T#11(model_1/ADVANCED_SPEC1/truediv_1) shape:[], type:FLOAT32 RO 4 bytes, buffer: 12, data:[0.262867]\n",
      "  T#12(model_1/GLOBAL_LME_POOL/ReadVariableOp) shape:[1], type:FLOAT32 RO 4 bytes, buffer: 13, data:[1.46607]\n",
      "  T#13(model_1/ADVANCED_SPEC1/stft/hann_window/sub_2) shape:[512], type:FLOAT32 RO 2048 bytes, buffer: 14, data:[0, 3.76403e-05, 0.000150591, 0.000338793, 0.000602275, ...]\n",
      "  T#14(model_1/ADVANCED_SPEC1/add/y) shape:[], type:FLOAT32 RO 4 bytes, buffer: 15, data:[1e-06]\n",
      "  T#15(model_1/BNORM_SPEC_NOQUANT/FusedBatchNormV3) shape:[1], type:FLOAT32 RO 4 bytes, buffer: 16, data:[1.24676]\n",
      "  T#16(model_1/BNORM_SPEC_NOQUANT/FusedBatchNormV31) shape:[1], type:FLOAT32 RO 4 bytes, buffer: 17, data:[-0.118066]\n",
      "  T#17(model_1/BNORM_0/FusedBatchNormV3) shape:[30], type:FLOAT32 RO 120 bytes, buffer: 18, data:[0.393604, 0.354697, 0.37672, 0.151211, 0.223293, ...]\n",
      "  T#18(model_1/BNORM_0/FusedBatchNormV31) shape:[30], type:FLOAT32 RO 120 bytes, buffer: 19, data:[-0.0201999, 0.021001, -0.00927022, 0.157607, 1.64969, ...]\n",
      "  T#19(model_1/BLOCK_1-1_BN_1/FusedBatchNormV3) shape:[60], type:FLOAT32 RO 240 bytes, buffer: 20, data:[0.0924833, 0.0966721, 0.190127, 0.0743629, 0.0926299, ...]\n",
      "  T#20(model_1/BLOCK_1-1_BN_1/FusedBatchNormV31) shape:[60], type:FLOAT32 RO 240 bytes, buffer: 21, data:[0.994801, 1.30954, 1.39498, 2.66734, 0.875943, ...]\n",
      "  T#21(model_1/BLOCK_1-1_BN_3/FusedBatchNormV3) shape:[60], type:FLOAT32 RO 240 bytes, buffer: 22, data:[0.45962, 0.387087, 0.436444, 0.420513, 0.645633, ...]\n",
      "  T#22(model_1/BLOCK_1-1_BN_3/FusedBatchNormV31) shape:[60], type:FLOAT32 RO 240 bytes, buffer: 23, data:[-0.572335, -0.296768, -1.17371, -0.676895, 1.66753, ...]\n",
      "  T#23(model_1/BLOCK_1-2_BN_1/FusedBatchNormV3) shape:[60], type:FLOAT32 RO 240 bytes, buffer: 24, data:[0.104591, 0.0764446, 0.11338, 0.124167, 0.127248, ...]\n",
      "  T#24(model_1/BLOCK_1-2_BN_1/FusedBatchNormV31) shape:[60], type:FLOAT32 RO 240 bytes, buffer: 25, data:[-2.62731, -0.868263, 0.266521, 0.0349529, 0.854639, ...]\n",
      "  T#25(model_1/BLOCK_1-2_BN_3/FusedBatchNormV3) shape:[60], type:FLOAT32 RO 240 bytes, buffer: 26, data:[0.481113, 0.475233, 0.482956, 0.453034, 0.538626, ...]\n",
      "  T#26(model_1/BLOCK_1-2_BN_3/FusedBatchNormV31) shape:[60], type:FLOAT32 RO 240 bytes, buffer: 27, data:[-0.463809, 0.940085, -1.53324, 1.37461, 0.36683, ...]\n",
      "  T#27(model_1/BLOCK_1-3_BN_1/FusedBatchNormV3) shape:[60], type:FLOAT32 RO 240 bytes, buffer: 28, data:[0.0527719, 0.089133, 0.0894682, 0.0644225, 0.0708844, ...]\n",
      "  T#28(model_1/BLOCK_1-3_BN_1/FusedBatchNormV31) shape:[60], type:FLOAT32 RO 240 bytes, buffer: 29, data:[1.23824, 1.12686, 1.3523, -0.914098, 1.96378, ...]\n",
      "  T#29(model_1/BLOCK_1-3_BN_3/FusedBatchNormV3) shape:[60], type:FLOAT32 RO 240 bytes, buffer: 30, data:[0.963974, 0.827579, 0.844023, 0.772655, 0.593594, ...]\n",
      "  T#30(model_1/BLOCK_1-3_BN_3/FusedBatchNormV31) shape:[60], type:FLOAT32 RO 240 bytes, buffer: 31, data:[0.224621, -0.89843, -0.404808, 2.08627, 0.154646, ...]\n",
      "  T#31(model_1/BLOCK_2-1_BN_1/FusedBatchNormV3) shape:[240], type:FLOAT32 RO 960 bytes, buffer: 32, data:[0.0365299, 0.0508964, 0.0387093, 0.0551016, 0.0589835, ...]\n",
      "  T#32(model_1/BLOCK_2-1_BN_1/FusedBatchNormV31) shape:[240], type:FLOAT32 RO 960 bytes, buffer: 33, data:[1.08094, 1.82718, 1.35988, 2.0496, 1.56324, ...]\n",
      "  T#33(model_1/BLOCK_2-1_BN_3/FusedBatchNormV3) shape:[120], type:FLOAT32 RO 480 bytes, buffer: 34, data:[0.227716, 0.263557, 0.245846, 0.213045, 0.257738, ...]\n",
      "  T#34(model_1/BLOCK_2-1_BN_3/FusedBatchNormV31) shape:[120], type:FLOAT32 RO 480 bytes, buffer: 35, data:[0.517984, 1.20353, -0.631186, -0.242215, -0.117523, ...]\n",
      "  T#35(model_1/BLOCK_2-2_BN_1/FusedBatchNormV3) shape:[240], type:FLOAT32 RO 960 bytes, buffer: 36, data:[0.0427378, 0.0743341, 0.0511247, 0.0502762, 0.0631282, ...]\n",
      "  T#36(model_1/BLOCK_2-2_BN_1/FusedBatchNormV31) shape:[240], type:FLOAT32 RO 960 bytes, buffer: 37, data:[-0.453432, 0.0377011, 0.029025, -1.10252, 1.40118, ...]\n",
      "  T#37(model_1/BLOCK_2-2_BN_3/FusedBatchNormV3) shape:[120], type:FLOAT32 RO 480 bytes, buffer: 38, data:[0.44491, 0.412983, 0.499194, 0.498357, 0.455934, ...]\n",
      "  T#38(model_1/BLOCK_2-2_BN_3/FusedBatchNormV31) shape:[120], type:FLOAT32 RO 480 bytes, buffer: 39, data:[-0.26637, 0.121703, 0.097885, -0.248943, 0.0860163, ...]\n",
      "  T#39(model_1/BLOCK_2-3_BN_1/FusedBatchNormV3) shape:[240], type:FLOAT32 RO 960 bytes, buffer: 40, data:[0.0519684, 0.0300522, 0.0498387, 0.0421653, 0.0495034, ...]\n",
      "  T#40(model_1/BLOCK_2-3_BN_1/FusedBatchNormV31) shape:[240], type:FLOAT32 RO 960 bytes, buffer: 41, data:[-0.624671, -0.705771, -0.00291157, -0.566265, 0.401846, ...]\n",
      "  T#41(model_1/BLOCK_2-3_BN_3/FusedBatchNormV3) shape:[120], type:FLOAT32 RO 480 bytes, buffer: 42, data:[0.764373, 0.543532, 0.692146, 0.815445, 0.541607, ...]\n",
      "  T#42(model_1/BLOCK_2-3_BN_3/FusedBatchNormV31) shape:[120], type:FLOAT32 RO 480 bytes, buffer: 43, data:[-0.334773, 0.358166, 0.595261, 0.789514, 0.299735, ...]\n",
      "  T#43(model_1/BLOCK_2-4_BN_1/FusedBatchNormV3) shape:[240], type:FLOAT32 RO 960 bytes, buffer: 44, data:[0.0400923, 0.0268514, 0.0356047, 0.0306164, 0.0325544, ...]\n",
      "  T#44(model_1/BLOCK_2-4_BN_1/FusedBatchNormV31) shape:[240], type:FLOAT32 RO 960 bytes, buffer: 45, data:[-0.0566794, -0.325451, -0.0834246, 0.0485905, -0.249119, ...]\n",
      "  T#45(model_1/BLOCK_2-4_BN_3/FusedBatchNormV3) shape:[120], type:FLOAT32 RO 480 bytes, buffer: 46, data:[0.878612, 0.595826, 0.752458, 0.769353, 0.855988, ...]\n",
      "  T#46(model_1/BLOCK_2-4_BN_3/FusedBatchNormV31) shape:[120], type:FLOAT32 RO 480 bytes, buffer: 47, data:[-1.07128, -0.719374, 0.521283, -0.52695, 0.637195, ...]\n",
      "  T#47(model_1/BLOCK_3-1_BN_1/FusedBatchNormV3) shape:[640], type:FLOAT32 RO 2560 bytes, buffer: 48, data:[0.0695646, 0.0555277, 0.0595143, 0.0618882, 0.0623795, ...]\n",
      "  T#48(model_1/BLOCK_3-1_BN_1/FusedBatchNormV31) shape:[640], type:FLOAT32 RO 2560 bytes, buffer: 49, data:[0.453774, -0.53238, 0.144864, 0.0808757, -0.600901, ...]\n",
      "  T#49(model_1/BLOCK_3-1_BN_3/FusedBatchNormV3) shape:[160], type:FLOAT32 RO 640 bytes, buffer: 50, data:[0.394457, 0.381189, 0.329165, 0.368043, 0.389706, ...]\n",
      "  T#50(model_1/BLOCK_3-1_BN_3/FusedBatchNormV31) shape:[160], type:FLOAT32 RO 640 bytes, buffer: 51, data:[0.028477, -0.247231, -0.271764, -0.436009, -0.0308246, ...]\n",
      "  T#51(model_1/BLOCK_3-2_BN_1/FusedBatchNormV3) shape:[640], type:FLOAT32 RO 2560 bytes, buffer: 52, data:[0.112116, 0.159339, 0.162716, 0.252225, 0.220685, ...]\n",
      "  T#52(model_1/BLOCK_3-2_BN_1/FusedBatchNormV31) shape:[640], type:FLOAT32 RO 2560 bytes, buffer: 53, data:[-0.530656, 0.133959, 0.382794, -0.28108, -0.106207, ...]\n",
      "  T#53(model_1/BLOCK_3-2_BN_3/FusedBatchNormV3) shape:[160], type:FLOAT32 RO 640 bytes, buffer: 54, data:[0.283559, 0.246089, 0.230343, 0.265336, 0.323128, ...]\n",
      "  T#54(model_1/BLOCK_3-2_BN_3/FusedBatchNormV31) shape:[160], type:FLOAT32 RO 640 bytes, buffer: 55, data:[-0.245983, -0.00641132, -0.0371882, 0.126828, -0.0948737, ...]\n",
      "  T#55(model_1/BLOCK_3-3_BN_1/FusedBatchNormV3) shape:[640], type:FLOAT32 RO 2560 bytes, buffer: 56, data:[0.144506, 0.12076, 0.149606, 0.0745747, 0.145628, ...]\n",
      "  T#56(model_1/BLOCK_3-3_BN_1/FusedBatchNormV31) shape:[640], type:FLOAT32 RO 2560 bytes, buffer: 57, data:[1.04701, 0.565516, -0.577064, 0.252158, -0.205602, ...]\n",
      "  T#57(model_1/BLOCK_3-3_BN_3/FusedBatchNormV3) shape:[160], type:FLOAT32 RO 640 bytes, buffer: 58, data:[0.267406, 0.25576, 0.252714, 0.240452, 0.30744, ...]\n",
      "  T#58(model_1/BLOCK_3-3_BN_3/FusedBatchNormV31) shape:[160], type:FLOAT32 RO 640 bytes, buffer: 59, data:[0.00867989, -0.0112237, -0.166217, 0.266443, -0.436209, ...]\n",
      "  T#59(model_1/BLOCK_3-4_BN_1/FusedBatchNormV3) shape:[640], type:FLOAT32 RO 2560 bytes, buffer: 60, data:[0.118934, 0.041895, 0.127009, 0.097867, 0.056087, ...]\n",
      "  T#60(model_1/BLOCK_3-4_BN_1/FusedBatchNormV31) shape:[640], type:FLOAT32 RO 2560 bytes, buffer: 61, data:[-0.280375, 0.916876, -0.271838, 0.260631, 0.065035, ...]\n",
      "  T#61(model_1/BLOCK_3-4_BN_3/FusedBatchNormV3) shape:[160], type:FLOAT32 RO 640 bytes, buffer: 62, data:[0.281396, 0.251337, 0.248814, 0.297997, 0.380178, ...]\n",
      "  T#62(model_1/BLOCK_3-4_BN_3/FusedBatchNormV31) shape:[160], type:FLOAT32 RO 640 bytes, buffer: 63, data:[0.0162673, -0.223772, -0.0793594, 0.196439, 0.0911259, ...]\n",
      "  T#63(model_1/BLOCK_3-5_BN_1/FusedBatchNormV3) shape:[640], type:FLOAT32 RO 2560 bytes, buffer: 64, data:[0.0654987, 0.0675247, 0.0192967, 0.0897804, 0.054354, ...]\n",
      "  T#64(model_1/BLOCK_3-5_BN_1/FusedBatchNormV31) shape:[640], type:FLOAT32 RO 2560 bytes, buffer: 65, data:[-1.13443, 0.0416078, -0.161235, -1.03465, -0.414547, ...]\n",
      "  T#65(model_1/BLOCK_3-5_BN_3/FusedBatchNormV3) shape:[160], type:FLOAT32 RO 640 bytes, buffer: 66, data:[0.295026, 0.259566, 0.291034, 0.29749, 0.367161, ...]\n",
      "  T#66(model_1/BLOCK_3-5_BN_3/FusedBatchNormV31) shape:[160], type:FLOAT32 RO 640 bytes, buffer: 67, data:[-0.0729527, -0.140453, -0.256495, 0.00789576, -0.0750102, ...]\n",
      "  T#67(model_1/BLOCK_4-1_BN_1/FusedBatchNormV3) shape:[1120], type:FLOAT32 RO 4480 bytes, buffer: 68, data:[0.05576, 0.0390297, 0.0263996, 0.0344951, 0.0707267, ...]\n",
      "  T#68(model_1/BLOCK_4-1_BN_1/FusedBatchNormV31) shape:[1120], type:FLOAT32 RO 4480 bytes, buffer: 69, data:[-0.501423, -0.781778, -1.06513, -0.807659, -0.451615, ...]\n",
      "  T#69(model_1/BLOCK_4-1_BN_3/FusedBatchNormV3) shape:[280], type:FLOAT32 RO 1120 bytes, buffer: 70, data:[0.176736, 0.159196, 0.137726, 0.16763, 0.127707, ...]\n",
      "  T#70(model_1/BLOCK_4-1_BN_3/FusedBatchNormV31) shape:[280], type:FLOAT32 RO 1120 bytes, buffer: 71, data:[0.00707362, -0.11092, -0.271591, -0.0480711, 0.1141, ...]\n",
      "  T#71(model_1/BLOCK_4-2_BN_1/FusedBatchNormV3) shape:[1120], type:FLOAT32 RO 4480 bytes, buffer: 72, data:[0.134551, 0.206991, 0.0149108, 0.191407, 0.0471444, ...]\n",
      "  T#72(model_1/BLOCK_4-2_BN_1/FusedBatchNormV31) shape:[1120], type:FLOAT32 RO 4480 bytes, buffer: 73, data:[0.214278, 0.192685, 0.0268124, -0.538281, -0.227158, ...]\n",
      "  T#73(model_1/BLOCK_4-2_BN_3/FusedBatchNormV3) shape:[280], type:FLOAT32 RO 1120 bytes, buffer: 74, data:[0.115839, 0.124925, 0.119503, 0.108775, 0.121917, ...]\n",
      "  T#74(model_1/BLOCK_4-2_BN_3/FusedBatchNormV31) shape:[280], type:FLOAT32 RO 1120 bytes, buffer: 75, data:[-0.0310575, -0.01638, -0.0170814, -0.0743111, -0.0226622, ...]\n",
      "  T#75(model_1/BLOCK_4-3_BN_1/FusedBatchNormV3) shape:[1120], type:FLOAT32 RO 4480 bytes, buffer: 76, data:[0.152431, 0.11323, 0.0156838, 0.125614, 0.136266, ...]\n",
      "  T#76(model_1/BLOCK_4-3_BN_1/FusedBatchNormV31) shape:[1120], type:FLOAT32 RO 4480 bytes, buffer: 77, data:[-0.212481, -1.1889, -0.0504375, 0.146119, -0.779445, ...]\n",
      "  T#77(model_1/BLOCK_4-3_BN_3/FusedBatchNormV3) shape:[280], type:FLOAT32 RO 1120 bytes, buffer: 78, data:[0.175299, 0.164073, 0.181724, 0.166745, 0.173134, ...]\n",
      "  T#78(model_1/BLOCK_4-3_BN_3/FusedBatchNormV31) shape:[280], type:FLOAT32 RO 1120 bytes, buffer: 79, data:[0.0840644, -0.110269, -0.105898, -0.161293, -0.123441, ...]\n",
      "  T#79(model_1/BLOCK_4-4_BN_1/FusedBatchNormV3) shape:[1120], type:FLOAT32 RO 4480 bytes, buffer: 80, data:[0.121358, 0.111509, 0.107304, 0.139252, 0.13843, ...]\n",
      "  T#80(model_1/BLOCK_4-4_BN_1/FusedBatchNormV31) shape:[1120], type:FLOAT32 RO 4480 bytes, buffer: 81, data:[-1.24407, -0.0376666, -1.2601, -0.504979, -0.504578, ...]\n",
      "  T#81(model_1/BLOCK_4-4_BN_3/FusedBatchNormV3) shape:[280], type:FLOAT32 RO 1120 bytes, buffer: 82, data:[0.231961, 0.204358, 0.265425, 0.210788, 0.265652, ...]\n",
      "  T#82(model_1/BLOCK_4-4_BN_3/FusedBatchNormV31) shape:[280], type:FLOAT32 RO 1120 bytes, buffer: 83, data:[-0.303293, -0.566926, -0.164678, -0.409574, -0.311323, ...]\n",
      "  T#83(model_1/BNORM_POST_NOQUANT/FusedBatchNormV3) shape:[280], type:FLOAT32 RO 1120 bytes, buffer: 84, data:[0.346286, 0.419046, 0.374378, 0.392926, 0.321534, ...]\n",
      "  T#84(model_1/BNORM_POST_NOQUANT/FusedBatchNormV31) shape:[280], type:FLOAT32 RO 1120 bytes, buffer: 85, data:[0.374975, 0.296045, 0.377884, 0.231385, 0.356721, ...]\n",
      "  T#85(model_1/POST_BN_1/FusedBatchNormV3) shape:[420], type:FLOAT32 RO 1680 bytes, buffer: 86, data:[0.0619425, 0.0666335, 0.0650078, 0.0599834, 0.0691542, ...]\n",
      "  T#86(model_1/POST_BN_1/FusedBatchNormV31) shape:[420], type:FLOAT32 RO 1680 bytes, buffer: 87, data:[0.703291, 0.151619, 0.929611, 1.05721, -0.0952325, ...]\n",
      "  T#87(model_1/dense/MatMul) shape:[2, 420], type:FLOAT32 RO 3360 bytes, buffer: 88, data:[0.0036203, 0.0829183, 0.107548, 0.132597, -0.108428, ...]\n",
      "  T#88(model_1/quant_CONV_0/Conv2D;model_1/quant_CONV_0/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[30, 4, 8, 1], type:FLOAT32 RO 3840 bytes, buffer: 89, data:[-0.294333, -0.599987, -0.543385, -0.464141, -0.0679231, ...]\n",
      "  T#89(model_1/quant_pool_0_CONV/Conv2D;model_1/quant_pool_0_CONV/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[30, 1, 1, 60], type:FLOAT32 RO 7200 bytes, buffer: 90, data:[0.451733, 0.182431, -0.321426, 0.0955589, 0.0434359, ...]\n",
      "  T#90(model_1/quant_BLOCK_1-1_CONV_3/Conv2D;model_1/quant_BLOCK_1-1_CONV_3/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[60, 1, 1, 60], type:FLOAT32 RO 14400 bytes, buffer: 91, data:[0.193869, -0.0484671, 0.605839, 0.306958, -0.436204, ...]\n",
      "  T#91(model_1/quant_BLOCK_1-2_CONV_1/Conv2D;model_1/quant_BLOCK_1-2_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[60, 3, 3, 60], type:FLOAT32 RO 129600 bytes, buffer: 92, data:[0.0496653, -0.0496653, 0, -0.186245, 0.0496653, ...]\n",
      "  T#92(model_1/quant_BLOCK_1-2_CONV_3/Conv2D;model_1/quant_BLOCK_1-2_CONV_3/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[60, 1, 1, 60], type:FLOAT32 RO 14400 bytes, buffer: 93, data:[-0.174247, -0.481255, 0.182545, -0.257222, -0.381685, ...]\n",
      "  T#93(model_1/quant_BLOCK_1-3_CONV_3/Conv2D;model_1/quant_BLOCK_1-3_CONV_3/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[60, 1, 1, 60], type:FLOAT32 RO 14400 bytes, buffer: 94, data:[-1.10777, 0, -0.409964, 0.0174453, -0.558248, ...]\n",
      "  T#94(model_1/quant_BLOCK_2-1_CONV_3/Conv2D;model_1/quant_BLOCK_2-1_CONV_3/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[120, 1, 1, 240], type:FLOAT32 RO 115200 bytes, buffer: 95, data:[-0.588423, 0.231166, 0.115583, 0.168121, 0.189136, ...]\n",
      "  T#95(model_1/quant_BLOCK_2-2_CONV_1/Conv2D;model_1/quant_BLOCK_2-2_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[240, 3, 3, 120], type:FLOAT32 RO 1036800 bytes, buffer: 96, data:[0.0892467, -0.118996, 0.594978, 0.163619, -0.118996, ...]\n",
      "  T#96(model_1/quant_BLOCK_2-2_CONV_3/Conv2D;model_1/quant_BLOCK_2-2_CONV_3/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[120, 1, 1, 240], type:FLOAT32 RO 115200 bytes, buffer: 97, data:[-0.515246, -0.350806, 0.0767388, -0.109627, -0.142515, ...]\n",
      "  T#97(model_1/quant_BLOCK_2-3_CONV_3/Conv2D;model_1/quant_BLOCK_2-3_CONV_3/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[120, 1, 1, 240], type:FLOAT32 RO 115200 bytes, buffer: 98, data:[0.206814, -0.272618, 0.864857, 0.404227, 0.695646, ...]\n",
      "  T#98(model_1/quant_BLOCK_2-4_CONV_3/Conv2D;model_1/quant_BLOCK_2-4_CONV_3/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[120, 1, 1, 240], type:FLOAT32 RO 115200 bytes, buffer: 99, data:[0.583704, -0.199899, -0.559716, -0.151923, 0.0719635, ...]\n",
      "  T#99(model_1/quant_BLOCK_3-1_SE_CONV_1/Conv2D;model_1/quant_BLOCK_3-1_SE_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[40, 1, 1, 640], type:FLOAT32 RO 102400 bytes, buffer: 100, data:[-0.110213, -0.251915, 0.251915, 0.196808, -0.149574, ...]\n",
      "  T#100(model_1/quant_BLOCK_3-1_CONV_3/Conv2D;model_1/quant_BLOCK_3-1_CONV_3/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[160, 1, 1, 640], type:FLOAT32 RO 409600 bytes, buffer: 101, data:[0.468411, -0.259121, -0.219256, -0.398648, 0.388682, ...]\n",
      "  T#101(model_1/quant_BLOCK_3-2_CONV_1/Conv2D;model_1/quant_BLOCK_3-2_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[640, 1, 1, 160], type:FLOAT32 RO 409600 bytes, buffer: 102, data:[0.530467, -0.23101, -0.00855591, -0.37646, -0.436352, ...]\n",
      "  T#102(model_1/quant_BLOCK_3-2_SE_CONV_1/Conv2D;model_1/quant_BLOCK_3-2_SE_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[40, 1, 1, 640], type:FLOAT32 RO 102400 bytes, buffer: 103, data:[0.314793, 0.143088, 0.467419, -0.372028, 0.429263, ...]\n",
      "  T#103(model_1/quant_BLOCK_3-2_CONV_3/Conv2D;model_1/quant_BLOCK_3-2_CONV_3/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[160, 1, 1, 640], type:FLOAT32 RO 409600 bytes, buffer: 104, data:[0.237999, -0.72533, 0.385332, 0.430665, 0.0453331, ...]\n",
      "  T#104(model_1/quant_BLOCK_3-3_SE_CONV_1/Conv2D;model_1/quant_BLOCK_3-3_SE_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[40, 1, 1, 640], type:FLOAT32 RO 102400 bytes, buffer: 105, data:[0.0239525, -0.0718576, 1.52099, -0.0239525, 0.119763, ...]\n",
      "  T#105(model_1/quant_BLOCK_3-3_CONV_3/Conv2D;model_1/quant_BLOCK_3-3_CONV_3/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[160, 1, 1, 640], type:FLOAT32 RO 409600 bytes, buffer: 106, data:[0.816519, 0.369745, 0.446775, 0.246496, -0.15406, ...]\n",
      "  T#106(model_1/quant_BLOCK_3-4_SE_CONV_1/Conv2D;model_1/quant_BLOCK_3-4_SE_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[40, 1, 1, 640], type:FLOAT32 RO 102400 bytes, buffer: 107, data:[0.334974, -0.0176302, -0.211562, 0.299713, -0.158672, ...]\n",
      "  T#107(model_1/quant_BLOCK_3-4_CONV_3/Conv2D;model_1/quant_BLOCK_3-4_CONV_3/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[160, 1, 1, 640], type:FLOAT32 RO 409600 bytes, buffer: 108, data:[0.050916, -0.627964, 0.390356, 0.08486, -0.08486, ...]\n",
      "  T#108(model_1/quant_BLOCK_3-5_SE_CONV_1/Conv2D;model_1/quant_BLOCK_3-5_SE_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[40, 1, 1, 640], type:FLOAT32 RO 102400 bytes, buffer: 109, data:[0.432953, -0.111014, -0.0444055, 0.310838, 0.0444055, ...]\n",
      "  T#109(model_1/quant_BLOCK_3-5_CONV_3/Conv2D;model_1/quant_BLOCK_3-5_CONV_3/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[160, 1, 1, 640], type:FLOAT32 RO 409600 bytes, buffer: 110, data:[0.127954, 0.479826, -0.0799711, -0.383861, -0.831699, ...]\n",
      "  T#110(model_1/quant_BLOCK_4-1_SE_CONV_1/Conv2D;model_1/quant_BLOCK_4-1_SE_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[70, 1, 1, 1120], type:FLOAT32 RO 313600 bytes, buffer: 111, data:[-0.107993, -0.48597, -0.080995, -0.107993, 0.215987, ...]\n",
      "  T#111(model_1/quant_BLOCK_4-1_CONV_3/Conv2D;model_1/quant_BLOCK_4-1_CONV_3/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[280, 1, 1, 1120], type:FLOAT32 RO 1254400 bytes, buffer: 112, data:[0.869552, 0.179907, -0.239877, -0.359815, -0.119938, ...]\n",
      "  T#112(model_1/quant_BLOCK_4-2_CONV_1/Conv2D;model_1/quant_BLOCK_4-2_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[1120, 1, 1, 280], type:FLOAT32 RO 1254400 bytes, buffer: 113, data:[0.570157, 0.223105, 0.582552, 0.247894, 0.396631, ...]\n",
      "  T#113(model_1/quant_BLOCK_4-2_SE_CONV_1/Conv2D;model_1/quant_BLOCK_4-2_SE_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[70, 1, 1, 1120], type:FLOAT32 RO 313600 bytes, buffer: 114, data:[-0.432328, -0.0345863, -0.146992, 0.380449, -0.440975, ...]\n",
      "  T#114(model_1/quant_BLOCK_4-2_CONV_3/Conv2D;model_1/quant_BLOCK_4-2_CONV_3/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[280, 1, 1, 1120], type:FLOAT32 RO 1254400 bytes, buffer: 115, data:[-0.214229, -0.708604, 0.164792, -0.181271, 0.543812, ...]\n",
      "  T#115(model_1/quant_BLOCK_4-3_SE_CONV_1/Conv2D;model_1/quant_BLOCK_4-3_SE_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[70, 1, 1, 1120], type:FLOAT32 RO 313600 bytes, buffer: 116, data:[-0.491611, -0.194057, -0.258742, -0.258742, 0.0258742, ...]\n",
      "  T#116(model_1/quant_BLOCK_4-3_CONV_3/Conv2D;model_1/quant_BLOCK_4-3_CONV_3/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[280, 1, 1, 1120], type:FLOAT32 RO 1254400 bytes, buffer: 117, data:[-0.584717, 0.38372, 0.511627, 0.182724, 0.5299, ...]\n",
      "  T#117(model_1/quant_BLOCK_4-4_SE_CONV_1/Conv2D;model_1/quant_BLOCK_4-4_SE_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[70, 1, 1, 1120], type:FLOAT32 RO 313600 bytes, buffer: 118, data:[-0.0138714, -0.0554855, -0.0554855, 0.0832283, 0.0554855, ...]\n",
      "  T#118(model_1/quant_BLOCK_4-4_CONV_3/Conv2D;model_1/quant_BLOCK_4-4_CONV_3/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[280, 1, 1, 1120], type:FLOAT32 RO 1254400 bytes, buffer: 119, data:[0.100043, 0.250108, 0.166739, 0.216761, -0.550239, ...]\n",
      "  T#119(model_1/ADVANCED_SPEC1/ExpandDims/dim) shape:[], type:INT32 RO 4 bytes, buffer: 120, data:[-1]\n",
      "  T#120(model_1/ADVANCED_SPEC1/stft/frame/Const) shape:[], type:INT32 RO 4 bytes, buffer: 121, data:[0]\n",
      "  T#121(model_1/ADVANCED_SPEC1/stft/frame/Reshape/shape_1) shape:[0], type:INT32\n",
      "  T#122(model_1/ADVANCED_SPEC1/stft/frame/concat_1/values_1/1) shape:[], type:INT32 RO 4 bytes, buffer: 123, data:[8]\n",
      "  T#123(model_1/ADVANCED_SPEC1/stft/frame/strided_slice/stack_1) shape:[1], type:INT32 RO 4 bytes, buffer: 121, data:[0]\n",
      "  T#124(model_1/ADVANCED_SPEC1/stft/frame/strided_slice/stack_2) shape:[1], type:INT32 RO 4 bytes, buffer: 125, data:[1]\n",
      "  T#125(model_1/ADVANCED_SPEC1/stft/frame/zeros_like) shape:[2], type:INT32 RO 8 bytes, buffer: 126, data:[0, 0]\n",
      "  T#126(model_1/ADVANCED_SPEC1/stft/frame_length) shape:[], type:INT32 RO 4 bytes, buffer: 127, data:[512]\n",
      "  T#127(model_1/ADVANCED_SPEC1/stft/frame_step) shape:[], type:INT32 RO 4 bytes, buffer: 128, data:[280]\n",
      "  T#128(model_1/ADVANCED_SPEC1/strided_slice/stack) shape:[3], type:INT32 RO 12 bytes, buffer: 129, data:[0, 0, 0]\n",
      "  T#129(model_1/ADVANCED_SPEC1/strided_slice/stack_1) shape:[3], type:INT32 RO 12 bytes, buffer: 130, data:[0, 0, 128]\n",
      "  T#130(model_1/ADVANCED_SPEC1/strided_slice/stack_2) shape:[3], type:INT32 RO 12 bytes, buffer: 131, data:[1, 1, 1]\n",
      "  T#131(model_1/ADVANCED_SPEC1/transpose/perm) shape:[3], type:INT32 RO 12 bytes, buffer: 132, data:[0, 2, 1]\n",
      "  T#132(model_1/BLOCK_3-1_SE_AVG_POOL_1/Mean/reduction_indices) shape:[2], type:INT32 RO 8 bytes, buffer: 133, data:[1, 2]\n",
      "  T#133(model_1/GLOBAL_LME_POOL/Reshape/shape) shape:[2], type:INT32 RO 8 bytes, buffer: 134, data:[-1, 420]\n",
      "  T#134(model_1/quant_BLOCK_3-1_SE_RESHAPE/Reshape/shape/3) shape:[], type:INT32 RO 4 bytes, buffer: 135, data:[640]\n",
      "  T#135(model_1/quant_BLOCK_4-1_SE_RESHAPE/Reshape/shape/3) shape:[], type:INT32 RO 4 bytes, buffer: 136, data:[1120]\n",
      "  T#136(model_1/ADVANCED_SPEC1/stft/frame/floordiv_2) shape:[], type:INT32 RO 4 bytes, buffer: 137, data:[35]\n",
      "  T#137(model_1/ADVANCED_SPEC1/stft/frame/ones_like) shape:[2], type:INT32 RO 8 bytes, buffer: 138, data:[1, 1]\n",
      "  T#138(model_1/ADVANCED_SPEC1/stft/frame/packed) shape:[3], type:INT32 RO 12 bytes, buffer: 139, data:[1, 1, 0]\n",
      "  T#139(model_1/ADVANCED_SPEC1/stft/frame/Reshape_3) shape:[1, 64], type:INT32 RO 256 bytes, buffer: 140, data:[0, 1, 2, 3, 4, ...]\n",
      "  T#140(model_1/ADVANCED_SPEC1/stft/frame/strided_slice) shape:[], type:INT32 RO 4 bytes, buffer: 125, data:[1]\n",
      "  T#141(model_1/dense/BiasAdd/ReadVariableOp) shape:[2], type:FLOAT32 RO 8 bytes, buffer: 142, data:[0.0141404, -0.0158713]\n",
      "  T#142(model_1/quant_BLOCK_4-4_CONV_3/Conv2D) shape:[280], type:FLOAT32 RO 1120 bytes, buffer: 143, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#143(model_1/quant_BLOCK_4-4_SE_CONV_1/Conv2D) shape:[70], type:FLOAT32 RO 280 bytes, buffer: 144, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#144(model_1/quant_BLOCK_3-5_CONV_3/Conv2D) shape:[160], type:FLOAT32 RO 640 bytes, buffer: 145, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#145(model_1/quant_BLOCK_3-5_SE_CONV_1/Conv2D) shape:[40], type:FLOAT32 RO 160 bytes, buffer: 146, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#146(model_1/quant_BLOCK_2-4_CONV_3/Conv2D) shape:[120], type:FLOAT32 RO 480 bytes, buffer: 147, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#147(model_1/quant_BLOCK_2-4_CONV_1/Conv2D) shape:[240], type:FLOAT32 RO 960 bytes, buffer: 148, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#148(model_1/quant_BLOCK_1-3_CONV_3/Conv2D) shape:[60], type:FLOAT32 RO 240 bytes, buffer: 149, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#149(model_1/quant_pool_0_CONV/Conv2D) shape:[30], type:FLOAT32 RO 120 bytes, buffer: 150, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#150(model_1/ADVANCED_SPEC1/stft/rfft) shape:[2], type:INT32 RO 8 bytes, buffer: 151, data:[1, 512]\n",
      "  T#151(model_1/ADVANCED_SPEC1/stft/rfft1) shape:[], type:INT32 RO 4 bytes, buffer: 152, data:[-2]\n",
      "  T#152(model_1/quant_BLOCK_4-4_SE_CONV_2/Conv2D) shape:[1120], type:FLOAT32 RO 4480 bytes, buffer: 153, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#153(model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D) shape:[640], type:FLOAT32 RO 2560 bytes, buffer: 154, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#154(model_1/BLOCK_3-1_BN_2_NOQUANT/FusedBatchNormV3) shape:[640], type:FLOAT32 RO 2560 bytes, buffer: 155, data:[0.507101, 0.170648, 0.0986953, 0.264744, 0.368448, ...]\n",
      "  T#155(model_1/BLOCK_3-2_BN_2_NOQUANT/FusedBatchNormV3) shape:[640], type:FLOAT32 RO 2560 bytes, buffer: 156, data:[0.156241, 0.10379, -0.12009, 0.251382, -0.321336, ...]\n",
      "  T#156(model_1/BLOCK_3-3_BN_2_NOQUANT/FusedBatchNormV3) shape:[640], type:FLOAT32 RO 2560 bytes, buffer: 157, data:[-0.052932, -0.15776, -0.162979, -1.74901, -0.0601944, ...]\n",
      "  T#157(model_1/BLOCK_3-4_BN_2_NOQUANT/FusedBatchNormV3) shape:[640], type:FLOAT32 RO 2560 bytes, buffer: 158, data:[-0.167197, -0.627172, -0.118439, -0.0867302, -0.0966517, ...]\n",
      "  T#158(model_1/BLOCK_3-5_BN_2_NOQUANT/FusedBatchNormV3) shape:[640], type:FLOAT32 RO 2560 bytes, buffer: 159, data:[0.199295, -0.693983, -0.0609956, -0.305395, -0.0527331, ...]\n",
      "  T#159(model_1/BLOCK_4-1_BN_2_NOQUANT/FusedBatchNormV3) shape:[1120], type:FLOAT32 RO 4480 bytes, buffer: 160, data:[-0.213642, 0.0989731, 0.289327, 0.228966, -0.0641489, ...]\n",
      "  T#160(model_1/BLOCK_4-2_BN_2_NOQUANT/FusedBatchNormV3) shape:[1120], type:FLOAT32 RO 4480 bytes, buffer: 161, data:[0.00732364, -0.15787, -0.631158, -0.106697, 0.0196075, ...]\n",
      "  T#161(model_1/BLOCK_4-3_BN_2_NOQUANT/FusedBatchNormV3) shape:[1120], type:FLOAT32 RO 4480 bytes, buffer: 162, data:[0.0843629, -0.0535078, -0.136284, -0.35959, 0.137284, ...]\n",
      "  T#162(model_1/BLOCK_4-4_BN_2_NOQUANT/FusedBatchNormV3) shape:[1120], type:FLOAT32 RO 4480 bytes, buffer: 163, data:[0.03562, -0.442556, 0.242526, -0.189173, -0.137097, ...]\n",
      "  T#163(model_1/quant_pool_0_CONV/BiasAdd/ReadVariableOp) shape:[30], type:FLOAT32 RO 120 bytes, buffer: 164, data:[0.177993, -0.00336044, 1.49043, 0.208713, -0.283743, ...]\n",
      "  T#164(model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D1) shape:[640], type:INT32 RO 2560 bytes, buffer: 154, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#165(model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D2) shape:[640], type:INT32 RO 2560 bytes, buffer: 154, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#166(model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D3) shape:[640], type:INT32 RO 2560 bytes, buffer: 154, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#167(model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D4) shape:[640], type:INT32 RO 2560 bytes, buffer: 154, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#168(model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D5) shape:[640], type:INT32 RO 2560 bytes, buffer: 154, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#169(model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D6) shape:[640], type:INT32 RO 2560 bytes, buffer: 154, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#170(model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D7) shape:[640], type:INT32 RO 2560 bytes, buffer: 154, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#171(model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D8) shape:[640], type:INT32 RO 2560 bytes, buffer: 154, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#172(model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D9) shape:[640], type:INT32 RO 2560 bytes, buffer: 154, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#173(model_1/quant_BLOCK_4-4_SE_CONV_2/Conv2D1) shape:[1120], type:INT32 RO 4480 bytes, buffer: 153, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#174(model_1/quant_BLOCK_4-4_SE_CONV_2/Conv2D2) shape:[1120], type:INT32 RO 4480 bytes, buffer: 153, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#175(model_1/quant_BLOCK_4-4_SE_CONV_2/Conv2D3) shape:[1120], type:INT32 RO 4480 bytes, buffer: 153, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#176(model_1/quant_BLOCK_4-4_SE_CONV_2/Conv2D4) shape:[1120], type:INT32 RO 4480 bytes, buffer: 153, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#177(model_1/quant_BLOCK_4-4_SE_CONV_2/Conv2D5) shape:[1120], type:INT32 RO 4480 bytes, buffer: 153, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#178(model_1/quant_BLOCK_4-4_SE_CONV_2/Conv2D6) shape:[1120], type:INT32 RO 4480 bytes, buffer: 153, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#179(model_1/quant_BLOCK_4-4_SE_CONV_2/Conv2D7) shape:[1120], type:INT32 RO 4480 bytes, buffer: 153, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#180(model_1/quant_BLOCK_1-3_CONV_3/Conv2D1) shape:[60], type:INT32 RO 240 bytes, buffer: 149, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#181(model_1/quant_BLOCK_1-3_CONV_3/Conv2D2) shape:[60], type:INT32 RO 240 bytes, buffer: 149, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#182(model_1/quant_BLOCK_2-4_CONV_1/Conv2D1) shape:[240], type:INT32 RO 960 bytes, buffer: 148, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#183(model_1/quant_BLOCK_2-4_CONV_1/Conv2D2) shape:[240], type:INT32 RO 960 bytes, buffer: 148, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#184(model_1/quant_BLOCK_2-4_CONV_1/Conv2D3) shape:[240], type:INT32 RO 960 bytes, buffer: 148, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#185(model_1/quant_POST_CONV_1/Conv2D) shape:[420], type:INT32 RO 1680 bytes, buffer: 186, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#186(model_1/ADVANCED_SPEC1/Min) shape_signature:[-1, 1], type:FLOAT32\n",
      "  T#187(model_1/ADVANCED_SPEC1/Sub) shape_signature:[-1, 144000], type:FLOAT32\n",
      "  T#188(model_1/ADVANCED_SPEC1/Max) shape_signature:[-1, 1], type:FLOAT32\n",
      "  T#189(model_1/ADVANCED_SPEC1/add) shape_signature:[-1, 1], type:FLOAT32\n",
      "  T#190(model_1/ADVANCED_SPEC1/truediv) shape_signature:[-1, 144000], type:FLOAT32\n",
      "  T#191(model_1/ADVANCED_SPEC1/stft/frame/Shape) shape:[2], type:INT32\n",
      "  T#192(model_1/ADVANCED_SPEC1/stft/frame/split) shape:[1], type:INT32\n",
      "  T#193(model_1/ADVANCED_SPEC1/stft/frame/split1) shape:[1], type:INT32\n",
      "  T#194(model_1/ADVANCED_SPEC1/stft/frame/split2) shape:[0], type:INT32\n",
      "  T#195(model_1/ADVANCED_SPEC1/stft/frame/Reshape) shape:[], type:INT32\n",
      "  T#196(model_1/ADVANCED_SPEC1/stft/frame/floordiv_3) shape:[], type:INT32\n",
      "  T#197(model_1/ADVANCED_SPEC1/stft/frame/concat_1/values_1) shape:[2], type:INT32\n",
      "  T#198(model_1/ADVANCED_SPEC1/stft/frame/mul) shape:[], type:INT32\n",
      "  T#199(model_1/ADVANCED_SPEC1/stft/frame/concat/values_1) shape:[1], type:INT32\n",
      "  T#200(model_1/ADVANCED_SPEC1/stft/frame/concat) shape:[2], type:INT32\n",
      "  T#201(model_1/ADVANCED_SPEC1/stft/frame/concat_1) shape:[3], type:INT32\n",
      "  T#202(model_1/ADVANCED_SPEC1/stft/frame/StridedSlice) shape_signature:[-1, -1], type:FLOAT32\n",
      "  T#203(model_1/ADVANCED_SPEC1/stft/frame/Reshape_1) shape_signature:[-1, -1, 8], type:FLOAT32\n",
      "  T#204(model_1/ADVANCED_SPEC1/stft/frame/sub_2) shape:[], type:INT32\n",
      "  T#205(model_1/ADVANCED_SPEC1/stft/frame/floordiv) shape:[], type:INT32\n",
      "  T#206(model_1/ADVANCED_SPEC1/stft/frame/add) shape:[], type:INT32\n",
      "  T#207(model_1/ADVANCED_SPEC1/stft/frame/Maximum) shape:[], type:INT32\n",
      "  T#208(model_1/ADVANCED_SPEC1/stft/frame/Reshape_2/shape) shape:[2], type:INT32\n",
      "  T#209(model_1/ADVANCED_SPEC1/stft/frame/packed_1) shape:[2], type:INT32\n",
      "  T#210(model_1/ADVANCED_SPEC1/stft/frame/concat_2) shape:[3], type:INT32\n",
      "  T#211(model_1/ADVANCED_SPEC1/stft/frame/range_1) shape_signature:[-1], type:INT32\n",
      "  T#212(model_1/ADVANCED_SPEC1/stft/frame/mul_1) shape_signature:[-1], type:INT32\n",
      "  T#213(model_1/ADVANCED_SPEC1/stft/frame/Reshape_2) shape_signature:[-1, 1], type:INT32\n",
      "  T#214(model_1/ADVANCED_SPEC1/stft/frame/add_1) shape_signature:[-1, 64], type:INT32\n",
      "  T#215(model_1/ADVANCED_SPEC1/stft/frame/GatherV2;model_1/ADVANCED_SPEC1/stft/frame/strided_slice) shape_signature:[-1, -1, 64, 8], type:FLOAT32\n",
      "  T#216(model_1/ADVANCED_SPEC1/stft/frame/Reshape_4) shape_signature:[-1, -1, 512], type:FLOAT32\n",
      "  T#217(model_1/ADVANCED_SPEC1/stft/mul) shape_signature:[-1, -1, 512], type:FLOAT32\n",
      "  T#218(model_1/ADVANCED_SPEC1/stft/rfft2) shape_signature:[-1, -1, 1, 512], type:FLOAT32\n",
      "  T#219(model_1/ADVANCED_SPEC1/stft/rfft3) shape_signature:[-1, -1, 1, 257], type:COMPLEX64\n",
      "  T#220(model_1/ADVANCED_SPEC1/stft/rfft4) shape_signature:[-1, -1, 257], type:COMPLEX64\n",
      "  T#221(model_1/ADVANCED_SPEC1/Abs) shape_signature:[-1, -1, 257], type:FLOAT32\n",
      "  T#222(model_1/ADVANCED_SPEC1/strided_slice) shape_signature:[-1, -1, 128], type:FLOAT32\n",
      "  T#223(model_1/ADVANCED_SPEC1/Pow;model_1/ADVANCED_SPEC1/Pow/y) shape_signature:[-1, -1, 128], type:FLOAT32\n",
      "  T#224(model_1/ADVANCED_SPEC1/Pow_1) shape_signature:[-1, -1, 128], type:FLOAT32\n",
      "  T#225(model_1/ADVANCED_SPEC1/transpose) shape_signature:[-1, 128, -1], type:FLOAT32\n",
      "  T#226(model_1/ADVANCED_SPEC1/ExpandDims) shape_signature:[-1, 128, -1, 1], type:FLOAT32\n",
      "  T#227(model_1/BNORM_SPEC_NOQUANT/FusedBatchNormV32) shape_signature:[-1, 128, -1, 1], type:FLOAT32\n",
      "  T#228(model_1/BNORM_SPEC_NOQUANT/FusedBatchNormV33) shape_signature:[-1, 128, -1, 1], type:FLOAT32\n",
      "  T#229(model_1/quant_CONV_0/Conv2D) shape_signature:[-1, 64, -1, 30], type:FLOAT32\n",
      "  T#230(model_1/quant_CONV_0/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_CONV_0/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_CONV_0/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 64, -1, 30], type:INT8\n",
      "  T#231(model_1/quant_CONV_0/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 64, -1, 30], type:FLOAT32\n",
      "  T#232(model_1/BNORM_0/FusedBatchNormV32) shape_signature:[-1, 64, -1, 30], type:FLOAT32\n",
      "  T#233(model_1/ACT_0/Relu;model_1/BNORM_0/FusedBatchNormV3) shape_signature:[-1, 64, -1, 30], type:FLOAT32\n",
      "  T#234(model_1/pool_0_AVG/AvgPool) shape_signature:[-1, 64, -1, 30], type:FLOAT32\n",
      "  T#235(model_1/pool_0_MAX/MaxPool) shape_signature:[-1, 64, -1, 30], type:FLOAT32\n",
      "  T#236(model_1/pool_0_CONCAT/concat) shape_signature:[-1, 64, -1, 60], type:FLOAT32\n",
      "  T#237(model_1/quant_pool_0_CONV/BiasAdd;model_1/quant_pool_0_CONV/Conv2D;model_1/quant_pool_0_CONV/BiasAdd/ReadVariableOp) shape_signature:[-1, 64, -1, 30], type:FLOAT32\n",
      "  T#238(model_1/quant_pool_0_CONV/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_pool_0_CONV/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_pool_0_CONV/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 64, -1, 30], type:INT8\n",
      "  T#239(model_1/quant_BLOCK_1-1_CONV_1/Conv2D;model_1/quant_BLOCK_1-1_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[60, 3, 3, 30], type:INT8 RO 16200 bytes, buffer: 240, data:[., ), ., ., ., ...]\n",
      "  T#240(model_1/quant_BLOCK_1-1_CONV_1/Conv2D) shape_signature:[-1, 32, -1, 60], type:INT8\n",
      "  T#241(model_1/quant_BLOCK_1-1_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#242(model_1/BLOCK_1-1_BN_1/FusedBatchNormV32) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#243(model_1/BLOCK_1-1_BN_1/FusedBatchNormV33) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#244(model_1/BLOCK_1-1_ACT_1/Sigmoid) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#245(model_1/BLOCK_1-1_ACT_1/mul_1) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#246(model_1/quant_BLOCK_1-1_CONV_3/Conv2D) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#247(model_1/quant_BLOCK_1-1_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_1-1_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_1-1_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 32, -1, 60], type:INT8\n",
      "  T#248(model_1/quant_BLOCK_1-1_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#249(model_1/BLOCK_1-1_BN_3/FusedBatchNormV32) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#250(model_1/BLOCK_1-1_BN_3/FusedBatchNormV33) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#251(model_1/quant_BLOCK_1-2_CONV_1/Conv2D) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#252(model_1/quant_BLOCK_1-2_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_1-2_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_1-2_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 32, -1, 60], type:INT8\n",
      "  T#253(model_1/quant_BLOCK_1-2_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#254(model_1/BLOCK_1-2_BN_1/FusedBatchNormV32) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#255(model_1/BLOCK_1-2_BN_1/FusedBatchNormV33) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#256(model_1/BLOCK_1-2_ACT_1/Sigmoid) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#257(model_1/BLOCK_1-2_ACT_1/mul_1) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#258(model_1/quant_BLOCK_1-2_CONV_3/Conv2D) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#259(model_1/quant_BLOCK_1-2_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_1-2_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_1-2_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 32, -1, 60], type:INT8\n",
      "  T#260(model_1/quant_BLOCK_1-2_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#261(model_1/BLOCK_1-2_BN_3/FusedBatchNormV32) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#262(model_1/BLOCK_1-2_BN_3/FusedBatchNormV33) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#263(model_1/quant_BLOCK_1-2_ADD/add) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#264(model_1/quant_BLOCK_1-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_1-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_1-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 32, -1, 60], type:INT8\n",
      "  T#265(model_1/quant_BLOCK_1-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_1-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_1-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_11) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#266(model_1/quant_BLOCK_1-3_CONV_1/Conv2D;model_1/quant_BLOCK_1-3_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[60, 3, 3, 60], type:INT8 RO 32400 bytes, buffer: 267, data:[\n",
      ", $, ., ., ), ...]\n",
      "  T#267(model_1/quant_BLOCK_1-3_CONV_1/Conv2D) shape_signature:[-1, 32, -1, 60], type:INT8\n",
      "  T#268(model_1/quant_BLOCK_1-3_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#269(model_1/BLOCK_1-3_BN_1/FusedBatchNormV32) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#270(model_1/BLOCK_1-3_BN_1/FusedBatchNormV33) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#271(model_1/BLOCK_1-3_ACT_1/Sigmoid) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#272(model_1/BLOCK_1-3_ACT_1/mul_1) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#273(model_1/quant_BLOCK_1-3_CONV_3/Conv2D3) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#274(model_1/quant_BLOCK_1-3_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_1-3_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_1-3_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 32, -1, 60], type:INT8\n",
      "  T#275(model_1/quant_BLOCK_1-3_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#276(model_1/BLOCK_1-3_BN_3/FusedBatchNormV32) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#277(model_1/BLOCK_1-3_BN_3/FusedBatchNormV33) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#278(model_1/quant_BLOCK_1-3_ADD/add) shape_signature:[-1, 32, -1, 60], type:FLOAT32\n",
      "  T#279(model_1/quant_BLOCK_1-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_1-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_1-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 32, -1, 60], type:INT8\n",
      "  T#280(model_1/quant_BLOCK_2-1_CONV_1/Conv2D;model_1/quant_BLOCK_2-1_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[240, 3, 3, 60], type:INT8 RO 129600 bytes, buffer: 281, data:[., ., ', ., ., ...]\n",
      "  T#281(model_1/quant_BLOCK_2-1_CONV_1/Conv2D) shape_signature:[-1, 16, -1, 240], type:INT8\n",
      "  T#282(model_1/quant_BLOCK_2-1_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#283(model_1/BLOCK_2-1_BN_1/FusedBatchNormV32) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#284(model_1/BLOCK_2-1_BN_1/FusedBatchNormV33) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#285(model_1/BLOCK_2-1_ACT_1/Sigmoid) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#286(model_1/BLOCK_2-1_ACT_1/mul_1) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#287(model_1/quant_BLOCK_2-1_CONV_3/Conv2D) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#288(model_1/quant_BLOCK_2-1_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_2-1_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_2-1_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 16, -1, 120], type:INT8\n",
      "  T#289(model_1/quant_BLOCK_2-1_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#290(model_1/BLOCK_2-1_BN_3/FusedBatchNormV32) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#291(model_1/BLOCK_2-1_BN_3/FusedBatchNormV33) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#292(model_1/quant_BLOCK_2-2_CONV_1/Conv2D) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#293(model_1/quant_BLOCK_2-2_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_2-2_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_2-2_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 16, -1, 240], type:INT8\n",
      "  T#294(model_1/quant_BLOCK_2-2_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#295(model_1/BLOCK_2-2_BN_1/FusedBatchNormV32) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#296(model_1/BLOCK_2-2_BN_1/FusedBatchNormV33) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#297(model_1/BLOCK_2-2_ACT_1/Sigmoid) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#298(model_1/BLOCK_2-2_ACT_1/mul_1) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#299(model_1/quant_BLOCK_2-2_CONV_3/Conv2D) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#300(model_1/quant_BLOCK_2-2_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_2-2_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_2-2_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 16, -1, 120], type:INT8\n",
      "  T#301(model_1/quant_BLOCK_2-2_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#302(model_1/BLOCK_2-2_BN_3/FusedBatchNormV32) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#303(model_1/BLOCK_2-2_BN_3/FusedBatchNormV33) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#304(model_1/quant_BLOCK_2-2_ADD/add) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#305(model_1/quant_BLOCK_2-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_2-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_2-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 16, -1, 120], type:INT8\n",
      "  T#306(model_1/quant_BLOCK_2-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_2-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_2-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_11) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#307(model_1/quant_BLOCK_2-3_CONV_1/Conv2D;model_1/quant_BLOCK_2-3_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[240, 3, 3, 120], type:INT8 RO 259200 bytes, buffer: 308, data:[., ., ., ., ., ...]\n",
      "  T#308(model_1/quant_BLOCK_2-3_CONV_1/Conv2D) shape_signature:[-1, 16, -1, 240], type:INT8\n",
      "  T#309(model_1/quant_BLOCK_2-3_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#310(model_1/BLOCK_2-3_BN_1/FusedBatchNormV32) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#311(model_1/BLOCK_2-3_BN_1/FusedBatchNormV33) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#312(model_1/BLOCK_2-3_ACT_1/Sigmoid) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#313(model_1/BLOCK_2-3_ACT_1/mul_1) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#314(model_1/quant_BLOCK_2-3_CONV_3/Conv2D) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#315(model_1/quant_BLOCK_2-3_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_2-3_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_2-3_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 16, -1, 120], type:INT8\n",
      "  T#316(model_1/quant_BLOCK_2-3_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#317(model_1/BLOCK_2-3_BN_3/FusedBatchNormV32) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#318(model_1/BLOCK_2-3_BN_3/FusedBatchNormV33) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#319(model_1/quant_BLOCK_2-3_ADD/add) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#320(model_1/quant_BLOCK_2-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_2-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_2-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 16, -1, 120], type:INT8\n",
      "  T#321(model_1/quant_BLOCK_2-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_2-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_2-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_11) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#322(model_1/quant_BLOCK_2-4_CONV_1/Conv2D;model_1/quant_BLOCK_2-4_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[240, 3, 3, 120], type:INT8 RO 259200 bytes, buffer: 323, data:[., ., !, ., ., ...]\n",
      "  T#323(model_1/quant_BLOCK_2-4_CONV_1/Conv2D4) shape_signature:[-1, 16, -1, 240], type:INT8\n",
      "  T#324(model_1/quant_BLOCK_2-4_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#325(model_1/BLOCK_2-4_BN_1/FusedBatchNormV32) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#326(model_1/BLOCK_2-4_BN_1/FusedBatchNormV33) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#327(model_1/BLOCK_2-4_ACT_1/Sigmoid) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#328(model_1/BLOCK_2-4_ACT_1/mul_1) shape_signature:[-1, 16, -1, 240], type:FLOAT32\n",
      "  T#329(model_1/quant_BLOCK_2-4_CONV_3/Conv2D1) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#330(model_1/quant_BLOCK_2-4_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_2-4_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_2-4_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 16, -1, 120], type:INT8\n",
      "  T#331(model_1/quant_BLOCK_2-4_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#332(model_1/BLOCK_2-4_BN_3/FusedBatchNormV32) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#333(model_1/BLOCK_2-4_BN_3/FusedBatchNormV33) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#334(model_1/quant_BLOCK_2-4_ADD/add) shape_signature:[-1, 16, -1, 120], type:FLOAT32\n",
      "  T#335(model_1/quant_BLOCK_2-4_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_2-4_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_2-4_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 16, -1, 120], type:INT8\n",
      "  T#336(model_1/quant_BLOCK_3-1_CONV_1/Conv2D;model_1/quant_BLOCK_3-1_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[640, 1, 1, 120], type:INT8 RO 76800 bytes, buffer: 337, data:[;, ., O, @, v, ...]\n",
      "  T#337(model_1/quant_BLOCK_3-1_CONV_1/Conv2D) shape_signature:[-1, 16, -1, 640], type:INT8\n",
      "  T#338(model_1/quant_BLOCK_3-1_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 16, -1, 640], type:FLOAT32\n",
      "  T#339(model_1/BLOCK_3-1_BN_1/FusedBatchNormV32) shape_signature:[-1, 16, -1, 640], type:FLOAT32\n",
      "  T#340(model_1/BLOCK_3-1_BN_1/FusedBatchNormV33) shape_signature:[-1, 16, -1, 640], type:FLOAT32\n",
      "  T#341(model_1/BLOCK_3-1_ACT_1/Sigmoid) shape_signature:[-1, 16, -1, 640], type:FLOAT32\n",
      "  T#342(model_1/BLOCK_3-1_ACT_1/mul_1) shape_signature:[-1, 16, -1, 640], type:FLOAT32\n",
      "  T#343(model_1/BLOCK_3-1_BN_2_NOQUANT/FusedBatchNormV3;model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D;model_1/BLOCK_3-1_CONV_2/depthwise) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#344(model_1/BLOCK_3-1_ACT_2/Sigmoid) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#345(model_1/BLOCK_3-1_ACT_2/mul_1) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#346(model_1/BLOCK_3-1_SE_AVG_POOL_1/Mean) shape_signature:[-1, 640], type:FLOAT32\n",
      "  T#347(model_1/quant_BLOCK_3-1_SE_RESHAPE/Shape) shape:[2], type:INT32\n",
      "  T#348(model_1/quant_BLOCK_3-1_SE_RESHAPE/strided_slice) shape:[], type:INT32\n",
      "  T#349(model_1/quant_BLOCK_3-1_SE_RESHAPE/Reshape/shape) shape:[4], type:INT32\n",
      "  T#350(model_1/quant_BLOCK_3-1_SE_RESHAPE/Reshape) shape_signature:[-1, 1, 1, 640], type:FLOAT32\n",
      "  T#351(model_1/quant_BLOCK_3-1_SE_CONV_1/Conv2D) shape_signature:[-1, 1, 1, 40], type:FLOAT32\n",
      "  T#352(model_1/quant_BLOCK_3-1_SE_CONV_1/Sigmoid) shape_signature:[-1, 1, 1, 40], type:FLOAT32\n",
      "  T#353(model_1/quant_BLOCK_3-1_SE_CONV_1/mul_1) shape_signature:[-1, 1, 1, 40], type:FLOAT32\n",
      "  T#354(model_1/quant_BLOCK_3-1_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_3-1_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_3-1_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 1, 1, 40], type:INT8\n",
      "  T#355(model_1/quant_BLOCK_3-1_SE_CONV_2/Conv2D;model_1/quant_BLOCK_3-1_SE_CONV_2/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[640, 1, 1, 40], type:INT8 RO 25600 bytes, buffer: 356, data:[G, s, =, ., ., ...]\n",
      "  T#356(model_1/quant_BLOCK_3-1_SE_CONV_2/Conv2D) shape_signature:[-1, 1, 1, 640], type:INT8\n",
      "  T#357(model_1/quant_BLOCK_3-1_SE_CONV_2/Sigmoid) shape_signature:[-1, 1, 1, 640], type:INT8\n",
      "  T#358(model_1/quant_BLOCK_3-1_SE_CONV_2/Sigmoid1) shape_signature:[-1, 1, 1, 640], type:FLOAT32\n",
      "  T#359(model_1/BLOCK_3-1_MULTIPLY/mul) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#360(model_1/quant_BLOCK_3-1_CONV_3/Conv2D) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#361(model_1/quant_BLOCK_3-1_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_3-1_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_3-1_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 8, -1, 160], type:INT8\n",
      "  T#362(model_1/quant_BLOCK_3-1_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#363(model_1/BLOCK_3-1_BN_3/FusedBatchNormV32) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#364(model_1/BLOCK_3-1_BN_3/FusedBatchNormV33) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#365(model_1/quant_BLOCK_3-2_CONV_1/Conv2D) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#366(model_1/quant_BLOCK_3-2_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_3-2_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_3-2_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 8, -1, 640], type:INT8\n",
      "  T#367(model_1/quant_BLOCK_3-2_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#368(model_1/BLOCK_3-2_BN_1/FusedBatchNormV32) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#369(model_1/BLOCK_3-2_BN_1/FusedBatchNormV33) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#370(model_1/BLOCK_3-2_ACT_1/Sigmoid) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#371(model_1/BLOCK_3-2_ACT_1/mul_1) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#372(model_1/BLOCK_3-2_BN_2_NOQUANT/FusedBatchNormV3;model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D;model_1/BLOCK_3-2_CONV_2/depthwise) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#373(model_1/BLOCK_3-2_ACT_2/Sigmoid) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#374(model_1/BLOCK_3-2_ACT_2/mul_1) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#375(model_1/BLOCK_3-2_SE_AVG_POOL_1/Mean) shape_signature:[-1, 640], type:FLOAT32\n",
      "  T#376(model_1/quant_BLOCK_3-2_SE_RESHAPE/Shape) shape:[2], type:INT32\n",
      "  T#377(model_1/quant_BLOCK_3-2_SE_RESHAPE/strided_slice) shape:[], type:INT32\n",
      "  T#378(model_1/quant_BLOCK_3-2_SE_RESHAPE/Reshape/shape) shape:[4], type:INT32\n",
      "  T#379(model_1/quant_BLOCK_3-2_SE_RESHAPE/Reshape) shape_signature:[-1, 1, 1, 640], type:FLOAT32\n",
      "  T#380(model_1/quant_BLOCK_3-2_SE_CONV_1/Conv2D) shape_signature:[-1, 1, 1, 40], type:FLOAT32\n",
      "  T#381(model_1/quant_BLOCK_3-2_SE_CONV_1/Sigmoid) shape_signature:[-1, 1, 1, 40], type:FLOAT32\n",
      "  T#382(model_1/quant_BLOCK_3-2_SE_CONV_1/mul_1) shape_signature:[-1, 1, 1, 40], type:FLOAT32\n",
      "  T#383(model_1/quant_BLOCK_3-2_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_3-2_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_3-2_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 1, 1, 40], type:INT8\n",
      "  T#384(model_1/quant_BLOCK_3-2_SE_CONV_2/Conv2D;model_1/quant_BLOCK_3-2_SE_CONV_2/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[640, 1, 1, 40], type:INT8 RO 25600 bytes, buffer: 385, data:[., ., ., ., ., ...]\n",
      "  T#385(model_1/quant_BLOCK_3-2_SE_CONV_2/Conv2D) shape_signature:[-1, 1, 1, 640], type:INT8\n",
      "  T#386(model_1/quant_BLOCK_3-2_SE_CONV_2/Sigmoid) shape_signature:[-1, 1, 1, 640], type:INT8\n",
      "  T#387(model_1/quant_BLOCK_3-2_SE_CONV_2/Sigmoid1) shape_signature:[-1, 1, 1, 640], type:FLOAT32\n",
      "  T#388(model_1/BLOCK_3-2_MULTIPLY/mul) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#389(model_1/quant_BLOCK_3-2_CONV_3/Conv2D) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#390(model_1/quant_BLOCK_3-2_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_3-2_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_3-2_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 8, -1, 160], type:INT8\n",
      "  T#391(model_1/quant_BLOCK_3-2_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#392(model_1/BLOCK_3-2_BN_3/FusedBatchNormV32) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#393(model_1/BLOCK_3-2_BN_3/FusedBatchNormV33) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#394(model_1/quant_BLOCK_3-2_ADD/add) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#395(model_1/quant_BLOCK_3-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_3-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_3-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 8, -1, 160], type:INT8\n",
      "  T#396(model_1/quant_BLOCK_3-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_3-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_3-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_11) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#397(model_1/quant_BLOCK_3-3_CONV_1/Conv2D;model_1/quant_BLOCK_3-3_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[640, 1, 1, 160], type:INT8 RO 102400 bytes, buffer: 398, data:[., ., ., ., ., ...]\n",
      "  T#398(model_1/quant_BLOCK_3-3_CONV_1/Conv2D) shape_signature:[-1, 8, -1, 640], type:INT8\n",
      "  T#399(model_1/quant_BLOCK_3-3_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#400(model_1/BLOCK_3-3_BN_1/FusedBatchNormV32) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#401(model_1/BLOCK_3-3_BN_1/FusedBatchNormV33) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#402(model_1/BLOCK_3-3_ACT_1/Sigmoid) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#403(model_1/BLOCK_3-3_ACT_1/mul_1) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#404(model_1/BLOCK_3-3_BN_2_NOQUANT/FusedBatchNormV3;model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D;model_1/BLOCK_3-3_CONV_2/depthwise) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#405(model_1/BLOCK_3-3_ACT_2/Sigmoid) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#406(model_1/BLOCK_3-3_ACT_2/mul_1) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#407(model_1/BLOCK_3-3_SE_AVG_POOL_1/Mean) shape_signature:[-1, 640], type:FLOAT32\n",
      "  T#408(model_1/quant_BLOCK_3-3_SE_RESHAPE/Shape) shape:[2], type:INT32\n",
      "  T#409(model_1/quant_BLOCK_3-3_SE_RESHAPE/strided_slice) shape:[], type:INT32\n",
      "  T#410(model_1/quant_BLOCK_3-3_SE_RESHAPE/Reshape/shape) shape:[4], type:INT32\n",
      "  T#411(model_1/quant_BLOCK_3-3_SE_RESHAPE/Reshape) shape_signature:[-1, 1, 1, 640], type:FLOAT32\n",
      "  T#412(model_1/quant_BLOCK_3-3_SE_CONV_1/Conv2D) shape_signature:[-1, 1, 1, 40], type:FLOAT32\n",
      "  T#413(model_1/quant_BLOCK_3-3_SE_CONV_1/Sigmoid) shape_signature:[-1, 1, 1, 40], type:FLOAT32\n",
      "  T#414(model_1/quant_BLOCK_3-3_SE_CONV_1/mul_1) shape_signature:[-1, 1, 1, 40], type:FLOAT32\n",
      "  T#415(model_1/quant_BLOCK_3-3_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_3-3_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_3-3_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 1, 1, 40], type:INT8\n",
      "  T#416(model_1/quant_BLOCK_3-3_SE_CONV_2/Conv2D;model_1/quant_BLOCK_3-3_SE_CONV_2/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[640, 1, 1, 40], type:INT8 RO 25600 bytes, buffer: 417, data:[., ., ., ., ., ...]\n",
      "  T#417(model_1/quant_BLOCK_3-3_SE_CONV_2/Conv2D) shape_signature:[-1, 1, 1, 640], type:INT8\n",
      "  T#418(model_1/quant_BLOCK_3-3_SE_CONV_2/Sigmoid) shape_signature:[-1, 1, 1, 640], type:INT8\n",
      "  T#419(model_1/quant_BLOCK_3-3_SE_CONV_2/Sigmoid1) shape_signature:[-1, 1, 1, 640], type:FLOAT32\n",
      "  T#420(model_1/BLOCK_3-3_MULTIPLY/mul) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#421(model_1/quant_BLOCK_3-3_CONV_3/Conv2D) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#422(model_1/quant_BLOCK_3-3_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_3-3_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_3-3_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 8, -1, 160], type:INT8\n",
      "  T#423(model_1/quant_BLOCK_3-3_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#424(model_1/BLOCK_3-3_BN_3/FusedBatchNormV32) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#425(model_1/BLOCK_3-3_BN_3/FusedBatchNormV33) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#426(model_1/quant_BLOCK_3-3_ADD/add) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#427(model_1/quant_BLOCK_3-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_3-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_3-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 8, -1, 160], type:INT8\n",
      "  T#428(model_1/quant_BLOCK_3-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_3-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_3-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_11) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#429(model_1/quant_BLOCK_3-4_CONV_1/Conv2D;model_1/quant_BLOCK_3-4_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[640, 1, 1, 160], type:INT8 RO 102400 bytes, buffer: 430, data:[., ., ., ., ., ...]\n",
      "  T#430(model_1/quant_BLOCK_3-4_CONV_1/Conv2D) shape_signature:[-1, 8, -1, 640], type:INT8\n",
      "  T#431(model_1/quant_BLOCK_3-4_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#432(model_1/BLOCK_3-4_BN_1/FusedBatchNormV32) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#433(model_1/BLOCK_3-4_BN_1/FusedBatchNormV33) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#434(model_1/BLOCK_3-4_ACT_1/Sigmoid) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#435(model_1/BLOCK_3-4_ACT_1/mul_1) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#436(model_1/BLOCK_3-4_BN_2_NOQUANT/FusedBatchNormV3;model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D;model_1/BLOCK_3-4_CONV_2/depthwise) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#437(model_1/BLOCK_3-4_ACT_2/Sigmoid) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#438(model_1/BLOCK_3-4_ACT_2/mul_1) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#439(model_1/BLOCK_3-4_SE_AVG_POOL_1/Mean) shape_signature:[-1, 640], type:FLOAT32\n",
      "  T#440(model_1/quant_BLOCK_3-4_SE_RESHAPE/Shape) shape:[2], type:INT32\n",
      "  T#441(model_1/quant_BLOCK_3-4_SE_RESHAPE/strided_slice) shape:[], type:INT32\n",
      "  T#442(model_1/quant_BLOCK_3-4_SE_RESHAPE/Reshape/shape) shape:[4], type:INT32\n",
      "  T#443(model_1/quant_BLOCK_3-4_SE_RESHAPE/Reshape) shape_signature:[-1, 1, 1, 640], type:FLOAT32\n",
      "  T#444(model_1/quant_BLOCK_3-4_SE_CONV_1/Conv2D) shape_signature:[-1, 1, 1, 40], type:FLOAT32\n",
      "  T#445(model_1/quant_BLOCK_3-4_SE_CONV_1/Sigmoid) shape_signature:[-1, 1, 1, 40], type:FLOAT32\n",
      "  T#446(model_1/quant_BLOCK_3-4_SE_CONV_1/mul_1) shape_signature:[-1, 1, 1, 40], type:FLOAT32\n",
      "  T#447(model_1/quant_BLOCK_3-4_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_3-4_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_3-4_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 1, 1, 40], type:INT8\n",
      "  T#448(model_1/quant_BLOCK_3-4_SE_CONV_2/Conv2D;model_1/quant_BLOCK_3-4_SE_CONV_2/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[640, 1, 1, 40], type:INT8 RO 25600 bytes, buffer: 449, data:[., ;, ., ., &, ...]\n",
      "  T#449(model_1/quant_BLOCK_3-4_SE_CONV_2/Conv2D) shape_signature:[-1, 1, 1, 640], type:INT8\n",
      "  T#450(model_1/quant_BLOCK_3-4_SE_CONV_2/Sigmoid) shape_signature:[-1, 1, 1, 640], type:INT8\n",
      "  T#451(model_1/quant_BLOCK_3-4_SE_CONV_2/Sigmoid1) shape_signature:[-1, 1, 1, 640], type:FLOAT32\n",
      "  T#452(model_1/BLOCK_3-4_MULTIPLY/mul) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#453(model_1/quant_BLOCK_3-4_CONV_3/Conv2D) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#454(model_1/quant_BLOCK_3-4_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_3-4_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_3-4_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 8, -1, 160], type:INT8\n",
      "  T#455(model_1/quant_BLOCK_3-4_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#456(model_1/BLOCK_3-4_BN_3/FusedBatchNormV32) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#457(model_1/BLOCK_3-4_BN_3/FusedBatchNormV33) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#458(model_1/quant_BLOCK_3-4_ADD/add) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#459(model_1/quant_BLOCK_3-4_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_3-4_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_3-4_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 8, -1, 160], type:INT8\n",
      "  T#460(model_1/quant_BLOCK_3-4_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_3-4_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_3-4_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_11) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#461(model_1/quant_BLOCK_3-5_CONV_1/Conv2D;model_1/quant_BLOCK_3-5_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[640, 1, 1, 160], type:INT8 RO 102400 bytes, buffer: 462, data:[., ., ., ., ., ...]\n",
      "  T#462(model_1/quant_BLOCK_3-5_CONV_1/Conv2D) shape_signature:[-1, 8, -1, 640], type:INT8\n",
      "  T#463(model_1/quant_BLOCK_3-5_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#464(model_1/BLOCK_3-5_BN_1/FusedBatchNormV32) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#465(model_1/BLOCK_3-5_BN_1/FusedBatchNormV33) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#466(model_1/BLOCK_3-5_ACT_1/Sigmoid) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#467(model_1/BLOCK_3-5_ACT_1/mul_1) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#468(model_1/BLOCK_3-5_BN_2_NOQUANT/FusedBatchNormV3;model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D;model_1/BLOCK_3-5_CONV_2/depthwise) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#469(model_1/BLOCK_3-5_ACT_2/Sigmoid) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#470(model_1/BLOCK_3-5_ACT_2/mul_1) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#471(model_1/BLOCK_3-5_SE_AVG_POOL_1/Mean) shape_signature:[-1, 640], type:FLOAT32\n",
      "  T#472(model_1/quant_BLOCK_3-5_SE_RESHAPE/Shape) shape:[2], type:INT32\n",
      "  T#473(model_1/quant_BLOCK_3-5_SE_RESHAPE/strided_slice) shape:[], type:INT32\n",
      "  T#474(model_1/quant_BLOCK_3-5_SE_RESHAPE/Reshape/shape) shape:[4], type:INT32\n",
      "  T#475(model_1/quant_BLOCK_3-5_SE_RESHAPE/Reshape) shape_signature:[-1, 1, 1, 640], type:FLOAT32\n",
      "  T#476(model_1/quant_BLOCK_3-5_SE_CONV_1/Conv2D1) shape_signature:[-1, 1, 1, 40], type:FLOAT32\n",
      "  T#477(model_1/quant_BLOCK_3-5_SE_CONV_1/Sigmoid) shape_signature:[-1, 1, 1, 40], type:FLOAT32\n",
      "  T#478(model_1/quant_BLOCK_3-5_SE_CONV_1/mul_1) shape_signature:[-1, 1, 1, 40], type:FLOAT32\n",
      "  T#479(model_1/quant_BLOCK_3-5_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_3-5_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_3-5_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 1, 1, 40], type:INT8\n",
      "  T#480(model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D;model_1/quant_BLOCK_3-5_SE_CONV_2/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[640, 1, 1, 40], type:INT8 RO 25600 bytes, buffer: 481, data:[., !, ., ., ., ...]\n",
      "  T#481(model_1/quant_BLOCK_3-5_SE_CONV_2/Conv2D10) shape_signature:[-1, 1, 1, 640], type:INT8\n",
      "  T#482(model_1/quant_BLOCK_3-5_SE_CONV_2/Sigmoid) shape_signature:[-1, 1, 1, 640], type:INT8\n",
      "  T#483(model_1/quant_BLOCK_3-5_SE_CONV_2/Sigmoid1) shape_signature:[-1, 1, 1, 640], type:FLOAT32\n",
      "  T#484(model_1/BLOCK_3-5_MULTIPLY/mul) shape_signature:[-1, 8, -1, 640], type:FLOAT32\n",
      "  T#485(model_1/quant_BLOCK_3-5_CONV_3/Conv2D1) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#486(model_1/quant_BLOCK_3-5_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_3-5_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_3-5_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 8, -1, 160], type:INT8\n",
      "  T#487(model_1/quant_BLOCK_3-5_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#488(model_1/BLOCK_3-5_BN_3/FusedBatchNormV32) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#489(model_1/BLOCK_3-5_BN_3/FusedBatchNormV33) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#490(model_1/quant_BLOCK_3-5_ADD/add) shape_signature:[-1, 8, -1, 160], type:FLOAT32\n",
      "  T#491(model_1/quant_BLOCK_3-5_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_3-5_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_3-5_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 8, -1, 160], type:INT8\n",
      "  T#492(model_1/quant_BLOCK_4-1_CONV_1/Conv2D;model_1/quant_BLOCK_4-1_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[1120, 1, 1, 160], type:INT8 RO 179200 bytes, buffer: 493, data:[., #, b, ., ., ...]\n",
      "  T#493(model_1/quant_BLOCK_4-1_CONV_1/Conv2D) shape_signature:[-1, 8, -1, 1120], type:INT8\n",
      "  T#494(model_1/quant_BLOCK_4-1_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 8, -1, 1120], type:FLOAT32\n",
      "  T#495(model_1/BLOCK_4-1_BN_1/FusedBatchNormV32) shape_signature:[-1, 8, -1, 1120], type:FLOAT32\n",
      "  T#496(model_1/BLOCK_4-1_BN_1/FusedBatchNormV33) shape_signature:[-1, 8, -1, 1120], type:FLOAT32\n",
      "  T#497(model_1/BLOCK_4-1_ACT_1/Sigmoid) shape_signature:[-1, 8, -1, 1120], type:FLOAT32\n",
      "  T#498(model_1/BLOCK_4-1_ACT_1/mul_1) shape_signature:[-1, 8, -1, 1120], type:FLOAT32\n",
      "  T#499(model_1/BLOCK_4-1_BN_2_NOQUANT/FusedBatchNormV3;model_1/quant_BLOCK_4-4_SE_CONV_2/Conv2D;model_1/BLOCK_4-1_CONV_2/depthwise) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#500(model_1/BLOCK_4-1_ACT_2/Sigmoid) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#501(model_1/BLOCK_4-1_ACT_2/mul_1) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#502(model_1/BLOCK_4-1_SE_AVG_POOL_1/Mean) shape_signature:[-1, 1120], type:FLOAT32\n",
      "  T#503(model_1/quant_BLOCK_4-1_SE_RESHAPE/Shape) shape:[2], type:INT32\n",
      "  T#504(model_1/quant_BLOCK_4-1_SE_RESHAPE/strided_slice) shape:[], type:INT32\n",
      "  T#505(model_1/quant_BLOCK_4-1_SE_RESHAPE/Reshape/shape) shape:[4], type:INT32\n",
      "  T#506(model_1/quant_BLOCK_4-1_SE_RESHAPE/Reshape) shape_signature:[-1, 1, 1, 1120], type:FLOAT32\n",
      "  T#507(model_1/quant_BLOCK_4-1_SE_CONV_1/Conv2D) shape_signature:[-1, 1, 1, 70], type:FLOAT32\n",
      "  T#508(model_1/quant_BLOCK_4-1_SE_CONV_1/Sigmoid) shape_signature:[-1, 1, 1, 70], type:FLOAT32\n",
      "  T#509(model_1/quant_BLOCK_4-1_SE_CONV_1/mul_1) shape_signature:[-1, 1, 1, 70], type:FLOAT32\n",
      "  T#510(model_1/quant_BLOCK_4-1_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_4-1_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_4-1_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 1, 1, 70], type:INT8\n",
      "  T#511(model_1/quant_BLOCK_4-1_SE_CONV_2/Conv2D;model_1/quant_BLOCK_4-1_SE_CONV_2/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[1120, 1, 1, 70], type:INT8 RO 78400 bytes, buffer: 512, data:[., ., ., ., ., ...]\n",
      "  T#512(model_1/quant_BLOCK_4-1_SE_CONV_2/Conv2D) shape_signature:[-1, 1, 1, 1120], type:INT8\n",
      "  T#513(model_1/quant_BLOCK_4-1_SE_CONV_2/Sigmoid) shape_signature:[-1, 1, 1, 1120], type:INT8\n",
      "  T#514(model_1/quant_BLOCK_4-1_SE_CONV_2/Sigmoid1) shape_signature:[-1, 1, 1, 1120], type:FLOAT32\n",
      "  T#515(model_1/BLOCK_4-1_MULTIPLY/mul) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#516(model_1/quant_BLOCK_4-1_CONV_3/Conv2D) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#517(model_1/quant_BLOCK_4-1_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_4-1_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_4-1_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 4, -1, 280], type:INT8\n",
      "  T#518(model_1/quant_BLOCK_4-1_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#519(model_1/BLOCK_4-1_BN_3/FusedBatchNormV32) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#520(model_1/BLOCK_4-1_BN_3/FusedBatchNormV33) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#521(model_1/quant_BLOCK_4-2_CONV_1/Conv2D) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#522(model_1/quant_BLOCK_4-2_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_4-2_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_4-2_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 4, -1, 1120], type:INT8\n",
      "  T#523(model_1/quant_BLOCK_4-2_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#524(model_1/BLOCK_4-2_BN_1/FusedBatchNormV32) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#525(model_1/BLOCK_4-2_BN_1/FusedBatchNormV33) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#526(model_1/BLOCK_4-2_ACT_1/Sigmoid) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#527(model_1/BLOCK_4-2_ACT_1/mul_1) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#528(model_1/BLOCK_4-2_BN_2_NOQUANT/FusedBatchNormV3;model_1/quant_BLOCK_4-4_SE_CONV_2/Conv2D;model_1/BLOCK_4-2_CONV_2/depthwise) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#529(model_1/BLOCK_4-2_ACT_2/Sigmoid) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#530(model_1/BLOCK_4-2_ACT_2/mul_1) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#531(model_1/BLOCK_4-2_SE_AVG_POOL_1/Mean) shape_signature:[-1, 1120], type:FLOAT32\n",
      "  T#532(model_1/quant_BLOCK_4-2_SE_RESHAPE/Shape) shape:[2], type:INT32\n",
      "  T#533(model_1/quant_BLOCK_4-2_SE_RESHAPE/strided_slice) shape:[], type:INT32\n",
      "  T#534(model_1/quant_BLOCK_4-2_SE_RESHAPE/Reshape/shape) shape:[4], type:INT32\n",
      "  T#535(model_1/quant_BLOCK_4-2_SE_RESHAPE/Reshape) shape_signature:[-1, 1, 1, 1120], type:FLOAT32\n",
      "  T#536(model_1/quant_BLOCK_4-2_SE_CONV_1/Conv2D) shape_signature:[-1, 1, 1, 70], type:FLOAT32\n",
      "  T#537(model_1/quant_BLOCK_4-2_SE_CONV_1/Sigmoid) shape_signature:[-1, 1, 1, 70], type:FLOAT32\n",
      "  T#538(model_1/quant_BLOCK_4-2_SE_CONV_1/mul_1) shape_signature:[-1, 1, 1, 70], type:FLOAT32\n",
      "  T#539(model_1/quant_BLOCK_4-2_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_4-2_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_4-2_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 1, 1, 70], type:INT8\n",
      "  T#540(model_1/quant_BLOCK_4-2_SE_CONV_2/Conv2D;model_1/quant_BLOCK_4-2_SE_CONV_2/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[1120, 1, 1, 70], type:INT8 RO 78400 bytes, buffer: 541, data:[., ., ., <, Q, ...]\n",
      "  T#541(model_1/quant_BLOCK_4-2_SE_CONV_2/Conv2D) shape_signature:[-1, 1, 1, 1120], type:INT8\n",
      "  T#542(model_1/quant_BLOCK_4-2_SE_CONV_2/Sigmoid) shape_signature:[-1, 1, 1, 1120], type:INT8\n",
      "  T#543(model_1/quant_BLOCK_4-2_SE_CONV_2/Sigmoid1) shape_signature:[-1, 1, 1, 1120], type:FLOAT32\n",
      "  T#544(model_1/BLOCK_4-2_MULTIPLY/mul) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#545(model_1/quant_BLOCK_4-2_CONV_3/Conv2D) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#546(model_1/quant_BLOCK_4-2_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_4-2_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_4-2_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 4, -1, 280], type:INT8\n",
      "  T#547(model_1/quant_BLOCK_4-2_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#548(model_1/BLOCK_4-2_BN_3/FusedBatchNormV32) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#549(model_1/BLOCK_4-2_BN_3/FusedBatchNormV33) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#550(model_1/quant_BLOCK_4-2_ADD/add) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#551(model_1/quant_BLOCK_4-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_4-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_4-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 4, -1, 280], type:INT8\n",
      "  T#552(model_1/quant_BLOCK_4-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_4-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_4-2_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_11) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#553(model_1/quant_BLOCK_4-3_CONV_1/Conv2D;model_1/quant_BLOCK_4-3_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[1120, 1, 1, 280], type:INT8 RO 313600 bytes, buffer: 554, data:[(, ., ., B, ., ...]\n",
      "  T#554(model_1/quant_BLOCK_4-3_CONV_1/Conv2D) shape_signature:[-1, 4, -1, 1120], type:INT8\n",
      "  T#555(model_1/quant_BLOCK_4-3_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#556(model_1/BLOCK_4-3_BN_1/FusedBatchNormV32) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#557(model_1/BLOCK_4-3_BN_1/FusedBatchNormV33) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#558(model_1/BLOCK_4-3_ACT_1/Sigmoid) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#559(model_1/BLOCK_4-3_ACT_1/mul_1) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#560(model_1/BLOCK_4-3_BN_2_NOQUANT/FusedBatchNormV3;model_1/quant_BLOCK_4-4_SE_CONV_2/Conv2D;model_1/BLOCK_4-3_CONV_2/depthwise) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#561(model_1/BLOCK_4-3_ACT_2/Sigmoid) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#562(model_1/BLOCK_4-3_ACT_2/mul_1) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#563(model_1/BLOCK_4-3_SE_AVG_POOL_1/Mean) shape_signature:[-1, 1120], type:FLOAT32\n",
      "  T#564(model_1/quant_BLOCK_4-3_SE_RESHAPE/Shape) shape:[2], type:INT32\n",
      "  T#565(model_1/quant_BLOCK_4-3_SE_RESHAPE/strided_slice) shape:[], type:INT32\n",
      "  T#566(model_1/quant_BLOCK_4-3_SE_RESHAPE/Reshape/shape) shape:[4], type:INT32\n",
      "  T#567(model_1/quant_BLOCK_4-3_SE_RESHAPE/Reshape) shape_signature:[-1, 1, 1, 1120], type:FLOAT32\n",
      "  T#568(model_1/quant_BLOCK_4-3_SE_CONV_1/Conv2D) shape_signature:[-1, 1, 1, 70], type:FLOAT32\n",
      "  T#569(model_1/quant_BLOCK_4-3_SE_CONV_1/Sigmoid) shape_signature:[-1, 1, 1, 70], type:FLOAT32\n",
      "  T#570(model_1/quant_BLOCK_4-3_SE_CONV_1/mul_1) shape_signature:[-1, 1, 1, 70], type:FLOAT32\n",
      "  T#571(model_1/quant_BLOCK_4-3_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_4-3_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_4-3_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 1, 1, 70], type:INT8\n",
      "  T#572(model_1/quant_BLOCK_4-3_SE_CONV_2/Conv2D;model_1/quant_BLOCK_4-3_SE_CONV_2/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[1120, 1, 1, 70], type:INT8 RO 78400 bytes, buffer: 573, data:[F, ., ., ., ., ...]\n",
      "  T#573(model_1/quant_BLOCK_4-3_SE_CONV_2/Conv2D) shape_signature:[-1, 1, 1, 1120], type:INT8\n",
      "  T#574(model_1/quant_BLOCK_4-3_SE_CONV_2/Sigmoid) shape_signature:[-1, 1, 1, 1120], type:INT8\n",
      "  T#575(model_1/quant_BLOCK_4-3_SE_CONV_2/Sigmoid1) shape_signature:[-1, 1, 1, 1120], type:FLOAT32\n",
      "  T#576(model_1/BLOCK_4-3_MULTIPLY/mul) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#577(model_1/quant_BLOCK_4-3_CONV_3/Conv2D) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#578(model_1/quant_BLOCK_4-3_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_4-3_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_4-3_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 4, -1, 280], type:INT8\n",
      "  T#579(model_1/quant_BLOCK_4-3_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#580(model_1/BLOCK_4-3_BN_3/FusedBatchNormV32) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#581(model_1/BLOCK_4-3_BN_3/FusedBatchNormV33) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#582(model_1/quant_BLOCK_4-3_ADD/add) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#583(model_1/quant_BLOCK_4-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_4-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_4-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 4, -1, 280], type:INT8\n",
      "  T#584(model_1/quant_BLOCK_4-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_4-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_4-3_ADD/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_11) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#585(model_1/quant_BLOCK_4-4_CONV_1/Conv2D;model_1/quant_BLOCK_4-4_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[1120, 1, 1, 280], type:INT8 RO 313600 bytes, buffer: 586, data:[X, ., ., ., ., ...]\n",
      "  T#586(model_1/quant_BLOCK_4-4_CONV_1/Conv2D) shape_signature:[-1, 4, -1, 1120], type:INT8\n",
      "  T#587(model_1/quant_BLOCK_4-4_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#588(model_1/BLOCK_4-4_BN_1/FusedBatchNormV32) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#589(model_1/BLOCK_4-4_BN_1/FusedBatchNormV33) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#590(model_1/BLOCK_4-4_ACT_1/Sigmoid) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#591(model_1/BLOCK_4-4_ACT_1/mul_1) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#592(model_1/BLOCK_4-4_BN_2_NOQUANT/FusedBatchNormV3;model_1/quant_BLOCK_4-4_SE_CONV_2/Conv2D;model_1/BLOCK_4-4_CONV_2/depthwise) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#593(model_1/BLOCK_4-4_ACT_2/Sigmoid) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#594(model_1/BLOCK_4-4_ACT_2/mul_1) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#595(model_1/BLOCK_4-4_SE_AVG_POOL_1/Mean) shape_signature:[-1, 1120], type:FLOAT32\n",
      "  T#596(model_1/quant_BLOCK_4-4_SE_RESHAPE/Shape) shape:[2], type:INT32\n",
      "  T#597(model_1/quant_BLOCK_4-4_SE_RESHAPE/strided_slice) shape:[], type:INT32\n",
      "  T#598(model_1/quant_BLOCK_4-4_SE_RESHAPE/Reshape/shape) shape:[4], type:INT32\n",
      "  T#599(model_1/quant_BLOCK_4-4_SE_RESHAPE/Reshape) shape_signature:[-1, 1, 1, 1120], type:FLOAT32\n",
      "  T#600(model_1/quant_BLOCK_4-4_SE_CONV_1/Conv2D1) shape_signature:[-1, 1, 1, 70], type:FLOAT32\n",
      "  T#601(model_1/quant_BLOCK_4-4_SE_CONV_1/Sigmoid) shape_signature:[-1, 1, 1, 70], type:FLOAT32\n",
      "  T#602(model_1/quant_BLOCK_4-4_SE_CONV_1/mul_1) shape_signature:[-1, 1, 1, 70], type:FLOAT32\n",
      "  T#603(model_1/quant_BLOCK_4-4_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_4-4_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_4-4_SE_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 1, 1, 70], type:INT8\n",
      "  T#604(model_1/quant_BLOCK_4-4_SE_CONV_2/Conv2D;model_1/quant_BLOCK_4-4_SE_CONV_2/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[1120, 1, 1, 70], type:INT8 RO 78400 bytes, buffer: 605, data:[., ., ., ., ., ...]\n",
      "  T#605(model_1/quant_BLOCK_4-4_SE_CONV_2/Conv2D8) shape_signature:[-1, 1, 1, 1120], type:INT8\n",
      "  T#606(model_1/quant_BLOCK_4-4_SE_CONV_2/Sigmoid) shape_signature:[-1, 1, 1, 1120], type:INT8\n",
      "  T#607(model_1/quant_BLOCK_4-4_SE_CONV_2/Sigmoid1) shape_signature:[-1, 1, 1, 1120], type:FLOAT32\n",
      "  T#608(model_1/BLOCK_4-4_MULTIPLY/mul) shape_signature:[-1, 4, -1, 1120], type:FLOAT32\n",
      "  T#609(model_1/quant_BLOCK_4-4_CONV_3/Conv2D1) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#610(model_1/quant_BLOCK_4-4_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_BLOCK_4-4_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_BLOCK_4-4_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 4, -1, 280], type:INT8\n",
      "  T#611(model_1/quant_BLOCK_4-4_CONV_3/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#612(model_1/BLOCK_4-4_BN_3/FusedBatchNormV32) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#613(model_1/BLOCK_4-4_BN_3/FusedBatchNormV33) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#614(model_1/BLOCK_4-4_ADD/add) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#615(model_1/BNORM_POST_NOQUANT/FusedBatchNormV32) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#616(model_1/quant_ACT_POST/Relu;model_1/BNORM_POST_NOQUANT/FusedBatchNormV3) shape_signature:[-1, 4, -1, 280], type:FLOAT32\n",
      "  T#617(model_1/quant_ACT_POST/MovingAvgQuantize/FakeQuantWithMinMaxVars;model_1/quant_ACT_POST/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;model_1/quant_ACT_POST/MovingAvgQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1) shape_signature:[-1, 4, -1, 280], type:INT8\n",
      "  T#618(model_1/quant_POST_CONV_1/Conv2D;model_1/quant_POST_CONV_1/LastValueQuant/FakeQuantWithMinMaxVarsPerChannel) shape:[420, 3, 3, 280], type:INT8 RO 1058400 bytes, buffer: 619, data:[., ., 0, 8, 7, ...]\n",
      "  T#619(model_1/quant_POST_CONV_1/Conv2D1) shape_signature:[-1, 2, -1, 420], type:INT8\n",
      "  T#620(model_1/quant_POST_CONV_1/MovingAvgQuantize/FakeQuantWithMinMaxVars) shape_signature:[-1, 2, -1, 420], type:FLOAT32\n",
      "  T#621(model_1/POST_BN_1/FusedBatchNormV32) shape_signature:[-1, 2, -1, 420], type:FLOAT32\n",
      "  T#622(model_1/POST_ACT_1/Relu;model_1/POST_BN_1/FusedBatchNormV3) shape_signature:[-1, 2, -1, 420], type:FLOAT32\n",
      "  T#623(model_1/GLOBAL_LME_POOL/Max) shape_signature:[-1, 1, 1, 420], type:FLOAT32\n",
      "  T#624(model_1/GLOBAL_LME_POOL/sub) shape_signature:[-1, 2, -1, 420], type:FLOAT32\n",
      "  T#625(model_1/GLOBAL_LME_POOL/mul) shape_signature:[-1, 2, -1, 420], type:FLOAT32\n",
      "  T#626(model_1/GLOBAL_LME_POOL/Exp) shape_signature:[-1, 2, -1, 420], type:FLOAT32\n",
      "  T#627(model_1/GLOBAL_LME_POOL/Mean) shape_signature:[-1, 420], type:FLOAT32\n",
      "  T#628(model_1/GLOBAL_LME_POOL/Log) shape_signature:[-1, 420], type:FLOAT32\n",
      "  T#629(model_1/GLOBAL_LME_POOL/truediv;model_1/GLOBAL_LME_POOL/ReadVariableOp1) shape_signature:[-1, 420], type:FLOAT32\n",
      "  T#630(model_1/GLOBAL_LME_POOL/add) shape_signature:[-1, 1, -1, 420], type:FLOAT32\n",
      "  T#631(model_1/GLOBAL_LME_POOL/Reshape) shape_signature:[-1, 420], type:FLOAT32\n",
      "  T#632(model_1/dense/MatMul;model_1/dense/BiasAdd) shape_signature:[-1, 2], type:FLOAT32\n",
      "  T#633(StatefulPartitionedCall:0) shape_signature:[-1, 2], type:FLOAT32\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Your TFLite model has '1' signature_def(s).\n",
      "\n",
      "Signature#0 key: 'serving_default'\n",
      "- Subgraph: Subgraph#0\n",
      "- Inputs: \n",
      "    'INPUT' : T#0\n",
      "- Outputs: \n",
      "    'activation' : T#633\n",
      "\n",
      "---------------------------------------------------------------\n",
      "              Model size:   16461840 bytes\n",
      "    Non-data buffer size:     474380 bytes (02.88 %)\n",
      "  Total data buffer size:   15987460 bytes (97.12 %)\n",
      "    (Zero value buffers):      12744 bytes (00.08 %)\n",
      "\n",
      "* Buffers of TFLite model are mostly used for constant tensors.\n",
      "  And zero value buffers are buffers filled with zeros.\n",
      "  Non-data buffers area are used to store operators, subgraphs and etc.\n",
      "  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.lite.experimental.Analyzer.analyze(model_path=quant_aware_INT8_tflite_path,\n",
    "                                      model_content=None,\n",
    "                                      gpu_compatibility=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17946353-1f90-4859-9fc7-22d844ffaa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Processed 0 files. Currently processing file: /home/jovyan/cut-data/testing/non_target/0.920_0001_703520.wav\n",
      "Processed 100 files. Currently processing file: /home/jovyan/cut-data/testing/non_target/0.920_0057_645986.wav\n",
      "Processed 200 files. Currently processing file: /home/jovyan/cut-data/testing/non_target/0.930_0263_741788.wav\n",
      "Processed 300 files. Currently processing file: /home/jovyan/cut-data/testing/non_target/0.950_0002_645965.wav\n",
      "Processed 400 files. Currently processing file: /home/jovyan/cut-data/testing/non_target/0.960_0024_103739801.wav\n",
      "Processed 500 files. Currently processing file: /home/jovyan/cut-data/testing/non_target/1.000_0001_-X5Ay0Wuew0_20.wav\n",
      "Processed 600 files. Currently processing file: /home/jovyan/cut-data/testing/non_target/1.000_0001_4TQzd0lB8IQ_30.wav\n",
      "Processed 700 files. Currently processing file: /home/jovyan/cut-data/testing/non_target/1.000_0002_1MF9_29YUZU_10.wav\n",
      "Processed 800 files. Currently processing file: /home/jovyan/cut-data/testing/non_target/1.000_0003_-8S_tLKfeJg_200.wav\n",
      "Processed 900 files. Currently processing file: /home/jovyan/cut-data/testing/non_target/1.000_0003_2hBkeX3k48M_180.wav\n",
      "Processed 0 files. Currently processing file: /home/jovyan/cut-data/testing/target/0.540_0001_200624_1647_11.wav\n",
      "Processed 100 files. Currently processing file: /home/jovyan/cut-data/testing/target/0.920_0282_R21_2022_02_25_08_07_04.wav\n",
      "Processed 200 files. Currently processing file: /home/jovyan/cut-data/testing/target/0.940_0009_784426.wav\n",
      "Processed 300 files. Currently processing file: /home/jovyan/cut-data/testing/target/0.950_0034_470177.wav\n",
      "Processed 400 files. Currently processing file: /home/jovyan/cut-data/testing/target/0.970_0015_419564251.wav\n",
      "Balance the test data...\n",
      "Balanced test data:\n",
      "balanced_x_test shape: (916,)\n",
      "balanced_y_test shape: (916, 2)\n",
      "balanced_file_paths_test shape: (916,)\n",
      "...Done. Loaded 916 test samples and 916 labels.\n",
      "Processing batch 1 of 77\n",
      "Input File Path: /home/jovyan/cut-data/testing/non_target/0.920_0057_141420.wav\n",
      "Entry: 100\n",
      "True Label: 0\n",
      "Predicted Label: 0\n",
      "Output Tensor: [[1.0000000e+00 1.2753271e-14]]\n",
      "Processing batch 11 of 77\n",
      "Input File Path: /home/jovyan/cut-data/testing/non_target/0.930_0186_741788.wav\n",
      "Entry: 200\n",
      "True Label: 0\n",
      "Predicted Label: 0\n",
      "Output Tensor: [[1.0000000e+00 4.4634277e-10]]\n",
      "Processing batch 21 of 77\n",
      "Input File Path: /home/jovyan/cut-data/testing/non_target/0.950_0002_595388.wav\n",
      "Entry: 300\n",
      "True Label: 0\n",
      "Predicted Label: 0\n",
      "Output Tensor: [[1.0000000e+00 1.3729901e-13]]\n",
      "Processing batch 31 of 77\n",
      "Input File Path: /home/jovyan/cut-data/testing/non_target/0.960_0018_782604.wav\n",
      "Entry: 400\n",
      "True Label: 0\n",
      "Predicted Label: 0\n",
      "Output Tensor: [[1.0000000e+00 3.6838117e-14]]\n",
      "Processing batch 41 of 77\n",
      "Input File Path: /home/jovyan/cut-data/testing/non_target/1.000_0001_-WmL01c-4ZE_20.wav\n",
      "Entry: 500\n",
      "True Label: 1\n",
      "Predicted Label: 1\n",
      "Output Tensor: [[1.8796516e-11 1.0000000e+00]]\n",
      "Input File Path: /home/jovyan/cut-data/testing/non_target/1.000_0001_4M0njWKFsME_30.wav\n",
      "Entry: 600\n",
      "True Label: 1\n",
      "Predicted Label: 1\n",
      "Output Tensor: [[1.7195707e-14 1.0000000e+00]]\n",
      "Processing batch 51 of 77\n",
      "Input File Path: /home/jovyan/cut-data/testing/non_target/1.000_0002_1C6VhOCffIE_60.wav\n",
      "Entry: 700\n",
      "True Label: 1\n",
      "Predicted Label: 1\n",
      "Output Tensor: [[7.464242e-12 1.000000e+00]]\n",
      "Processing batch 61 of 77\n",
      "Input File Path: /home/jovyan/cut-data/testing/non_target/1.000_0003_-7z662AsuTE_380.wav\n",
      "Entry: 800\n",
      "True Label: 1\n",
      "Predicted Label: 1\n",
      "Output Tensor: [[3.3357954e-09 1.0000000e+00]]\n",
      "Processing batch 71 of 77\n",
      "Input File Path: /home/jovyan/cut-data/testing/non_target/1.000_0003_2hBVVym00rc_30.wav\n",
      "Entry: 900\n",
      "True Label: 1\n",
      "Predicted Label: 1\n",
      "Output Tensor: [[1.11487514e-10 1.00000000e+00]]\n",
      "Accuracy: 100.00%\n",
      "Recall: 200.00%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[458   0]\n",
      " [  0 458]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  non_target       1.00      1.00      1.00       458\n",
      "      target       1.00      1.00      1.00       458\n",
      "\n",
      "    accuracy                           1.00       916\n",
      "   macro avg       1.00      1.00      1.00       916\n",
      "weighted avg       1.00      1.00      1.00       916\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import evaluateTFliteModel\n",
    "\n",
    "test_data_path = \"/home/jovyan/cut-data/testing/\"\n",
    "batch_size = 12\n",
    "\n",
    "evaluateTFliteModel.evaluate_tflite_model(quant_aware_INT8_tflite_path, test_data_path, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

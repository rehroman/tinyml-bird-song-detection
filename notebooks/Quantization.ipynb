{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "424f1f80-d145-48f5-b3f2-162591538540",
   "metadata": {},
   "source": [
    "### Full integer quantization of weights and activations into 8-bit integer\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594e23c-5ff3-4f30-8d00-afdd74de1f8c",
   "metadata": {},
   "source": [
    "#### Load validation data and convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "914f973f-d82a-413d-9e9b-15f8a9eaad4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-05 14:48:53.664467: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-05 14:48:53.706897: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-05 14:48:53.707424: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-05 14:48:54.347320: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data...\n",
      "Processed 0 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/0.920_0001_270097.wav\n",
      "Processed 100 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/0.930_0002_182583971.wav\n",
      "Processed 200 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/0.940_0004_534761.wav\n",
      "Processed 300 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/0.950_0018_226391901.wav\n",
      "Processed 400 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/0.970_0017_647758.wav\n",
      "Processed 500 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/1.000_0001_0H2uMhzSitY_520.wav\n",
      "Processed 600 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/1.000_0002_--ivFZu-hlc_30.wav\n",
      "Processed 700 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/1.000_0002_2RpOd9MJjyQ_10.wav\n",
      "Processed 800 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/1.000_0003_-w4HLksto_k_30.wav\n",
      "Processed 900 files. Currently processing file: /home/jovyan/cut-data/validation/non_target/1.000_0003_6m5hv5BX7KU_40.wav\n",
      "Processed 0 files. Currently processing file: /home/jovyan/cut-data/validation/target/0.480_0001_200614_1467_1.wav\n",
      "Processed 100 files. Currently processing file: /home/jovyan/cut-data/validation/target/0.920_0103_669234.wav\n",
      "Processed 200 files. Currently processing file: /home/jovyan/cut-data/validation/target/0.940_0015_139518451.wav\n",
      "Processed 300 files. Currently processing file: /home/jovyan/cut-data/validation/target/0.950_0064_558294.wav\n",
      "Processed 400 files. Currently processing file: /home/jovyan/cut-data/validation/target/0.970_0001_155008.wav\n",
      "...Done. Loaded 1381 validation samples and 2 labels.\n"
     ]
    }
   ],
   "source": [
    "import data as datapy\n",
    "\n",
    "val_data_path = \"/home/jovyan/cut-data/validation/\"\n",
    "\n",
    "# load validation as representative data for quantization\n",
    "print('Loading validation data...', flush=True)\n",
    "x_val, y_val, labels, file_paths_val = datapy.loadData(val_data_path)\n",
    "print('...Done. Loaded {} validation samples and {} labels.'.format(x_val.shape[0], y_val.shape[1]), flush=True)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "import numpy as np\n",
    "\n",
    "x_val = np.array(x_val, dtype='float32')\n",
    "y_val = np.array(y_val, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f2832e-eef0-4abb-b7be-ede92734a361",
   "metadata": {},
   "source": [
    "##### Load dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "819b4b75-8a70-4c53-ac3f-3ad2c799bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model as modelpy\n",
    "\n",
    "batch_size = 12\n",
    "\n",
    "val_gen = modelpy.AudioDataGenerator(file_paths_val, y_val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c68221-60b0-4c9e-8492-35449de50a78",
   "metadata": {},
   "source": [
    "##### Load Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaba55cc-6a9c-413c-9d06-9a6331de521e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading keras model\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# baseline keras file which still has the activation layer in the end\n",
    "keras_baselineModel_activation_path = \"/home/jovyan/models/checkpoints_/baseline_two_class_model_activation/\"\n",
    "keras_baselineModel_activation = keras.models.load_model(keras_baselineModel_activation_path)\n",
    "\n",
    "print(\"Finished loading keras model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae4e23a7-4a21-491a-b19d-62e720e5bcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpumczl62m/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpumczl62m/assets\n",
      "/opt/conda/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:887: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2023-08-05 16:34:23.913509: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-08-05 16:34:23.913563: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-08-05 16:34:23.913827: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpumczl62m\n",
      "2023-08-05 16:34:23.938658: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2023-08-05 16:34:23.938704: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpumczl62m\n",
      "2023-08-05 16:34:24.019821: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-08-05 16:34:24.406891: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpumczl62m\n",
      "2023-08-05 16:34:24.558529: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 644701 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "def representative_data_gen():\n",
    "  for input_value, _ in val_gen:\n",
    "    # Here we yield only input data because during conversion only this part is used\n",
    "    yield [input_value]\n",
    "\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_baselineModel_activation)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f5ef73-1121-4598-95af-2f6d09009cd2",
   "metadata": {},
   "source": [
    "##### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ed352fe-7f46-44e5-b8fe-b6a816f512d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the TFLite model: 6.99 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "tflite_size = len(tflite_quant_model) / (1024 * 1024)\n",
    "print(f\"Size of the TFLite model: {tflite_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8c2344-08e6-4611-921c-a8b5ae7e252d",
   "metadata": {},
   "source": [
    "##### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c72f756-be8d-412b-a04b-671a81e4a802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the quantized model:\n",
    "tflite_model_quant_file = \"/home/jovyan/models/checkpoints_/baseline_activation_INT8.tflite\"\n",
    "with open(tflite_model_quant_file, \"wb\") as f:\n",
    "    f.write(tflite_quant_model)\n",
    "print(\"Saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
